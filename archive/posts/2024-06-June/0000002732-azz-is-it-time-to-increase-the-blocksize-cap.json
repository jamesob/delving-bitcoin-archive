{
  "id": 2732,
  "name": "",
  "username": "azz",
  "avatar_template": "/user_avatar/delvingbitcoin.org/azz/{size}/405_2.png",
  "created_at": "2024-06-18T06:44:43.460Z",
  "cooked": "<p>NACK</p>\n<aside class=\"quote no-group\" data-username=\"shocknet_justin\" data-post=\"19\" data-topic=\"941\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/shocknet_justin/48/600_2.png\" class=\"avatar\"> shocknet_justin:</div>\n<blockquote>\n<p>Every economic entity could have a channel in the next year</p>\n</blockquote>\n</aside>\n<p>They certainly could, however increasing the L1 block limit probably isn\u2019t the way to do this. When Lightning\u2019s original paper came out, it was already known that it wouldn\u2019t scale to 7 billion occupants. Instead of modifying L1 further, it might be better to design a layer either between the timechain and lightning or on top of lightning.</p>\n<p>Does anyone pushing big blocks run a node themselves? The main issue with, say, 32MB blocks is just that. It\u2019s not that most consumer hardware couldn\u2019t handle this (though it makes decentralisation over low power, non-traditional IP networks and low bandwidth ones more difficult), but the storage requirements would change enormously. 32MB a block, 144 blocks a day is a <strong>4.608GB per day storage requirement</strong>. My node currently is storing ~658GB (ignoring electrum indexes too). With 32 byte blocks, my hardware could totally handle it, but you\u2019d rival the entire chain size in less than half a year. I can\u2019t handle buying 1TB drives every 7.12 months, and I doubt many other operators would be fond of this either.</p>\n<p>Maybe investigate opcodes that don\u2019t cause issues to build additional layers, or layers without new opcodes (best, really), but I don\u2019t think this is viable. Not on the base layer.</p>",
  "post_number": 21,
  "post_type": 1,
  "posts_count": 26,
  "updated_at": "2024-06-18T06:44:43.460Z",
  "reply_count": 1,
  "reply_to_post_number": null,
  "quote_count": 1,
  "incoming_link_count": 6,
  "reads": 62,
  "readers_count": 61,
  "score": 47.4,
  "yours": false,
  "topic_id": 941,
  "topic_slug": "is-it-time-to-increase-the-blocksize-cap",
  "topic_title": "Is it time to increase the blocksize cap?",
  "topic_html_title": "Is it time to increase the blocksize cap?",
  "category_id": 5,
  "display_username": "",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "NACK\n\n[quote=\"shocknet_justin, post:19, topic:941\"]\nEvery economic entity could have a channel in the next year\n[/quote]\n\nThey certainly could, however increasing the L1 block limit probably isn't the way to do this. When Lightning's original paper came out, it was already known that it wouldn't scale to 7 billion occupants. Instead of modifying L1 further, it might be better to design a layer either between the timechain and lightning or on top of lightning. \n\nDoes anyone pushing big blocks run a node themselves? The main issue with, say, 32MB blocks is just that. It's not that most consumer hardware couldn't handle this (though it makes decentralisation over low power, non-traditional IP networks and low bandwidth ones more difficult), but the storage requirements would change enormously. 32MB a block, 144 blocks a day is a **4.608GB per day storage requirement**. My node currently is storing ~658GB (ignoring electrum indexes too). With 32 byte blocks, my hardware could totally handle it, but you'd rival the entire chain size in less than half a year. I can't handle buying 1TB drives every 7.12 months, and I doubt many other operators would be fond of this either. \n\nMaybe investigate opcodes that don't cause issues to build additional layers, or layers without new opcodes (best, really), but I don't think this is viable. Not on the base layer.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 334,
  "hidden": false,
  "trust_level": 1,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "NACK \n\nThey certainly could, however increasing the L1 block limit probably isn\u2019t the way to do this. When Lightning\u2019s original paper came out, it was already known that it wouldn\u2019t scale to 7 billion occupants. Instead of modifying L1 further, it might be better to design a layer either between the&hellip;",
  "truncated": true,
  "post_url": "/t/is-it-time-to-increase-the-blocksize-cap/941/21",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}