{
  "id": 2874,
  "name": "Pieter Wuille",
  "username": "sipa",
  "avatar_template": "/user_avatar/delvingbitcoin.org/sipa/{size}/102_2.png",
  "created_at": "2024-07-09T18:42:57.127Z",
  "cooked": "<h1><a name=\"introduction-to-cluster-linearization-1\" class=\"anchor\" href=\"#introduction-to-cluster-linearization-1\"></a>Introduction to cluster linearization</h1>\n<div data-theme-toc=\"true\"> </div>\n<h2><a name=\"h-1-abstract-2\" class=\"anchor\" href=\"#h-1-abstract-2\"></a>1. Abstract</h2>\n<p>This document attempts to explain the background, reasoning, and complexities behind cluster linearization. I started writing this as reading material for those who want to review Bitcoin Core PRs <a href=\"https://github.com/bitcoin/bitcoin/pull/30126\">#30126</a> (the basic linearization/improvement algorithm) and <a href=\"https://github.com/bitcoin/bitcoin/pull/30285\">#30285</a> (post-processing and merging of linearizations), but it is probably a useful explanation in general.</p>\n<p>Things that are <em>not</em> covered here:</p>\n<ul>\n<li>The reasons for wanting a cluster mempool based approach in the first place. See <a href=\"https://delvingbitcoin.org/t/an-overview-of-the-cluster-mempool-proposal/393\">this topic</a> for that.</li>\n<li>More rigorous theory and proofs. See <a href=\"https://delvingbitcoin.org/t/cluster-mempool-definitions-theory/202\">this topic</a> for that.</li>\n<li>A detailed walk-through of the code in the PRs. This document just discusses abstract algorithms, but there are many implementation details needed to make them efficient. I believe those are best documented as comments in the code itself.</li>\n<li>Timeline, organization, or status of the involved PRs. See the <a href=\"https://github.com/bitcoin/bitcoin/issues/30289\">Tracking Issue</a> for that.</li>\n<li>As will become clear further, the most computationally expensive part of linearization is finding high-feerate subsets of a cluster to move to the front. That aspect is treated as black box here, but is largely covered in <a href=\"https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303\">this topic</a>.</li>\n</ul>\n<h2><a name=\"h-2-basics-3\" class=\"anchor\" href=\"#h-2-basics-3\"></a>2. Basics</h2>\n<p>Briefly summarized, the cluster mempool proposal consists of:</p>\n<ul>\n<li>\n<p>New data structures:</p>\n<ul>\n<li><strong>Clusters</strong>: The set of mempool transactions is partitioned into groups known as <em>clusters</em> based on their connectivity. In order to bound the computational cost of operations on these clusters (see further), a <em>cluster count limit</em> is introduced, replacing the ancestor and descendant count limits.</li>\n<li><strong>Linearizations</strong>: For every cluster, a <em>linearization</em> is precomputed and maintained at all times. This is the order in which the transactions of that cluster would be mined, which is independent of other clusters. The goal of this document is explaining how we can find and maintain these linearizations.</li>\n<li><strong>Chunks</strong>: When higher-feerate transactions follow lower-feerate transactions in a linearization, they will naturally be included in blocks together, as a generalization of child-pays-for-parent (as the higher ones help pay for the lower ones). These groupings are known as the <em>chunks</em> of the linearization, and can be precomputed along with the linearizations themselves.</li>\n</ul>\n</li>\n<li>\n<p>Affected node operations:</p>\n<ul>\n<li><strong>Transaction relay</strong>: When new transactions are relayed, they will become new clusters, attach to existing clusters, or even merge multiple existing clusters. When this happens, the linearizations for those clusters need to be updated and/or recomputed.</li>\n<li><strong>Block validation</strong>: When new blocks are relayed and accepted, the mempool needs to be updated to account for the resulting changes (removing mined and conflicted transactions, and in the case of reorganizations, moving transactions back to the mempool). This will generally involve a bulk update to the mempool\u2019s linearizations.</li>\n<li><strong>Block building</strong>: Having precomputed linearizations and chunks for all mempool clusters, the construction of a block template for mining consists of repeatedly including the highest-feerate remaining first chunk across all clusters, until the block is (close to) full.</li>\n<li><strong>Eviction</strong>: Eviction (removal of mempool transactions when the mempool grows too large) becomes the exact opposite of block building: removing the chunk which would be included in a block last, which is simply the lowest-feerate last chunk across all clusters.</li>\n<li><strong>Replace-by-fee evaluation</strong>: In order to determine whether accepting an RBF transaction is an improvement to the mempool, we can directly compare the quality of the linearization of the affected clusters before and after, by comparing the cumulative-size-vs-cumulative-fee diagram of both (called the <em>feerate diagram</em>). See <a href=\"https://delvingbitcoin.org/t/post-clustermempool-package-rbf-per-chunk-processing/190\">this topic</a> for more information about (package) RBF and cluster mempool.</li>\n<li><strong>Fee estimation</strong>: fee estimation can benefit from having access to chunk feerates of the transactions involved, which act as effective mining scores. See <a href=\"https://delvingbitcoin.org/t/package-aware-fee-estimator-post-cluster-mempool/312\">this topic</a>.</li>\n</ul>\n</li>\n</ul>\n<p>Below we expand on some of these concepts.</p>\n<h3><a name=\"h-21-clusters-and-linearizations-4\" class=\"anchor\" href=\"#h-21-clusters-and-linearizations-4\"></a>2.1. Clusters and linearizations</h3>\n<p>Consider the following example mempool, with 9 transactions, each labeled \u201cname: fee/size\u201d. The arrows point up from children to their parents (so, e.g. transaction D spends an output of A and an output of B):</p>\n<pre data-code-height=\"236\" data-code-wrap=\"mermaid\"><code class=\"lang-mermaid\">graph BT\n   C[\"C: 2/1\"] --&gt; A[\"A: 1/2\"];\n   D[\"D: 1/3\"] --&gt; A;\n   D --&gt; B[\"B: 3/1\"];\n   E[\"E: 1/1\"] --&gt; D;\n   G[\"G: 4/1\"] --&gt; F[\"F: 1/1\"];\n   H[\"H: 3/4\"] --&gt; F;\n   I[\"I: 2/1\"];\n</code></pre>\n<p>Clusters are defined as groups of transactions that are reachable from one another using any combination of is-parent-of and is-child-of relations. In the mempool above there are three clusters: ABCDE, FGH, and I. Here, and in what follows, names like FGH are a short notation for \u201cthe set of transactions containing F, G, and H\u201d.</p>\n<p>For these clusters, at all times, a <em>linearization</em> (the order those transactions are to be mined in w.r.t. each other) is maintained. A valid linearization requires that all parents appear before their children. Still, there can be significant differences in quality between linearizations for the same cluster (see below). The idea is that whenever transactions are added or removed from clusters, the linearization for it is updated and/or recomputed. For the ABCDE cluster above, valid linearizations include (A,B,C,D,E), (B,A,D,C,E), and (A,C,B,D,E), but not (B,D,A,C,E) (D before A).</p>\n<p>The <em>prefixes</em> of a linearization are all sets consisting of consecutive elements of the linearization from the start up to every point. For the (A,C,B,D,E) linearization, the prefixes are {A, AC, ACB, ACBD, ACBDE}.</p>\n<p>To make sure some reasonable quality of linearization is always achievable within acceptable transaction relay latency, the cluster mempool proposal involves introducing a <em>cluster count limit</em> (in number of transactions), replacing the ancestor set and descendant set count limits.</p>\n<p>It turns out that knowing the linearization of every cluster is sufficient to quickly construct good block templates for a given mempool. Furthermore, it lets us quickly determine what effective feerate a transaction will be mined at, even without actually constructing a block.</p>\n<h3><a name=\"h-22-feerate-diagrams-and-chunking-5\" class=\"anchor\" href=\"#h-22-feerate-diagrams-and-chunking-5\"></a>2.2. Feerate diagrams and chunking</h3>\n<p>To analyze linearizations, we make use of the <em>feerate diagram</em>, here demonstrated for the (A,B,C,D,E) linearization for the cluster shown above:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/1X/9bc9b067bd9f96f765e08fd8ef650fe28e222064.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/9bc9b067bd9f96f765e08fd8ef650fe28e222064\" title=\"diagram\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/1X/9bc9b067bd9f96f765e08fd8ef650fe28e222064_2_264x250.png\" alt=\"diagram\" data-base62-sha1=\"mea7JGdJlqVoDvMEgJ4owg45Hvu\" width=\"264\" height=\"250\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/1X/9bc9b067bd9f96f765e08fd8ef650fe28e222064_2_264x250.png, https://delvingbitcoin.org/uploads/default/optimized/1X/9bc9b067bd9f96f765e08fd8ef650fe28e222064_2_396x375.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/1X/9bc9b067bd9f96f765e08fd8ef650fe28e222064_2_528x500.png 2x\" data-dominant-color=\"F9F9FA\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">diagram</span><span class=\"informations\">567\u00d7535 23.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>In a diagram where the X-axis represents the cumulative transaction size (which in practice could mean vsize or weight), and the Y-axis represents the cumulative transaction fee, place points for each prefix of the linearization (black dots). Then draw the <a href=\"https://en.wikipedia.org/wiki/Convex_hull\">convex hull</a> line of those points (purple line). The resulting line is the feerate diagram, which gives an approximation of how much fee can be collected within any given size. Each of its segments will correspond to one or more transactions; we call those groups of transactions the <em>chunks</em> of the linearization. The slope of the line segment corresponds to the <em>chunk feerate</em> (total fee divided by total size) of that chunk. Collectively, the chunks of a linearization are its <em>chunking</em>.</p>\n<p>The example has two chunks, with chunking [ABC,DE]. Note the use of square brackets for chunkings, and parentheses for linearizations.</p>\n<p>The <em>prefixes</em> of a chunking are, similar to the prefixes of a linearization, the sets formed as the unions of all chunks from the beginning of the chunking up to every point. The prefixes of [ABC,DE] are {ABC, ABCDE}. These prefixes correspond to the diagram points that form the chunk boundaries.</p>\n<p>It turns out that the chunking can be computed using the following algorithm:</p>\n<ul>\n<li><span class=\"math\">\\operatorname{Chunking}(C, L)</span>, for a cluster <span class=\"math\">C</span> with linearization <span class=\"math\">L</span>:\n<ul>\n<li>Set <span class=\"math\">R = []</span>, a list of sets of transactions.</li>\n<li>For <span class=\"math\">tx \\in L</span>:\n<ul>\n<li>Append the singleton <span class=\"math\">\\{tx\\}</span> to <span class=\"math\">R</span>.</li>\n<li>While <span class=\"math\">R</span> has two or more elements, and the feerate of the last one is higher than that of the penultimate one:\n<ul>\n<li>Replace the last two elements of <span class=\"math\">R</span> with their union.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Return <span class=\"math\">R</span>.</li>\n</ul>\n</li>\n</ul>\n<p>Working out our example, we get the following steps:</p>\n<ul>\n<li>Add A: [A]</li>\n<li>Add B: [A,B]\n<ul>\n<li>B\u2019s feerate is higher than A\u2019s, combine them: [AB]</li>\n</ul>\n</li>\n<li>Add C: [AB,C]\n<ul>\n<li>C\u2019s feerate is higher than AB\u2019s, combine them: [ABC]</li>\n</ul>\n</li>\n<li>Add D: [ABC,D]\n<ul>\n<li>D\u2019s feerate is <em>not</em> higher than ABC\u2019s, stop the while loop: [ABC,D]</li>\n</ul>\n</li>\n<li>Add E: [ABC,D,E]\n<ul>\n<li>E\u2019s feerate is higher than D\u2019s, combine them: [ABC,DE]</li>\n<li>DE\u2019s feerate is <em>not</em> higher than ABC\u2019s, stop the while loop: [ABC,DE]</li>\n</ul>\n</li>\n</ul>\n<p>Next we analyze the complexity of this algorithm. Note that here and in what follows, the complexities assume that set operations (including intersections, unions, subtractions, \u2026) are <span class=\"math\">\\mathcal{O}(1)</span>. This is not true in an asymptotic sense (operations on larger sets are necessarily <span class=\"math\">\\mathcal{O}(n)</span>, just due to reading the set), but is largely true in practice as the implementation uses bitsets (of bounded size) to represent sets.</p>\n<p>The <span class=\"math\">\\operatorname{Chunking}</span> algorithm consists of two nested loops, but the inner loop can run at most once per element of <span class=\"math\">L</span>, making it have only <span class=\"math\">\\mathcal{O}(n)</span> iterations. If we additionally cache the combined fee and size of the sets in <span class=\"math\">R</span>, the cost per iteration is <span class=\"math\">\\mathcal{O}(1)</span>, and the result is an <span class=\"math\">\\mathcal{O}(n)</span> algorithm, which is so fast that we can generally ignore the distinction between linearizations and their chunkings. For some operations the linearization is more convenient, and for others the chunking is; we will proceed as if both are always available, even though it may mean recomputing the chunking from time to time.</p>\n<p>An important property is that the first chunk of a linearization has the highest feerate/slope of any prefix of the linearization, and further chunks have non-increasing feerates/slope. This must be the case: if not, a higher-slope line would follow a lower-slope line, which would be in contradiction with the convex hull shape of the feerate diagram.</p>\n<p>In a way, chunks can be seen as a generalization of <a href=\"https://bitcoinops.org/en/topics/cpfp/\">child-pays-for-parent</a>: whenever children pay a higher feerate than their ancestors, they may get chunked together with them, effectively pooling their fees to pay for their joint size. The effective feerate, or mining score, of a transaction is the feerate of the chunk it is part of. This effective feerate can be higher or lower than a transaction\u2019s individual feerate: it is lower when it is paying for lower-feerate transaction in the same chunk, and it is higher when another higher-feerate transaction in the same chunk is paying for it.</p>\n<h3><a name=\"h-23-comparing-linearizations-6\" class=\"anchor\" href=\"#h-23-comparing-linearizations-6\"></a>2.3. Comparing linearizations</h3>\n<p>Our goal is constructing and maintaining good linearizations for our clusters, but we need a metric to know what to optimize for. To do so, we compare their feerate diagrams for every X-axis (size) coordinate. Depending on how the diagram lines compare, two linearizations can be <strong>equivalent</strong> (coinciding), <strong>better</strong> (above), <strong>worse</strong> (below), or <strong>incomparable</strong> (both above and below).</p>\n<p>For example, (A,B,C,D,E) and (B,A,D,C,E) are incomparable, because both are above the other one for some sizes:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/1X/6a0054b1ca7d019eede0dd645fc75a9c8f8d5c5a.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/6a0054b1ca7d019eede0dd645fc75a9c8f8d5c5a\" title=\"incomparable\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/1X/6a0054b1ca7d019eede0dd645fc75a9c8f8d5c5a_2_264x250.png\" alt=\"incomparable\" data-base62-sha1=\"f7JjSI5ABMR5GPDiGwm3IKvkWqm\" width=\"264\" height=\"250\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/1X/6a0054b1ca7d019eede0dd645fc75a9c8f8d5c5a_2_264x250.png, https://delvingbitcoin.org/uploads/default/optimized/1X/6a0054b1ca7d019eede0dd645fc75a9c8f8d5c5a_2_396x375.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/1X/6a0054b1ca7d019eede0dd645fc75a9c8f8d5c5a_2_528x500.png 2x\" data-dominant-color=\"F9F8F9\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">incomparable</span><span class=\"informations\">567\u00d7535 24 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>While the convex-hull aspect of the feerate diagram results in diagrams only being an approximation for the quality of a linearization (by smoothing out fees over chunks), it is one with a very important property: <strong>every cluster has an <em>optimal</em> linearization which is equivalent to or better than every valid linearization of the same cluster</strong>. There can be multiple optimal linearizations that are all equivalent, but they will all be strictly better than all non-optimal linearizations. This implies that when faced with two incomparable linearizations, a third one always exists that is strictly better than both.</p>\n<p>For our ABCDE example, that optimal linearization is (B,A,C,D,E), with chunking [B,AC,DE]. It is clearly better than both of our earlier linearizations (A,B,C,D,E) and (B,A,D,C,E):</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/1X/fe9279bdf35c6707e146ca60314a1c384e891464.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/fe9279bdf35c6707e146ca60314a1c384e891464\" title=\"optimal\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/1X/fe9279bdf35c6707e146ca60314a1c384e891464_2_264x250.png\" alt=\"optimal\" data-base62-sha1=\"Ak36c6lTEAwwH4mqXQygz3KdzJq\" width=\"264\" height=\"250\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/1X/fe9279bdf35c6707e146ca60314a1c384e891464_2_264x250.png, https://delvingbitcoin.org/uploads/default/optimized/1X/fe9279bdf35c6707e146ca60314a1c384e891464_2_396x375.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/1X/fe9279bdf35c6707e146ca60314a1c384e891464_2_528x500.png 2x\" data-dominant-color=\"F9F8F8\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">optimal</span><span class=\"informations\">567\u00d7535 25.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>Note that since the names of the points in the feerate diagram simply represent sets of transactions without implying an order, BAC is the same as ABC, and BACDE is the same as ABCDE. They are rendered here in linearization order for readability.</p>\n<p>Furthermore, it can be shown that an optimal linearization can be constructed using the following algorithm:</p>\n<ul>\n<li><span class=\"math\">\\operatorname{OptLin}(C)</span>, for a transaction cluster <span class=\"math\">C</span>:\n<ul>\n<li>Set <span class=\"math\">R = []</span>, an empty list of transactions.</li>\n<li>While there are transactions left in <span class=\"math\">C</span>:\n<ul>\n<li>Find the highest-feerate topological (=including all its own not yet included ancestors) subset <span class=\"math\">s \\subset C</span>, using <span class=\"math\">\\operatorname{OptSubset}</span> (see further).</li>\n<li>Append the elements of <span class=\"math\">s</span> to <span class=\"math\">R</span> in any valid order, and remove them from <span class=\"math\">C</span>.</li>\n</ul>\n</li>\n<li>Return <span class=\"math\">R</span>.</li>\n</ul>\n</li>\n</ul>\n<p>The <span class=\"math\">\\operatorname{OptSubset}</span> algorithm is not described in this post, but the approach in Section 2 of <a href=\"https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303#h-2-finding-high-feerate-subsets-5\">How to linearize your cluster</a> can be used to implement it with <span class=\"math\">\\mathcal{O}(n \\cdot 2^{n/2})</span> complexity. Due to the exponential nature, this approach is not computationally feasible for every cluster size we want to support. Still, it is a good starting point for the algorithms that follow.</p>\n<p>Besides acting as a criterion to define better and worse linearizations for a given cluster, feerate diagram comparison can also be used to determine which of two <em>clusters</em> is better, or even two whole mempools. This is relevant in RBF (replace by fee) settings, where we want to determine whether a transaction replacement is <a href=\"https://delvingbitcoin.org/t/mempool-incentive-compatibility/553\">incentive compatble</a>. Specific ideas on how to leverage cluster mempool based feerate diagram comparisons in a (package) RBF setting are explored in <a href=\"https://delvingbitcoin.org/t/post-clustermempool-package-rbf-per-chunk-processing/190\">this topic</a>.</p>\n<h3><a name=\"h-24-block-building-and-eviction-7\" class=\"anchor\" href=\"#h-24-block-building-and-eviction-7\"></a>2.4. Block building and eviction</h3>\n<p>When building block templates from a mempool, or when evicting from a mempool, we need to look at all clusters, rather than just one. To reason about that, notice that nothing prevents us from defining a linearization or its chunking for the entire mempool. We do not need to actually compute this full-mempool chunking, but we can still reason about its properties.</p>\n<p>One important property is that given optimal linearizations\u2019 chunkings for all clusters in the mempool, the optimal chunking of the entire mempool consists of a \u201cmerge sort\u201d of the chunks of the individual clusters. That is, include the chunks one by one from all clusters, from high to low feerate. Since chunks within a cluster are already in decreasing feerate order, the result will respect the individual cluster linearization orders. When there are multiple chunks of equal feerate within one linearization, their order has to be maintained of course. This merge-sort property is what makes cluster mempool \u201cwork\u201d: the fact that transactions do not affect the linearizations beyond their own clusters.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/1X/cb2d39fdd2ce7b88d73f1ea6b30a285ab0ff4339.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/cb2d39fdd2ce7b88d73f1ea6b30a285ab0ff4339\" title=\"full\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/1X/cb2d39fdd2ce7b88d73f1ea6b30a285ab0ff4339_2_367x374.png\" alt=\"full\" data-base62-sha1=\"sZnP52PPeM03bMtwEyFLj3d9BpL\" width=\"367\" height=\"374\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/1X/cb2d39fdd2ce7b88d73f1ea6b30a285ab0ff4339_2_367x374.png, https://delvingbitcoin.org/uploads/default/optimized/1X/cb2d39fdd2ce7b88d73f1ea6b30a285ab0ff4339_2_550x561.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/1X/cb2d39fdd2ce7b88d73f1ea6b30a285ab0ff4339_2_734x748.png 2x\" data-dominant-color=\"FBFAFA\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">full</span><span class=\"informations\">855\u00d7871 46.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>For our example mempool, the optimal chunkings are [B,AC,DE], [FG,H], and [I]. The optimal linearization of the entire mempool is (B,I,F,G,A,C,H,D,E) with chunking [B,I,FG,AC,H,DE], which is just the chunks from all clusters combined and sorted.</p>\n<p>Even though we cannot guarantee that our linearizations are optimal, the approach is still usable for building blocks: we pick however many of the first chunks of the whole-mempool chunking fit. This can be done by consecutively picking the next highest-feerate remaining first chunk among all cluster linearizations, until the next one is too large to fit.</p>\n<p>Even if the cluster linearizations are optimal, this approach does not guarantee that the resulting block is optimal (in terms of maximizing total fees), due to bin packing effects near the end of the block. However, a weaker property does hold: <strong>if the cluster linearizations are optimal, then the prefixes of their merged chunkings have the maximum possible fee for their own size</strong>. Concretely, in our example here, that means for all sets of size 1 (B), 2 (BI), 4 (BIFG), 7 (BIFGAC), 10 (BIFGACH), and 14 (BIFGACHDE), no sets exist with a higher fee at the same or smaller size than these. In other words, <em>if</em> the block vsize limit happens to be exactly equal to the size of one of those prefixes, and the cluster linearizations are optimal, the block fee will be maximal.</p>\n<p>If the block vsize limit does not match the size of a prefix of chunks, the total fee may be lower than the highest possible one. Still, the feerate diagram of an optimal linearizations does provide an upper bound on how many fees are achievable in each size.</p>\n<p>In our example, if the block vsize limit were (for presentation purposes) just 3, then based on the diagram (knowing it is optimal) we can tell that no more than 6+\u00bd fee is possible (the fee of the point where the BI-BIFG line crosses size 3) with any topological set of transactions. The approach of picking chunks as long as they fit would yield 5 fee (the BI point), but the actual optimal has 6 fee (BFG or BFI). This difference is limited by the unfilled size of the block multiplied by the chunk feerate of the first chunk that does not fit. Because of this, we want a limit on the size (in vbytes) of clusters (in addition to the cluster count limit), as it limits the fee loss caused by this. Some of the loss of block template fees due to this effect can still be mitigated by e.g. skipping chunks too large and trying further ones, or searching different combinations at block building time, but it is hard to guarantee much this way.</p>\n<p>Note that all of the above is about block template fees that are lost just due to end-of-block size effects, which exist even when all cluster linearizations are optimal. If the linearizations are suboptimal, the losses due to that will compound.</p>\n<p>Given the insight that the (implicit) chunking for the entire mempool is the sorted combination of all individual cluster chunkings, and block building is (roughly) picking the beginning of that chunking, it is easy to reverse the process for eviction. When the mempool grows too large, we find the lowest-feerate <em>last</em> chunk among all cluster chunkings, and evict that, as it would be the last thing we would include in a block template.</p>\n<h2><a name=\"h-3-maintaining-linearizations-8\" class=\"anchor\" href=\"#h-3-maintaining-linearizations-8\"></a>3. Maintaining linearizations</h2>\n<p>With some of the basics covered, we dive into how cluster linearizations are constructed and maintained.</p>\n<h3><a name=\"h-31-dealing-with-suboptimality-9\" class=\"anchor\" href=\"#h-31-dealing-with-suboptimality-9\"></a>3.1. Dealing with suboptimality</h3>\n<p>Due to the exponential nature of the <span class=\"math\">\\operatorname{OptSubset}</span> algorithm, we have to accept that for some clusters we may not always know an optimal linearization. Let us turn <span class=\"math\">\\operatorname{OptLin}</span> into a <span class=\"math\">\\operatorname{Lin}</span> algorithm that replaces <span class=\"math\">\\operatorname{OptSubset}</span> with an arbitrary <span class=\"math\">f</span> function that finds a high-feerate non-empty topological subset of what it is passed:</p>\n<ul>\n<li><span class=\"math\">\\operatorname{Lin}_f(C)</span>, for a transaction cluster <span class=\"math\">C</span>, and a function <span class=\"math\">f</span> that returns a non-empty topological subset, preferably with a high feerate:\n<ul>\n<li>Set <span class=\"math\">R = []</span>, an empty list of transactions.</li>\n<li>While there are transactions left in <span class=\"math\">C</span>:\n<ul>\n<li>Let <span class=\"math\">s = f(C)</span>.</li>\n<li>Append the elements of <span class=\"math\">s</span> to <span class=\"math\">R</span> in any topological order, and remove them from <span class=\"math\">C</span>.</li>\n</ul>\n</li>\n<li>Return <span class=\"math\">R</span>.</li>\n</ul>\n</li>\n</ul>\n<p>While suboptimality does mean a degradation of block template quality, this is a minor effect only, as long as the result is close. There are more serious complications however. For example, in the RBF setting it makes it possible that we conclude an objectively worse replacement is an improvement and accept it, or vice versa. <a href=\"https://delvingbitcoin.org/t/post-clustermempool-package-rbf-per-chunk-processing/190/14\">Imagine</a> a complicated cluster for which network nodes are generally unable to find the optimal linearization. An attacker-constructed replacement may objectively be a degradation then, but if it simplifies the cluster enough that the optimal linearization does get discovered, network nodes may still treat it as an improvement.</p>\n<p>This highlights the need for not just cluster linearization quality in an absolute sense, but also in a relative one: ideally, if cluster A is better than cluster B when linearized optimally, we still recognize A as better than B even without optimal linearizations. This is not a fully addressable problem, but there are algorithms that can help improve the situation.</p>\n<p>Specifically, we can instead of only using algorithms that compute (hopefully, good) linearizations from scratch, also use algorithms that take as input one or more existing linearizations, and produce a new linearization that is equivalent or better than each input linearization. Three algorithms will be described:</p>\n<ul>\n<li><span class=\"math\">\\operatorname{Merge}(C, L_1, L_2)</span>: given a cluster <span class=\"math\">C</span> and two linearizations for it, produce a new linearization that is equivalent to or better than both. In particular, if the two input linearizations are incomparable, this implies their merging will be strictly better than both. See <a href=\"https://delvingbitcoin.org/t/merging-incomparable-linearizations/209\">this topic</a> for history. Complexity: <span class=\"math\">\\mathcal{O}(n^2)</span>.</li>\n<li><span class=\"math\">\\operatorname{LIMO}_f(C, L)</span>: given a cluster <span class=\"math\">C</span> and linearization for it, improve it by finding high-feerate subsets to move closer to the front using <span class=\"math\">f</span>. It is a variant of the <span class=\"math\">\\operatorname{Lin}_f</span> algorithm above, but crucially guarantees that the resulting linearization is equivalent to or better than the input linearization. See <a href=\"https://delvingbitcoin.org/t/limo-combining-the-best-parts-of-linearization-search-and-merging/825\">this topic</a> for history. Complexity: depends on how much time is spent in <span class=\"math\">f</span>, but at least <span class=\"math\">\\mathcal{O}(n^2)</span>.</li>\n<li><span class=\"math\">\\operatorname{PostLin}(C, L)</span>: given a cluster <span class=\"math\">C</span> and a linearization for it, possibly make some \u201cobvious\u201d improvements to the linearization which are not guaranteed by merging or LIMO. See <a href=\"https://delvingbitcoin.org/t/linearization-post-processing-o-n-2-fancy-chunking/201\">this topic</a> for history. Complexity: <span class=\"math\">\\mathcal{O}(n^2)</span>.</li>\n</ul>\n<p>The actual possible approaches using these algorithms are countless, but here are some example of how these algorithms can be employed when evaluating RBF replacements. Imagine a new transaction <span class=\"math\">a</span> (\u201cadd\u201d) arrives that would attach to an existing cluster <span class=\"math\">C</span>, which has an existing linearization, and where it conflicts with an old transaction <span class=\"math\">d</span> (\u201cdelete\u201d).</p>\n<ul>\n<li>We can compute an updated cluster <span class=\"math\">C'</span> (with <span class=\"math\">a</span> added, and <span class=\"math\">d</span> deleted), compute a linearization for <span class=\"math\">C'</span> from scratch using <span class=\"math\">\\operatorname{Lin}</span>, and compare the feerate diagrams of the old and new linearizations to determine if the result is an improvement.</li>\n<li>However, it is possible that significant effort had already been spent coming up with a good linearization for <span class=\"math\">C</span> before <span class=\"math\">a</span> arrived. In order to not throw that away, it is possible to first construct a \u201cnaive\u201d relinearization by taking the old one, removing <span class=\"math\">d</span> from it, and appending <span class=\"math\">a</span> at the end, and then using <span class=\"math\">\\operatorname{LIMO}</span> on that instead. Applying <span class=\"math\">\\operatorname{PostLin}</span> on that naive relinearization first can make the process better still.</li>\n<li>Even with this, it is possible that <span class=\"math\">\\operatorname{LIMO}</span> discovers an improvement in the new linearization that was actually already possible in the old one, but just was not known. In this case, it would be useful to \u201cbackport\u201d that knowledge to the old linearization before comparing. To do so, we can take the resulting new cluster\u2019s linearization, remove <span class=\"math\">a</span> and reinsert <span class=\"math\">d</span> again, and then use <span class=\"math\">\\operatorname{Merge}</span> to combine it with the original old linearization, possibly after another <span class=\"math\">\\operatorname{PostLin}</span>.</li>\n</ul>\n<p>Even in a non-replacement setting, <span class=\"math\">\\operatorname{LIMO}</span> and <span class=\"math\">\\operatorname{PostLin}</span> are useful for updating linearizations after a transaction is accepted.</p>\n<h3><a name=\"h-32-improving-linearizations-10\" class=\"anchor\" href=\"#h-32-improving-linearizations-10\"></a>3.2. Improving linearizations</h3>\n<p>Before digging into the actual algorithms, it is useful to gain some insight into what exactly it is that determines whether changes to a linearization constitute an improvement.</p>\n<h4><a name=\"h-321-naively-moving-higher-feerate-sets-11\" class=\"anchor\" href=\"#h-321-naively-moving-higher-feerate-sets-11\"></a>3.2.1 Naively moving higher-feerate sets</h4>\n<p>One might think that moving any topological subset with a feerate higher than the first chunk of a linearization to the front would necessarily be an improvement to its feerate diagram.</p>\n<p>Consider this simpler example, where transactions are labelled \u201cname: fee\u201d (they all have size 1):</p>\n<pre data-code-height=\"168\" data-code-wrap=\"mermaid\"><code class=\"lang-mermaid\">graph RL\n   S[\"S: 1\"];\n   T[\"T: 1\"];\n   U[\"U: 1\"];\n   V[\"V: 2\"];\n   W[\"W: 3\"];\n   V --&gt; T --&gt; S;\n   V --&gt; W;\n   U --&gt; W; \n</code></pre>\n<p>The linearization (S,T,W,V,U) has chunking [STWV,U], whose first chunk STWV has feerate 7/4=1.75:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/1X/a8bbc92ea54a9499c85128e3a400ac175d88715c.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/a8bbc92ea54a9499c85128e3a400ac175d88715c\" title=\"improve_pre\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/1X/a8bbc92ea54a9499c85128e3a400ac175d88715c_2_197x250.png\" alt=\"improve_pre\" data-base62-sha1=\"o4GxKGgFl77YDTrzCwsc9H74LPm\" width=\"197\" height=\"250\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/1X/a8bbc92ea54a9499c85128e3a400ac175d88715c_2_197x250.png, https://delvingbitcoin.org/uploads/default/optimized/1X/a8bbc92ea54a9499c85128e3a400ac175d88715c_2_295x375.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/1X/a8bbc92ea54a9499c85128e3a400ac175d88715c_2_394x500.png 2x\" data-dominant-color=\"F8F7F8\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">improve_pre</span><span class=\"informations\">423\u00d7535 22.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>Now consider the topological subset WU, with feerate 4/2=2. Despite having a higher feerate than the first chunk STWV of (S,T,W,V,U), moving it to the front is <em>not</em> a strict improvement:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/1X/fcb824f5622c84f7a1c34142cbc5e453736b8080.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/fcb824f5622c84f7a1c34142cbc5e453736b8080\" title=\"improve_post\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/1X/fcb824f5622c84f7a1c34142cbc5e453736b8080_2_197x250.png\" alt=\"improve_post\" data-base62-sha1=\"A3EQHzsP0W1LHuA81AtDbZPxqyk\" width=\"197\" height=\"250\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/1X/fcb824f5622c84f7a1c34142cbc5e453736b8080_2_197x250.png, https://delvingbitcoin.org/uploads/default/optimized/1X/fcb824f5622c84f7a1c34142cbc5e453736b8080_2_295x375.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/1X/fcb824f5622c84f7a1c34142cbc5e453736b8080_2_394x500.png 2x\" data-dominant-color=\"F6F6F7\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">improve_post</span><span class=\"informations\">423\u00d7535 25.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>By moving WU to the front, W becomes a new chunk on its own, and U merges with what follows. The resulting USTV chunk is actually worse in some places than the old linearization.</p>\n<p>Thus, when moving a topological subset to the front of a linearization, it clearly does not suffice that the moved set has a higher feerate than the first chunk. Another condition is needed to prevent the moved subset from splitting up and worsening further parts.</p>\n<h4><a name=\"h-322-guaranteed-non-worsening-12\" class=\"anchor\" href=\"#h-322-guaranteed-non-worsening-12\"></a>3.2.2 Guaranteed non-worsening</h4>\n<p>Imagine a very simple linearization <span class=\"math\">L</span> whose chunking consists of just two chunks, <span class=\"math\">u</span> and <span class=\"math\">(C \\setminus u)</span>, which necessarily implies that the feerate of <span class=\"math\">u</span> is not below that of <span class=\"math\">(C \\setminus u)</span>. Now consider a topological subset <span class=\"math\">s</span> of the cluster. We want to know under what conditions moving <span class=\"math\">s</span> to the front of the linearization will be an improvement.</p>\n<p>It turns out that sufficient conditions for non-worsening are:</p>\n<ul>\n<li>\n<ol>\n<li>The feerate of <span class=\"math\">s</span> needs to be at least as high as that of <span class=\"math\">u</span>.</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>The intersection <span class=\"math\">s \\cap u</span> needs to not exceed the feerate of <span class=\"math\">s</span>, or this intersection must be empty.</li>\n</ol>\n</li>\n</ul>\n<p>If additionally <span class=\"math\">s</span> has a strictly higher feerate than <span class=\"math\">u</span>, moving it will be a strict improvement.</p>\n<p>These are not necessary conditions (moving <span class=\"math\">s</span> may be a non-worsening/improvement even if both do not hold), but they are sufficient, and moreover they will turn out to be easy to satisfy. A proof for a more general statement can be found <a href=\"https://delvingbitcoin.org/t/limo-combining-the-best-parts-of-linearization-search-and-merging/825/8#h-13-the-set-gathering-theorem-4\">here</a>.</p>\n<p>And this gives a hint for the solution already: if we ever consider moving a higher-feerate set to the front of a linearization, but that set has an even-higher-feerate intersection with a prefix of what we already have, then we should move that intersection to the front instead. In our example there is indeed such an intersection (W), and if we were to move that intersection instead to the front, we do get a strictly improved chunking [W,STV,U]:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/1X/6d0cc39f66497bd044f29f5b2b66c4a6525574cd.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/6d0cc39f66497bd044f29f5b2b66c4a6525574cd\" title=\"improve_opt\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/1X/6d0cc39f66497bd044f29f5b2b66c4a6525574cd_2_197x250.png\" alt=\"improve_opt\" data-base62-sha1=\"fyHogkMQjSpvMNwIkVNvXHRjnbL\" width=\"197\" height=\"250\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/1X/6d0cc39f66497bd044f29f5b2b66c4a6525574cd_2_197x250.png, https://delvingbitcoin.org/uploads/default/optimized/1X/6d0cc39f66497bd044f29f5b2b66c4a6525574cd_2_295x375.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/1X/6d0cc39f66497bd044f29f5b2b66c4a6525574cd_2_394x500.png 2x\" data-dominant-color=\"F7F5F5\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">improve_opt</span><span class=\"informations\">423\u00d7535 27.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<h4><a name=\"h-323-composing-topological-sets-13\" class=\"anchor\" href=\"#h-323-composing-topological-sets-13\"></a>3.2.3. Composing topological sets</h4>\n<p>Whenever we are improving things, we will be building a new linearization while simultaneously guaranteeing that certain points on the feerate diagram which we know how to hit, are actually hit. In our original example, when considering these two linearizations:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/1X/653d95f8cc22cc0b4754d9e9a8671d6217f770de.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/653d95f8cc22cc0b4754d9e9a8671d6217f770de\" title=\"prefixes\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/1X/653d95f8cc22cc0b4754d9e9a8671d6217f770de_2_264x250.png\" alt=\"prefixes\" data-base62-sha1=\"erCaDpmQJGnPA14OtTg0hFWwxn8\" width=\"264\" height=\"250\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/1X/653d95f8cc22cc0b4754d9e9a8671d6217f770de_2_264x250.png, https://delvingbitcoin.org/uploads/default/optimized/1X/653d95f8cc22cc0b4754d9e9a8671d6217f770de_2_396x375.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/1X/653d95f8cc22cc0b4754d9e9a8671d6217f770de_2_528x500.png 2x\" data-dominant-color=\"F8F7F8\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">prefixes</span><span class=\"informations\">567\u00d7535 27.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>Any linearization whose feerate diagram goes through or above all the labeled points (corresponding to the prefixes of both linearizations) in the diagram will necessarily be as good or better as both the (A,B,C,D,E) and (B,A,D,C,E) linearizations. It turns out that a strategy exists to do this generically: given any number of topologically valid sets <span class=\"math\">u_1, u_2, \\ldots, u_k</span> which together include each cluster transaction at least once (in our example, the <span class=\"math\">u_i</span> sets would be A, B, AB, ABC, ABD, ABCD, and ABCDE), we can construct a linearization whose diagram goes through or above all of their (size, fee) points as follows:</p>\n<ul>\n<li><span class=\"math\">\\operatorname{Compose}(C, \\{u_1, u_2, \\ldots, u_k\\})</span>, for cluster <span class=\"math\">C</span> and topological subsets <span class=\"math\">u_i</span> which together cover all of <span class=\"math\">C</span>:\n<ul>\n<li>Let <span class=\"math\">R = []</span>, the returned linearization.</li>\n<li>While <span class=\"math\">C</span> is not empty:\n<ul>\n<li>Find a topological subset <span class=\"math\">s</span> whose feerate is at least the maximum of all <span class=\"math\">u_i</span>'s feerates, and such that no <span class=\"math\">s \\cap u_i</span> has an even higher feerate.</li>\n<li>Append <span class=\"math\">s</span> to <span class=\"math\">R</span> in any valid order, and remove it from <span class=\"math\">C</span>, and from all <span class=\"math\">u_i</span>.</li>\n</ul>\n</li>\n<li>Return <span class=\"math\">R</span>.</li>\n</ul>\n</li>\n</ul>\n<p>Ways to find <span class=\"math\">s</span> such that the necessary properties are fulfilled will follow later, we can already give an intuition for why this strategy works.</p>\n<p>Consider an equivalent algorithm that instead operates on linearizations <span class=\"math\">L_i</span> that each consist of just two chunks <span class=\"math\">[u_i, C \\setminus u_i]</span>. In each iteration it finds a subset <span class=\"math\">s</span> satisfying the conditions above, and constructs <span class=\"math\">k</span> new linearizations by moving <span class=\"math\">s</span> to the front of all old <span class=\"math\">L_i</span> ones. Since the two conditions for non-worsening are satisfied, each new linearization is at least as good as its corresponding old linearization. The algorithm then continues doing the same on the non-<span class=\"math\">s</span> part of the new linearizations. Ultimately, all the same sets <span class=\"math\">s</span> have been moved to the front, so the final linearizations are all the same, and thus this common output linearization is at least as good as every input linearization. Since for each <span class=\"math\">u_i</span> there was an input linearization whose feerate diagram goes through its (size, fee) point, the diagram of the resulting linearization goes through or above all those points too.</p>\n<p>This explanation is not exact, because there may be sets <span class=\"math\">u_i</span> for which it is impossible to construct a linearization <span class=\"math\">L_i</span> that has <span class=\"math\">u_i</span> as its first chunk (and everything else as one other chunk). Still, a <a href=\"https://delvingbitcoin.org/t/cluster-mempool-definitions-theory/202/14\">somewhat more rigorous</a> treatment shows that this <span class=\"math\">\\operatorname{Compose}</span> approach works for any set of topological <span class=\"math\">u_i</span> sets.</p>\n<p>To find each <span class=\"math\">s</span> set, we make use of what was hinted at already. Start with the highest-feerate <span class=\"math\">u_i</span>, but when that set has a non-empty higher-feerate intersection with any of the other <span class=\"math\">u_i</span> sets, switch to considering that intersection. Then try intersecting that with all other <span class=\"math\">u_i</span> sets again (even ones tried already), as another intersection may make it better yet, and keep doing that until no improvement is made anymore:</p>\n<ul>\n<li><span class=\"math\">\\operatorname{Compose}(C, \\{u_1, u_2, \\ldots, u_k\\})</span>, for cluster <span class=\"math\">C</span> and topological subsets <span class=\"math\">u_i</span> which together cover all of <span class=\"math\">C</span>:\n<ul>\n<li>Let <span class=\"math\">R = []</span>, the returned linearization.</li>\n<li>While <span class=\"math\">C</span> is not empty:\n<ul>\n<li>Set <span class=\"math\">s</span> to be the highest-feerate <span class=\"math\">u_i</span>.</li>\n<li>While <span class=\"math\">s</span> is being improved:\n<ul>\n<li>For every <span class=\"math\">u_i</span>, if <span class=\"math\">s \\cap u_i</span> is non-empty and has a higher feerate than <span class=\"math\">s</span>, update <span class=\"math\">s = s \\cap u_i</span>, and restart the while loop.</li>\n</ul>\n</li>\n<li>Append <span class=\"math\">s</span> to <span class=\"math\">R</span> in any valid order, and remove it from <span class=\"math\">C</span>, and from all <span class=\"math\">u_i</span>.</li>\n</ul>\n</li>\n<li>Return <span class=\"math\">R</span>.</li>\n</ul>\n</li>\n</ul>\n<p>To determine the complexity of this, note that:</p>\n<ul>\n<li>The other \u201cWhile <span class=\"math\">C</span> is not empty\u201d loop may run up to <span class=\"math\">n</span> times.</li>\n<li>The \u201cWhile <span class=\"math\">s</span> is being improved\u201d loop may run up to <span class=\"math\">\\operatorname{min}(n, k)</span> times.</li>\n<li>The \u201cFor every <span class=\"math\">u_i</span>\u201d loop will run <span class=\"math\">k</span> times.</li>\n<li>By caching the feerates of intersections between <span class=\"math\">s</span> and every <span class=\"math\">u_i</span>, the number of feerate operations done in the inner loop is limited to <span class=\"math\">nk</span> per iteration of the \u201cWhile <span class=\"math\">C</span> is not empty\u201d loop.</li>\n</ul>\n<p>This means the overall algorithm complexity is <span class=\"math\">\\mathcal{O}(n^2k)</span>. The practical specializations of it that follow will do better, though.</p>\n<h3><a name=\"h-33-practical-algorithms-14\" class=\"anchor\" href=\"#h-33-practical-algorithms-14\"></a>3.3. Practical algorithms</h3>\n<h4><a name=\"h-331-merge-15\" class=\"anchor\" href=\"#h-331-merge-15\"></a>3.3.1. Merge</h4>\n<p>The goal when merging two linearizations is constructing a linearization whose feerate diagram is for every size on or above both input linearization diagrams. As shown in the previous section, this can be accomplished by running the generic <span class=\"math\">\\operatorname{Compose}</span> algorithm with all prefixes of both input linearizations as <span class=\"math\">u_i</span> sets. Doing so would have <span class=\"math\">\\mathcal{O}(n^3)</span> complexity, and we can do better.</p>\n<p>Instead of using the general approach from the previous section for finding each <span class=\"math\">s</span> set, we can use a more specialized variant with better efficiency. The culprit is the need to restart the search for better intersections with the <span class=\"math\">u_i</span> sets every time <span class=\"math\">s</span> improves, which turns out, can be avoided. All we need is a way to find an <span class=\"math\">s</span> each time that satisfies the two conditions:</p>\n<ul>\n<li>\n<ol>\n<li>The feerate of <span class=\"math\">s</span> is at least as high as every prefix of the remainder of the two linearizations.</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>The feerate of the intersection between <span class=\"math\">s</span> and any linearization remainder prefix.</li>\n</ol>\n</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/1X/653d95f8cc22cc0b4754d9e9a8671d6217f770de.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/653d95f8cc22cc0b4754d9e9a8671d6217f770de\" title=\"prefixes\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/1X/653d95f8cc22cc0b4754d9e9a8671d6217f770de_2_264x250.png\" alt=\"prefixes\" data-base62-sha1=\"erCaDpmQJGnPA14OtTg0hFWwxn8\" width=\"264\" height=\"250\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/1X/653d95f8cc22cc0b4754d9e9a8671d6217f770de_2_264x250.png, https://delvingbitcoin.org/uploads/default/optimized/1X/653d95f8cc22cc0b4754d9e9a8671d6217f770de_2_396x375.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/1X/653d95f8cc22cc0b4754d9e9a8671d6217f770de_2_528x500.png 2x\" data-dominant-color=\"F8F7F8\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">prefixes</span><span class=\"informations\">567\u00d7535 27.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>One key insight is that many of the linearization prefixes are redundant. Any feerate diagram that hits or exceeds the convex hull vertices of the input linearization (B and ABCDE for the first; ABC and ABCDE for the second) will also hit or exceed every other point. In other words, it suffices to include in the <span class=\"math\">u_i</span> sets only the <em>chunk</em> prefixes rather than the linearization prefixes.</p>\n<p>This is not a full solution: we have reduced the problem to searching prefixes of what remains of the <em>original</em> chunks rather than all prefixes of what remains, but the restarting remains necessary. However, no harm is done by switching to the <em>actual</em> chunking of what remains of the linearization in the specific iteration of <span class=\"math\">\\operatorname{Compose}</span> we are in instead, as that will be at least as good as the remainder of the <span class=\"math\">u_i</span> sets after removing <span class=\"math\">s</span>. And with that, the loop is no longer needed:</p>\n<ul>\n<li><span class=\"math\">\\operatorname{Merge}(C, L_1, L_2)</span>:\n<ul>\n<li>Let <span class=\"math\">R = []</span></li>\n<li>While <span class=\"math\">C</span> has transactions left:\n<ul>\n<li>Compute the chunkings of what remains of both linearizations (using <span class=\"math\">\\operatorname{Chunking}</span>).</li>\n<li>Let <span class=\"math\">h</span> be the highest-feerate first chunk of the two (remember that the first chunk of a linearization is its highest-feerate prefix).</li>\n<li>For every prefix <span class=\"math\">p</span> of the <em>chunking</em> of the <em>other</em> linearization (the one which <span class=\"math\">h</span> is not the first chunk of), from short to long:\n<ul>\n<li>If the intersection <span class=\"math\">h \\cap p</span> is non-empty and has higher-feerate than <span class=\"math\">h</span> itself, set <span class=\"math\">s = h \\cap p</span> and stop.</li>\n</ul>\n</li>\n<li>If the loop above did not find any better intersection, set <span class=\"math\">s = h</span>.</li>\n<li>Append <span class=\"math\">s</span> in any valid order to <span class=\"math\">R</span>, remove its transactions from <span class=\"math\">C</span> and <span class=\"math\">L_1</span> and <span class=\"math\">L_2</span>.</li>\n</ul>\n</li>\n<li>Return <span class=\"math\">R</span>.</li>\n</ul>\n</li>\n</ul>\n<p>This works, because <span class=\"math\">h</span> in each iteration starts off being already the shortest possible prefix of the chunkings of what remains of one of the two linearizations, so whatever happens with intersections with the other linearization, there is never a need to go back to trying intersections with the chunks of the first one. And there is no point continuing either with intersections with the other one either: <span class=\"math\">s</span> is already a subset of all of the longer (later) ones, and the shorter (earlier) ones have been tried before, so cannot be an improvement (if one were better, it would already have been picked instead).</p>\n<p>Regarding complexity, the while loop can run at most <span class=\"math\">\\mathcal{O}(n)</span> times, and each iteration performs a <span class=\"math\">\\operatorname{Chunking}</span> which is also <span class=\"math\">\\mathcal{O}(n)</span>, as is the cost of iterating over the prefixes <span class=\"math\">p</span>. Together, this means an overall complexity of <span class=\"math\">\\mathcal{O}(n^2)</span>.</p>\n<p><strong>The existence of the merging algorithm proves that every cluster has an optimal linearization.</strong> Take every valid linearization, and merge them one by one together to obtain a merging of all them. The result is necessarily as good as every linearization, and thus optimal.</p>\n<h4><a name=\"h-332-limo-16\" class=\"anchor\" href=\"#h-332-limo-16\"></a>3.3.2. LIMO</h4>\n<p>Going back to the naive linearization algorithm from earlier:</p>\n<ul>\n<li><span class=\"math\">\\operatorname{Lin}_f(C)</span>, for a transaction cluster <span class=\"math\">C</span>, and a function <span class=\"math\">f</span> for finding high-feerate topological subsets:\n<ul>\n<li>Set <span class=\"math\">R = []</span>, an empty list of transactions.</li>\n<li>While there are transactions left in <span class=\"math\">C</span>:\n<ul>\n<li>Let <span class=\"math\">s = f(C)</span>.</li>\n<li>Append the elements of <span class=\"math\">s</span> to <span class=\"math\">R</span> in any valid order, and remove them from <span class=\"math\">C</span>.</li>\n</ul>\n</li>\n<li>Return <span class=\"math\">R</span>.</li>\n</ul>\n</li>\n</ul>\n<p>Our goal is constructing an (efficient) variant of this that also takes in an existing linearization, while also invoking <span class=\"math\">f</span> to find good subsets to move to the front. Naively, this can be accomplished by instead of building an <span class=\"math\">R</span> from scratch, starting from the existing linearization <span class=\"math\">L</span> and then for every set <span class=\"math\">s</span> found invoke <span class=\"math\">\\operatorname{Merge}</span> to merge <span class=\"math\">L</span> with a version of itself that has <span class=\"math\">s</span> moved to the front. This is where the name LIMO comes from: \u201cLinearization through Incremental Merging of Optimizations\u201d.</p>\n<p>The same behavior can be achieved more efficiently however:</p>\n<ul>\n<li><span class=\"math\">\\operatorname{LIMO}_f(C, L)</span>, for a transaction cluster <span class=\"math\">C</span>, an existing linearization <span class=\"math\">L</span>, and a function <span class=\"math\">f</span> that returns a non-empty topological subset, preferably with a high feerate:\n<ul>\n<li>Set <span class=\"math\">R = []</span>, an empty list of transactions.</li>\n<li>While there are transactions left in <span class=\"math\">C</span>:\n<ul>\n<li>Let <span class=\"math\">s</span> be the highest-feerate prefix of what remains of <span class=\"math\">L</span>.</li>\n<li>If <span class=\"math\">f(C)</span> has a higher feerate than <span class=\"math\">s</span>:\n<ul>\n<li>Set <span class=\"math\">s = f(C)</span>.</li>\n<li>For every prefix <span class=\"math\">p</span> of what remains of <span class=\"math\">L</span> from short to long:\n<ul>\n<li>If the intersection <span class=\"math\">s \\cap p</span> has an even higher feerate: set <span class=\"math\">s</span> to that and stop.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Append the elements of <span class=\"math\">s</span> to <span class=\"math\">R</span> in any valid order, and remove them from <span class=\"math\">C</span> and <span class=\"math\">L</span>.</li>\n</ul>\n</li>\n<li>Return <span class=\"math\">R</span>.</li>\n</ul>\n</li>\n</ul>\n<p>This is effectively a variant of <span class=\"math\">\\operatorname{Compose}</span> again where the <span class=\"math\">u_i</span> sets are chosen to be the prefixes of the original linearization <span class=\"math\">L</span>, and with a specialized <span class=\"math\">s</span>-finding step that additionally guarantees a feerate not below that of whatever <span class=\"math\">f(C)</span> is at that point. It satisfies the improvement properties for similar reasons as the <span class=\"math\">\\operatorname{Merge}</span> algorithm: (1) <span class=\"math\">s</span> starts off with a feerate not below the highest-feerate prefix, and (2) no further intersections with prefixes can improve it (shorter ones have been tried already, longer ones it is already a subset of). Therefore <span class=\"math\">R</span> will be at least as good as <span class=\"math\">L</span>.</p>\n<p>It is possible to restrict each initial <span class=\"math\">s</span> as well as the intersections to prefixes at the chunk boundaries of <span class=\"math\">L</span>. Unlike for <span class=\"math\">\\operatorname{Merge}</span>, there is no second linearization whose intersections may trigger a restart with intersections of the first linearization, so it suffices to only consider the <em>original</em> chunk boundaries (from before any <span class=\"math\">s</span> was removed from <span class=\"math\">L</span>). It would not hurt to re-chunk, but it is also not necessary here.</p>\n<ul>\n<li><span class=\"math\">\\operatorname{LIMO}_f(C, L)</span>, for a transaction cluster <span class=\"math\">C</span>, an existing linearization <span class=\"math\">L</span>, and a function <span class=\"math\">f</span> that returns a non-empty topological subset, preferably with a high feerate:\n<ul>\n<li>Set <span class=\"math\">R = []</span>, an empty list of transactions.</li>\n<li>Compute the chunking of <span class=\"math\">L</span>.</li>\n<li>While there are transactions left in <span class=\"math\">C</span>:\n<ul>\n<li>Let <span class=\"math\">s</span> be the highest-feerate prefix of what remains of <span class=\"math\">L</span>, at the (original) chunk boundaries.</li>\n<li>If <span class=\"math\">f(C)</span> has a higher feerate than <span class=\"math\">s</span>:\n<ul>\n<li>Set <span class=\"math\">s = f(C)</span>.</li>\n<li>For every prefix <span class=\"math\">p</span> of what remains of <span class=\"math\">L</span>, at the (original) chunk boundaries, from short to long:\n<ul>\n<li>If the intersection <span class=\"math\">s \\cap p</span> has an even higher feerate: set <span class=\"math\">s</span> to that and stop.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Append the elements of <span class=\"math\">s</span> to <span class=\"math\">R</span> in any valid order, and remove them from <span class=\"math\">C</span> and <span class=\"math\">L</span> (and optionally recompute chunking of what remains of <span class=\"math\">L</span>).</li>\n</ul>\n</li>\n<li>Return <span class=\"math\">R</span>.</li>\n</ul>\n</li>\n</ul>\n<p>If <span class=\"math\">f(C)</span> is cheap, the algorithm\u2019s complexity is dominated by the inner <span class=\"math\">p</span> loop, which may run up to <span class=\"math\">\\mathcal{O}(n^2)</span> times. If <span class=\"math\">f(C)</span> is expensive, its complexity will dominate.</p>\n<h4><a name=\"h-333-post-linearization-17\" class=\"anchor\" href=\"#h-333-post-linearization-17\"></a>3.3.3 Post-linearization</h4>\n<p>Consider the following example, with transactions labelled \u201cname: fee/size\u201d:</p>\n<pre data-code-height=\"236\" data-code-wrap=\"mermaid\"><code class=\"lang-mermaid\">graph BT\n   J[\"J: 2/3\"];\n   K[\"K: 1/4\"];\n   L[\"L: 2/1\"];\n   M[\"M: 2/1\"];\n   N[\"N: 1/4\"];\n   N --&gt; J;\n   N --&gt; L --&gt; K;\n   N --&gt; M --&gt; K;\n</code></pre>\n<p>The highest-feerate ancestor set in this graph is J (2/3=0.666), better than KL or KM (3/5=0.6), and better than JKLMN (8/13=0.615). Thus, in case the <span class=\"math\">f</span> function for <span class=\"math\">\\operatorname{LIMO}_f</span> is sufficiently restricted in search iterations, it is not unreasonable that it finds J as first <span class=\"math\">s</span>. In this case the resulting linearization is (J,K,L,M,N), with chunking [JKLM,N].</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/1X/617eebbb54650695e3a5a8d5092ef110813ffa8c.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/617eebbb54650695e3a5a8d5092ef110813ffa8c\" title=\"postproc\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/1X/617eebbb54650695e3a5a8d5092ef110813ffa8c_2_345x228.png\" alt=\"postproc\" data-base62-sha1=\"dUueXn1G1HXvblZ8ubZaS1s9Sws\" width=\"345\" height=\"228\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/1X/617eebbb54650695e3a5a8d5092ef110813ffa8c_2_345x228.png, https://delvingbitcoin.org/uploads/default/optimized/1X/617eebbb54650695e3a5a8d5092ef110813ffa8c_2_517x342.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/1X/617eebbb54650695e3a5a8d5092ef110813ffa8c_2_690x456.png 2x\" data-dominant-color=\"FAFAFA\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">postproc</span><span class=\"informations\">807\u00d7535 28.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>There is something silly about this result: JKLM is not even connected. It consists of two disconnected components J and KLM. Disconnected components can obviously be swapped without breaking topology. And in this case, doing so improves the feerate diagram, as shown above, but <span class=\"math\">\\operatorname{LIMO}</span> does not necessarily discover this.</p>\n<p>To address this, recall the <span class=\"math\">\\operatorname{Chunking}</span> algorithm:</p>\n<ul>\n<li><span class=\"math\">\\operatorname{Chunking}(C, L)</span>, for a cluster <span class=\"math\">C</span> with linearization <span class=\"math\">L</span>:\n<ul>\n<li>Set <span class=\"math\">R = []</span>, a list of sets of transactions.</li>\n<li>For <span class=\"math\">tx \\in L</span>:\n<ul>\n<li>Append the singleton <span class=\"math\">\\{tx\\}</span> to <span class=\"math\">R</span>.</li>\n<li>While <span class=\"math\">R</span> has two or more elements, and the feerate of the last one is higher than that of the penultimate one:\n<ul>\n<li>Replace the last two elements of <span class=\"math\">R</span> with their union.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Return <span class=\"math\">R</span>.</li>\n</ul>\n</li>\n</ul>\n<p>It only considers two options:</p>\n<ol>\n<li>Merging two sets (if the latter has a higher feerate).</li>\n<li>Deciding to leave two sets separate (if not).</li>\n</ol>\n<p>If we add a third option, namely swapping the two sets (if the latter has higher feerate, but does not depend on the first one), while also swapping the corresponding transactions in the linearization, all resulting chunks will necessarily be connected (because no two sets get merged that have no dependency). The result is the <span class=\"math\">\\operatorname{PostLin}</span> algorithm.</p>\n<ul>\n<li><span class=\"math\">\\operatorname{PostLin}(C, L)</span>, for a cluster <span class=\"math\">C</span> with linearization <span class=\"math\">L</span>:\n<ul>\n<li>Set <span class=\"math\">R = []</span>, a list of <em>lists</em> of transactions.</li>\n<li>For <span class=\"math\">tx \\in L</span>:\n<ul>\n<li>Append the single-element list <span class=\"math\">[tx]</span> to <span class=\"math\">R</span>.</li>\n<li>Let <span class=\"math\">cur</span> be this new single-element list.</li>\n<li>While <span class=\"math\">cur</span> is not the first list of <span class=\"math\">R</span>, and <span class=\"math\">cur</span> has a higher feerate than the list before it:\n<ul>\n<li>If <span class=\"math\">cur</span> includes a transaction that depends on one in the list before it:\n<ul>\n<li>Prepend this previous list to <span class=\"math\">cur</span>, and continue with it.</li>\n</ul>\n</li>\n<li>Otherwise, swap <span class=\"math\">cur</span> with this previous list, and continue with <span class=\"math\">cur</span>.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Return the concatenation of all <span class=\"math\">R</span> lists.</li>\n</ul>\n</li>\n</ul>\n<p>There may be up to <span class=\"math\">\\frac{n(n-1)}{2}</span> swap operations in the above algorithm, which makes it in the worst case <span class=\"math\">\\mathcal{O}(n^2)</span> (unlike <span class=\"math\">\\operatorname{Chunking}</span> which is <span class=\"math\">\\mathcal{O}(n)</span>). For this reason we do not use it for chunking itself, but leave it as a separate post-processing step that can improve linearizations (note that it returns a list of transactions, not a chunking).</p>\n<p>The algorithm is described here in a front-to-back manner, but it can also be done back-to-front. Both guarantee connected chunks, but provide other properties in addition to it. For example, the back-to-front variant has the following property. If we start from an existing linearization <span class=\"math\">L</span>, and move a leaf transaction (one with no children) of its cluster to the end of a linearization (and possibly increase its fee), then postlinearization in a back-to-front order guarantees an output linearization that is at least as good as <span class=\"math\">L</span>. This makes it particularly interesting before comparisons/mergings in RBF settings like the ones described in the \u201cDealing with suboptimality\u201d section above.</p>\n<h2><a name=\"h-4-implementations-18\" class=\"anchor\" href=\"#h-4-implementations-18\"></a>4. Implementations</h2>\n<p>The <span class=\"math\">\\operatorname{Merge}</span> algorithm is implemented as <code>MergeLinearizations()</code> in <a href=\"https://github.com/bitcoin/bitcoin/pull/30285\">PR #30285</a>. The cluster <span class=\"math\">C</span> and linearizations <span class=\"math\">L_1</span> and <span class=\"math\">L_2</span> are not actually modified during the loop, but instead a set <code>todo</code> is kept with <span class=\"math\">C</span> transactions that are left to do. Another optimization is that it avoids recomputing the chunking of linearizations when the <span class=\"math\">s</span> exactly matches either first chunk; instead, that chunk can just be cut off.</p>\n<p>The <span class=\"math\">\\operatorname{LIMO}_f</span> algorithm is implemented in the <code>Linearize()</code> function in <a href=\"https://github.com/bitcoin/bitcoin/pull/30126\">PR #30126</a>, where the <span class=\"math\">f</span> function is instantiated as returning the highest-feerate subset among:</p>\n<ol>\n<li>The highest-feerate remaining ancestor set: this closely matches what the current Bitcoin Core block building algorithm uses (since version 0.13.0 with <a href=\"https://github.com/bitcoin/bitcoin/pull/7600\">PR #7600</a> in 2016), and is just an efficient good first start.</li>\n<li>The highest-feerate prefix of what remains of <span class=\"math\">L</span> (at original chunk boundaries), as <span class=\"math\">\\operatorname{LIMO}</span> needs to compute that anyway.</li>\n<li>The highest-feerate topological subset found through <a href=\"https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303#h-2-finding-high-feerate-subsets-5\">search</a> with an iteration limit to bound its cost (an efficient implementation of this search algorithm is in <a href=\"https://github.com/bitcoin/bitcoin/pull/30286\">PR #30286</a>). The best among (1) and (2) above is used as initial best in this algorithm, allowing it to avoid parts of the search tree which can be proven not to be able to produce better results than that.</li>\n</ol>\n<p>The existing linearization is passed to the <code>Linearize()</code> function as <code>old_linearization</code>, which is allowed to be empty. If it is, <code>Linearize</code> simply implements the <span class=\"math\">\\operatorname{Lin}_f</span> algorithm, with the same <span class=\"math\">f</span> function (dropping option (2) above, as it does not apply). This way, <code>Linearize()</code> can be used both to construct an initial linearization or improve one.</p>\n<p>The <code>PostLinearize()</code> function implemented in <a href=\"https://github.com/bitcoin/bitcoin/pull/30285\">PR #30285</a> performs the <span class=\"math\">\\operatorname{PostLin}</span> operation twice: first in back-to-front order, and once again in front-to-back order.</p>\n<h2><a name=\"acknowledgements-19\" class=\"anchor\" href=\"#acknowledgements-19\"></a>Acknowledgements</h2>\n<p>Many of the results presented here are the result of discussions with various people, and this writeup received many comments and suggestions from them, including Suhas Daftuar (<a class=\"mention\" href=\"/u/sdaftuar\">@sdaftuar</a>), Anthony Towns (<a class=\"mention\" href=\"/u/ajtowns\">@ajtowns</a>), Greg Sanders (<a class=\"mention\" href=\"/u/instagibbs\">@instagibbs</a>), Mark Erhardt (<a class=\"mention\" href=\"/u/murch\">@murch</a>), Gloria Zhao (<a class=\"mention\" href=\"/u/glozow\">@glozow</a>), and others.</p>\n<h2><a name=\"code-for-diagram-generation-20\" class=\"anchor\" href=\"#code-for-diagram-generation-20\"></a>Code for diagram generation</h2>\n<details>\n<summary>\nCode to generate feerate diagrams</summary>\n<pre data-code-wrap=\"python\"><code class=\"lang-python\">import subprocess\n\ndef plot(filename, transactions, linearizations):\n    \"\"\"Generate PNG filename with fee-size diagrams for linearizations using transactions.\"\"\"\n    maxsize = 0\n    maxfee = 0\n    for line in linearizations:\n        if len(line) == 4:\n            maxsize = max(maxsize, sum(sum(transactions[txn][0] for txn in txnam) for txnam in line[1]))\n            maxfee = max(maxfee, sum(sum(transactions[txn][1] for txn in txnam) for txnam in line[1]))\n    width = (maxsize + 1) * 48 + 135\n    height = (maxfee + 1) * 48 + 103\n    labcount = 1\n\n    # Start gnuplot, and define function to send lines of code to it.\n    plot_process = subprocess.Popen(\"gnuplot\", stdin=subprocess.PIPE)\n    def send(code):\n        print(code)\n        plot_process.stdin.write((code + \"\\n\").encode())\n\n    # Common preamble\n    send(f\"set terminal pngcairo enhanced size {width},{height} font \\\"Verdana,15\\\"\")\n    send(f\"set output \\\"{filename}\\\"\")\n    send(\"set key bottom right\")\n    send(\"set size ratio -1\")\n    send(\"set grid x y\")\n    send(f\"set xtics 0, 1, {maxsize}\")\n    send(f\"set ytics 0, 1, {maxfee}\")\n    send(f\"set xrange [0:{maxsize + 1}]\")\n    send(f\"set yrange [0:{maxfee + 1}]\")\n    send(\"set xlabel \\\"cumulative size\\\"\")\n    send(\"set ylabel \\\"cumulative fee\\\"\")\n    send(f\"set label 1 \\\"\\\" at 0,0 point ps 2 pt 7 offset 0.4,-0.4\")\n\n    # Strings with arguments to plot command.\n    graphs = []\n    # At which positions dots have been placed already.\n    dots = set()\n    # Iterate over all linearizations\n    linnum = 0\n    for line in linearizations:\n        if len(line) == 4:\n            col, lin, ondots, offdots = line\n            # Compute the (size,fee,names) pairs of all chunks of the linearization.\n            chunks = [(0, 0, \"\")]\n            for txnam in lin:\n                nchunk = (sum(transactions[txn][0] for txn in txnam), sum(transactions[txn][1] for txn in txnam), txnam)\n                while len(chunks) &gt; 1 and nchunk[1] * chunks[-1][0] &gt; nchunk[0] * chunks[-1][1]:\n                    nchunk = (nchunk[0] + chunks[-1][0], nchunk[1] + chunks[-1][1], chunks[-1][2] + nchunk[2])\n                    chunks.pop()\n                chunks.append(nchunk)\n            # Compute the set of (size,fee) pairs of chunk prefixes.\n            diadots = set()\n            csize = 0\n            cfee = 0\n            for size, fee, txnams in chunks:\n                csize += size\n                cfee += fee\n                diadots.add((csize, cfee))\n            # Determine which dots to output.\n            cat = \"\"\n            accsize = 0\n            accfee = 0\n            for txnam in lin:\n                labcount += 1\n                cat += txnam\n                accsize += sum(transactions[txn][0] for txn in txnam)\n                accfee += sum(transactions[txn][1] for txn in txnam)\n                if (ondots and ((accsize, accfee) in diadots)) or (offdots and ((accsize, accfee) not in diadots)):\n                    if (accsize, accfee) not in dots:\n                        if accsize == maxsize:\n                            send(f\"set label {labcount} \\\"{cat}\\\" at {accsize},{accfee} right point ps 2 pt 7 offset -1.2,0.2\")\n                        else:\n                            send(f\"set label {labcount} \\\"{cat}\\\" at {accsize},{accfee} left point ps 2 pt 7 offset 0.4,-0.4\")\n                        dots.add((accsize, accfee))\n            # Create a data variable(s) for the diagram itself.\n            if isinstance(col, str):\n                send(f\"$data{linnum} &lt;&lt; EOD\")\n                acsize = 0\n                acfee = 0\n                chunknames = []\n                for csize, cfee, cnams in chunks:\n                    acsize += csize\n                    acfee += cfee\n                    chunknames.append(cnams)\n                    send(f\"{acsize} {acfee}\")\n                send(f\"EOD\")\n                # Add a graph line.\n                if isinstance(lin, list):\n                    graphs.append(f\"$data{linnum} with line lw 2 lc rgb \\\"{col}\\\" title \\\"({','.join(lin)})\\\"\")\n                else:\n                    graphs.append(f\"$data{linnum} with line lw 2 lc rgb \\\"{col}\\\" title \\\"[{','.join(chunknames[1:])}]\\\"\")\n            elif isinstance(col, list):\n                cnum = 0\n                acsize = 0\n                acfee = 0\n                for csize, cfee, cnams in chunks[1:]:\n                    send(f\"$data{linnum}_{cnum} &lt;&lt; EOD\")\n                    send(f\"{acsize} {acfee}\")\n                    acsize += csize\n                    acfee += cfee\n                    send(f\"{acsize} {acfee}\")\n                    send(\"EOD\")\n                    graphs.append(f\"$data{linnum}_{cnum} with line lw 2 lc rgb \\\"{col[cnum]}\\\" notitle\")\n                    cnum += 1\n            linnum += 1\n        elif len(line) == 2:\n            tsize = sum(transactions[txnam][0] for txnam in line[1])\n            tfee = sum(transactions[txnam][1] for txnam in line[1])\n            labcount += 1\n            send(f\"set label {labcount} \\\"{line[1]}\\\" at {tsize},{tfee} right point ps 2 pt 7 lc rgb \\\"{line[0]}\\\" offset -1.2,0.2 tc rgb \\\"{line[0]}\\\"\")\n    # Send a combined plot command with all graph lines.\n    send((\"plot \" + \", \".join(graphs)))\n    # Let gnuplot do its job.\n    plot_process.communicate()\n\ndef plots(diag):\n    \"\"\"Generate a number of feerate diagram PNGs described by diag.\"\"\"\n    for filename, linearizations in diag[\"diagrams\"].items():\n        plot(filename, diag[\"transactions\"], linearizations)\n        subprocess.run([\"pngcrush\", filename])\n\nDIAG1 = {\n    \"transactions\": {\n        \"A\": (2, 1),\n        \"B\": (1, 3),\n        \"C\": (1, 2),\n        \"D\": (3, 1),\n        \"E\": (1, 1),\n        \"F\": (1, 1),\n        \"G\": (1, 2),\n        \"H\": (3, 2),\n        \"I\": (1, 2),\n    },\n    \"diagrams\": {\n        \"diagram.png\": [\n            (\"dark-violet\", \"ABCDE\", True, True),\n        ],\n        \"incomparable.png\": [\n            (\"dark-violet\", \"ABCDE\", True, False),\n            (\"sea-green\", \"BADCE\", True, False),\n        ],\n        \"optimal.png\": [\n            (\"dark-grey\", \"ABCDE\", False, False),\n            (\"dark-grey\", \"BADCE\", False, False),\n            (\"red\", \"BACDE\", True, False)\n        ],\n        \"prefixes.png\": [\n            (\"dark-violet\", \"ABCDE\", True, True),\n            (\"sea-green\", \"BADCE\", True, True),\n        ],\n        \"full.png\": [\n            (\"red\", \"BACDE\", True, False),\n            (\"dark-yellow\", \"FGH\", True, False),\n            (\"purple\", \"I\", True, False),\n            ([\"red\", \"purple\", \"dark-yellow\", \"red\", \"dark-yellow\", \"red\"], \"BIFGACHDE\", True, False),\n            (\"blue\", \"BFG\"),\n        ],\n        \"partial.png\": [\n            (\"red\", \"BACDE\", True, False),\n            (\"dark-violet\", [\"BAC\", \"DE\"], True, True),\n            (\"cyan\", [\"BA\", \"CD\", \"E\"], True, True),\n            (\"sea-green\", [\"BACDE\"], True, True),\n        ],\n    },\n}\n\nDIAG2 = {\n    \"transactions\": {\n        \"S\": (4, 3),\n        \"T\": (5, 1),\n        \"U\": (1, 3),\n        \"V\": (1, 3),\n        \"W\": (4, 1)\n    },\n    \"diagrams\": {\n        \"improve_pre.png\": [\n            (\"dark-violet\", \"STWVU\", True, True),\n            (\"blue\", \"WU\"),\n        ],\n        \"improve_post.png\": [\n            (\"dark-violet\", \"STWVU\", False, False),\n            (\"sea-green\", \"WUSTV\", True, True),\n        ],\n        \"improve_opt.png\": [\n            (\"dark-grey\", \"STWVU\", False, False),\n            (\"dark-grey\", \"WUSTV\", False, False),\n            (\"red\", \"WSTVU\", True, True),\n        ],\n    },\n}\n\nDIAG3 = {\n    \"transactions\": {\n        \"J\": (3, 2),\n        \"K\": (4, 1),\n        \"L\": (1, 2),\n        \"M\": (1, 2),\n        \"N\": (4, 1),\n    },\n    \"diagrams\": {\n        \"postproc.png\": [\n            (\"dark-violet\", \"JKLMN\", True, False),\n            (\"sea-green\", \"KLMJN\", True, False),\n        ],\n    },\n}\n\nplots(DIAG1)\nplots(DIAG2)\nplots(DIAG3)\n</code></pre>\n</details>",
  "post_number": 1,
  "post_type": 1,
  "updated_at": "2024-07-09T20:23:11.915Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 23,
  "reads": 22,
  "readers_count": 21,
  "score": 178.8,
  "yours": false,
  "topic_id": 1032,
  "topic_slug": "introduction-to-cluster-linearization",
  "topic_title": "Introduction to cluster linearization",
  "topic_html_title": "Introduction to cluster linearization",
  "category_id": 8,
  "display_username": "Pieter Wuille",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "# Introduction to cluster linearization\n\n<div data-theme-toc=\"true\"> </div>\n\n## 1. Abstract\n\nThis document attempts to explain the background, reasoning, and complexities behind cluster linearization. I started writing this as reading material for those who want to review Bitcoin Core PRs [#30126](https://github.com/bitcoin/bitcoin/pull/30126) (the basic linearization/improvement algorithm) and [#30285](https://github.com/bitcoin/bitcoin/pull/30285) (post-processing and merging of linearizations), but it is probably a useful explanation in general.\n\nThings that are *not* covered here:\n* The reasons for wanting a cluster mempool based approach in the first place. See [this topic](https://delvingbitcoin.org/t/an-overview-of-the-cluster-mempool-proposal/393) for that.\n* More rigorous theory and proofs. See [this topic](https://delvingbitcoin.org/t/cluster-mempool-definitions-theory/202) for that.\n* A detailed walk-through of the code in the PRs. This document just discusses abstract algorithms, but there are many implementation details needed to make them efficient. I believe those are best documented as comments in the code itself.\n* Timeline, organization, or status of the involved PRs. See the [Tracking Issue](https://github.com/bitcoin/bitcoin/issues/30289) for that.\n* As will become clear further, the most computationally expensive part of linearization is finding high-feerate subsets of a cluster to move to the front. That aspect is treated as black box here, but is largely covered in [this topic](https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303).\n\n## 2. Basics\n\nBriefly summarized, the cluster mempool proposal consists of:\n* New data structures:\n  * **Clusters**: The set of mempool transactions is partitioned into groups known as *clusters* based on their connectivity. In order to bound the computational cost of operations on these clusters (see further), a *cluster count limit* is introduced, replacing the ancestor and descendant count limits.\n  * **Linearizations**: For every cluster, a *linearization* is precomputed and maintained at all times. This is the order in which the transactions of that cluster would be mined, which is independent of other clusters. The goal of this document is explaining how we can find and maintain these linearizations.\n  * **Chunks**: When higher-feerate transactions follow lower-feerate transactions in a linearization, they will naturally be included in blocks together, as a generalization of child-pays-for-parent (as the higher ones help pay for the lower ones). These groupings are known as the *chunks* of the linearization, and can be precomputed along with the linearizations themselves.\n\n* Affected node operations:\n  * **Transaction relay**: When new transactions are relayed, they will become new clusters, attach to existing clusters, or even merge multiple existing clusters. When this happens, the linearizations for those clusters need to be updated and/or recomputed.\n  * **Block validation**: When new blocks are relayed and accepted, the mempool needs to be updated to account for the resulting changes (removing mined and conflicted transactions, and in the case of reorganizations, moving transactions back to the mempool). This will generally involve a bulk update to the mempool's linearizations.\n  * **Block building**: Having precomputed linearizations and chunks for all mempool clusters, the construction of a block template for mining consists of repeatedly including the highest-feerate remaining first chunk across all clusters, until the block is (close to) full.\n  * **Eviction**: Eviction (removal of mempool transactions when the mempool grows too large) becomes the exact opposite of block building: removing the chunk which would be included in a block last, which is simply the lowest-feerate last chunk across all clusters.\n  * **Replace-by-fee evaluation**: In order to determine whether accepting an RBF transaction is an improvement to the mempool, we can directly compare the quality of the linearization of the affected clusters before and after, by comparing the cumulative-size-vs-cumulative-fee diagram of both (called the *feerate diagram*). See [this topic](https://delvingbitcoin.org/t/post-clustermempool-package-rbf-per-chunk-processing/190) for more information about (package) RBF and cluster mempool.\n  * **Fee estimation**: fee estimation can benefit from having access to chunk feerates of the transactions involved, which act as effective mining scores. See [this topic](https://delvingbitcoin.org/t/package-aware-fee-estimator-post-cluster-mempool/312).\n\nBelow we expand on some of these concepts.\n\n### 2.1. Clusters and linearizations\n\nConsider the following example mempool, with 9 transactions, each labeled \"name: fee/size\". The arrows point up from children to their parents (so, e.g. transaction D spends an output of A and an output of B):\n```mermaid height=236,auto\ngraph BT\n   C[\"C: 2/1\"] --> A[\"A: 1/2\"];\n   D[\"D: 1/3\"] --> A;\n   D --> B[\"B: 3/1\"];\n   E[\"E: 1/1\"] --> D;\n   G[\"G: 4/1\"] --> F[\"F: 1/1\"];\n   H[\"H: 3/4\"] --> F;\n   I[\"I: 2/1\"];\n```\n\nClusters are defined as groups of transactions that are reachable from one another using any combination of is-parent-of and is-child-of relations. In the mempool above there are three clusters: ABCDE, FGH, and I. Here, and in what follows, names like FGH are a short notation for \"the set of transactions containing F, G, and H\".\n\nFor these clusters, at all times, a *linearization* (the order those transactions are to be mined in w.r.t. each other) is maintained. A valid linearization requires that all parents appear before their children. Still, there can be significant differences in quality between linearizations for the same cluster (see below). The idea is that whenever transactions are added or removed from clusters, the linearization for it is updated and/or recomputed. For the ABCDE cluster above, valid linearizations include (A,B,C,D,E), (B,A,D,C,E), and (A,C,B,D,E), but not (B,D,A,C,E) (D before A).\n\nThe *prefixes* of a linearization are all sets consisting of consecutive elements of the linearization from the start up to every point. For the (A,C,B,D,E) linearization, the prefixes are {A, AC, ACB, ACBD, ACBDE}.\n\nTo make sure some reasonable quality of linearization is always achievable within acceptable transaction relay latency, the cluster mempool proposal involves introducing a *cluster count limit* (in number of transactions), replacing the ancestor set and descendant set count limits.\n\nIt turns out that knowing the linearization of every cluster is sufficient to quickly construct good block templates for a given mempool. Furthermore, it lets us quickly determine what effective feerate a transaction will be mined at, even without actually constructing a block.\n\n### 2.2. Feerate diagrams and chunking\n\nTo analyze linearizations, we make use of the *feerate diagram*, here demonstrated for the (A,B,C,D,E) linearization for the cluster shown above:\n\n![diagram|529x500, 50%](upload://mea7JGdJlqVoDvMEgJ4owg45Hvu.png)\n\nIn a diagram where the X-axis represents the cumulative transaction size (which in practice could mean vsize or weight), and the Y-axis represents the cumulative transaction fee, place points for each prefix of the linearization (black dots). Then draw the [convex hull](https://en.wikipedia.org/wiki/Convex_hull) line of those points (purple line). The resulting line is the feerate diagram, which gives an approximation of how much fee can be collected within any given size. Each of its segments will correspond to one or more transactions; we call those groups of transactions the *chunks* of the linearization. The slope of the line segment corresponds to the *chunk feerate* (total fee divided by total size) of that chunk. Collectively, the chunks of a linearization are its *chunking*.\n\nThe example has two chunks, with chunking [ABC,DE]. Note the use of square brackets for chunkings, and parentheses for linearizations.\n\nThe *prefixes* of a chunking are, similar to the prefixes of a linearization, the sets formed as the unions of all chunks from the beginning of the chunking up to every point. The prefixes of [ABC,DE] are {ABC, ABCDE}. These prefixes correspond to the diagram points that form the chunk boundaries.\n\nIt turns out that the chunking can be computed using the following algorithm:\n* $\\operatorname{Chunking}(C, L)$, for a cluster $C$ with linearization $L$:\n  * Set $R = []$, a list of sets of transactions.\n  * For $tx \\in L$:\n    * Append the singleton $\\{tx\\}$ to $R$.\n    * While $R$ has two or more elements, and the feerate of the last one is higher than that of the penultimate one:\n      * Replace the last two elements of $R$ with their union.\n  * Return $R$.\n\nWorking out our example, we get the following steps:\n* Add A: [A]\n* Add B: [A,B]\n  * B's feerate is higher than A's, combine them: [AB]\n* Add C: [AB,C]\n  * C's feerate is higher than AB's, combine them: [ABC]\n* Add D: [ABC,D]\n  * D's feerate is *not* higher than ABC's, stop the while loop: [ABC,D]\n* Add E: [ABC,D,E]\n  * E's feerate is higher than D's, combine them: [ABC,DE]\n  * DE's feerate is *not* higher than ABC's, stop the while loop: [ABC,DE]\n\nNext we analyze the complexity of this algorithm. Note that here and in what follows, the complexities assume that set operations (including intersections, unions, subtractions, ...) are $\\mathcal{O}(1)$. This is not true in an asymptotic sense (operations on larger sets are necessarily $\\mathcal{O}(n)$, just due to reading the set), but is largely true in practice as the implementation uses bitsets (of bounded size) to represent sets.\n\nThe $\\operatorname{Chunking}$ algorithm consists of two nested loops, but the inner loop can run at most once per element of $L$, making it have only $\\mathcal{O}(n)$ iterations. If we additionally cache the combined fee and size of the sets in $R$, the cost per iteration is $\\mathcal{O}(1)$, and the result is an $\\mathcal{O}(n)$ algorithm, which is so fast that we can generally ignore the distinction between linearizations and their chunkings. For some operations the linearization is more convenient, and for others the chunking is; we will proceed as if both are always available, even though it may mean recomputing the chunking from time to time.\n\nAn important property is that the first chunk of a linearization has the highest feerate/slope of any prefix of the linearization, and further chunks have non-increasing feerates/slope. This must be the case: if not, a higher-slope line would follow a lower-slope line, which would be in contradiction with the convex hull shape of the feerate diagram.\n\nIn a way, chunks can be seen as a generalization of [child-pays-for-parent](https://bitcoinops.org/en/topics/cpfp/): whenever children pay a higher feerate than their ancestors, they may get chunked together with them, effectively pooling their fees to pay for their joint size. The effective feerate, or mining score, of a transaction is the feerate of the chunk it is part of. This effective feerate can be higher or lower than a transaction's individual feerate: it is lower when it is paying for lower-feerate transaction in the same chunk, and it is higher when another higher-feerate transaction in the same chunk is paying for it.\n\n### 2.3. Comparing linearizations\n\nOur goal is constructing and maintaining good linearizations for our clusters, but we need a metric to know what to optimize for. To do so, we compare their feerate diagrams for every X-axis (size) coordinate. Depending on how the diagram lines compare, two linearizations can be **equivalent** (coinciding), **better** (above), **worse** (below), or **incomparable** (both above and below).\n\nFor example, (A,B,C,D,E) and (B,A,D,C,E) are incomparable, because both are above the other one for some sizes:\n\n![incomparable|529x500, 50%](upload://f7JjSI5ABMR5GPDiGwm3IKvkWqm.png)\n\nWhile the convex-hull aspect of the feerate diagram results in diagrams only being an approximation for the quality of a linearization (by smoothing out fees over chunks), it is one with a very important property: **every cluster has an *optimal* linearization which is equivalent to or better than every valid linearization of the same cluster**. There can be multiple optimal linearizations that are all equivalent, but they will all be strictly better than all non-optimal linearizations. This implies that when faced with two incomparable linearizations, a third one always exists that is strictly better than both.\n\nFor our ABCDE example, that optimal linearization is (B,A,C,D,E), with chunking [B,AC,DE]. It is clearly better than both of our earlier linearizations (A,B,C,D,E) and (B,A,D,C,E):\n\n![optimal|529x500, 50%](upload://Ak36c6lTEAwwH4mqXQygz3KdzJq.png)\n\nNote that since the names of the points in the feerate diagram simply represent sets of transactions without implying an order, BAC is the same as ABC, and BACDE is the same as ABCDE. They are rendered here in linearization order for readability.\n\nFurthermore, it can be shown that an optimal linearization can be constructed using the following algorithm:\n* $\\operatorname{OptLin}(C)$, for a transaction cluster $C$:\n  * Set $R = []$, an empty list of transactions.\n  * While there are transactions left in $C$:\n    * Find the highest-feerate topological (=including all its own not yet included ancestors) subset $s \\subset C$, using $\\operatorname{OptSubset}$ (see further).\n    * Append the elements of $s$ to $R$ in any valid order, and remove them from $C$.\n  * Return $R$.\n\nThe $\\operatorname{OptSubset}$ algorithm is not described in this post, but the approach in Section 2 of [How to linearize your cluster](https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303#h-2-finding-high-feerate-subsets-5) can be used to implement it with $\\mathcal{O}(n \\cdot 2^{n/2})$ complexity. Due to the exponential nature, this approach is not computationally feasible for every cluster size we want to support. Still, it is a good starting point for the algorithms that follow.\n\nBesides acting as a criterion to define better and worse linearizations for a given cluster, feerate diagram comparison can also be used to determine which of two *clusters* is better, or even two whole mempools. This is relevant in RBF (replace by fee) settings, where we want to determine whether a transaction replacement is [incentive compatble](https://delvingbitcoin.org/t/mempool-incentive-compatibility/553). Specific ideas on how to leverage cluster mempool based feerate diagram comparisons in a (package) RBF setting are explored in [this topic](https://delvingbitcoin.org/t/post-clustermempool-package-rbf-per-chunk-processing/190).\n\n### 2.4. Block building and eviction\n\nWhen building block templates from a mempool, or when evicting from a mempool, we need to look at all clusters, rather than just one. To reason about that, notice that nothing prevents us from defining a linearization or its chunking for the entire mempool. We do not need to actually compute this full-mempool chunking, but we can still reason about its properties.\n\nOne important property is that given optimal linearizations' chunkings for all clusters in the mempool, the optimal chunking of the entire mempool consists of a \"merge sort\" of the chunks of the individual clusters. That is, include the chunks one by one from all clusters, from high to low feerate. Since chunks within a cluster are already in decreasing feerate order, the result will respect the individual cluster linearization orders. When there are multiple chunks of equal feerate within one linearization, their order has to be maintained of course. This merge-sort property is what makes cluster mempool \"work\": the fact that transactions do not affect the linearizations beyond their own clusters.\n\n![full|490x499, 75%](upload://sZnP52PPeM03bMtwEyFLj3d9BpL.png)\n\nFor our example mempool, the optimal chunkings are [B,AC,DE], [FG,H], and [I]. The optimal linearization of the entire mempool is (B,I,F,G,A,C,H,D,E) with chunking [B,I,FG,AC,H,DE], which is just the chunks from all clusters combined and sorted.\n\nEven though we cannot guarantee that our linearizations are optimal, the approach is still usable for building blocks: we pick however many of the first chunks of the whole-mempool chunking fit. This can be done by consecutively picking the next highest-feerate remaining first chunk among all cluster linearizations, until the next one is too large to fit.\n\nEven if the cluster linearizations are optimal, this approach does not guarantee that the resulting block is optimal (in terms of maximizing total fees), due to bin packing effects near the end of the block. However, a weaker property does hold: **if the cluster linearizations are optimal, then the prefixes of their merged chunkings have the maximum possible fee for their own size**. Concretely, in our example here, that means for all sets of size 1 (B), 2 (BI), 4 (BIFG), 7 (BIFGAC), 10 (BIFGACH), and 14 (BIFGACHDE), no sets exist with a higher fee at the same or smaller size than these. In other words, *if* the block vsize limit happens to be exactly equal to the size of one of those prefixes, and the cluster linearizations are optimal, the block fee will be maximal.\n\nIf the block vsize limit does not match the size of a prefix of chunks, the total fee may be lower than the highest possible one. Still, the feerate diagram of an optimal linearizations does provide an upper bound on how many fees are achievable in each size.\n\nIn our example, if the block vsize limit were (for presentation purposes) just 3, then based on the diagram (knowing it is optimal) we can tell that no more than 6+\u00bd fee is possible (the fee of the point where the BI-BIFG line crosses size 3) with any topological set of transactions. The approach of picking chunks as long as they fit would yield 5 fee (the BI point), but the actual optimal has 6 fee (BFG or BFI). This difference is limited by the unfilled size of the block multiplied by the chunk feerate of the first chunk that does not fit. Because of this, we want a limit on the size (in vbytes) of clusters (in addition to the cluster count limit), as it limits the fee loss caused by this. Some of the loss of block template fees due to this effect can still be mitigated by e.g. skipping chunks too large and trying further ones, or searching different combinations at block building time, but it is hard to guarantee much this way.\n\nNote that all of the above is about block template fees that are lost just due to end-of-block size effects, which exist even when all cluster linearizations are optimal. If the linearizations are suboptimal, the losses due to that will compound.\n\nGiven the insight that the (implicit) chunking for the entire mempool is the sorted combination of all individual cluster chunkings, and block building is (roughly) picking the beginning of that chunking, it is easy to reverse the process for eviction. When the mempool grows too large, we find the lowest-feerate *last* chunk among all cluster chunkings, and evict that, as it would be the last thing we would include in a block template.\n\n## 3. Maintaining linearizations\n\nWith some of the basics covered, we dive into how cluster linearizations are constructed and maintained.\n\n### 3.1. Dealing with suboptimality\n\nDue to the exponential nature of the $\\operatorname{OptSubset}$ algorithm, we have to accept that for some clusters we may not always know an optimal linearization. Let us turn $\\operatorname{OptLin}$ into a $\\operatorname{Lin}$ algorithm that replaces $\\operatorname{OptSubset}$ with an arbitrary $f$ function that finds a high-feerate non-empty topological subset of what it is passed:\n* $\\operatorname{Lin}_f(C)$, for a transaction cluster $C$, and a function $f$ that returns a non-empty topological subset, preferably with a high feerate:\n  * Set $R = []$, an empty list of transactions.\n  * While there are transactions left in $C$:\n    * Let $s = f(C)$.\n    * Append the elements of $s$ to $R$ in any topological order, and remove them from $C$.\n  * Return $R$.\n\nWhile suboptimality does mean a degradation of block template quality, this is a minor effect only, as long as the result is close. There are more serious complications however. For example, in the RBF setting it makes it possible that we conclude an objectively worse replacement is an improvement and accept it, or vice versa. [Imagine](https://delvingbitcoin.org/t/post-clustermempool-package-rbf-per-chunk-processing/190/14) a complicated cluster for which network nodes are generally unable to find the optimal linearization. An attacker-constructed replacement may objectively be a degradation then, but if it simplifies the cluster enough that the optimal linearization does get discovered, network nodes may still treat it as an improvement.\n\nThis highlights the need for not just cluster linearization quality in an absolute sense, but also in a relative one: ideally, if cluster A is better than cluster B when linearized optimally, we still recognize A as better than B even without optimal linearizations. This is not a fully addressable problem, but there are algorithms that can help improve the situation.\n\nSpecifically, we can instead of only using algorithms that compute (hopefully, good) linearizations from scratch, also use algorithms that take as input one or more existing linearizations, and produce a new linearization that is equivalent or better than each input linearization. Three algorithms will be described:\n* $\\operatorname{Merge}(C, L_1, L_2)$: given a cluster $C$ and two linearizations for it, produce a new linearization that is equivalent to or better than both. In particular, if the two input linearizations are incomparable, this implies their merging will be strictly better than both. See [this topic](https://delvingbitcoin.org/t/merging-incomparable-linearizations/209) for history. Complexity: $\\mathcal{O}(n^2)$.\n* $\\operatorname{LIMO}_f(C, L)$: given a cluster $C$ and linearization for it, improve it by finding high-feerate subsets to move closer to the front using $f$. It is a variant of the $\\operatorname{Lin}_f$ algorithm above, but crucially guarantees that the resulting linearization is equivalent to or better than the input linearization. See [this topic](https://delvingbitcoin.org/t/limo-combining-the-best-parts-of-linearization-search-and-merging/825) for history. Complexity: depends on how much time is spent in $f$, but at least $\\mathcal{O}(n^2)$.\n* $\\operatorname{PostLin}(C, L)$: given a cluster $C$ and a linearization for it, possibly make some \"obvious\" improvements to the linearization which are not guaranteed by merging or LIMO. See [this topic](https://delvingbitcoin.org/t/linearization-post-processing-o-n-2-fancy-chunking/201) for history. Complexity: $\\mathcal{O}(n^2)$.\n\nThe actual possible approaches using these algorithms are countless, but here are some example of how these algorithms can be employed when evaluating RBF replacements. Imagine a new transaction $a$ (\"add\") arrives that would attach to an existing cluster $C$, which has an existing linearization, and where it conflicts with an old transaction $d$ (\"delete\").\n* We can compute an updated cluster $C'$ (with $a$ added, and $d$ deleted), compute a linearization for $C'$ from scratch using $\\operatorname{Lin}$, and compare the feerate diagrams of the old and new linearizations to determine if the result is an improvement.\n* However, it is possible that significant effort had already been spent coming up with a good linearization for $C$ before $a$ arrived. In order to not throw that away, it is possible to first construct a \"naive\" relinearization by taking the old one, removing $d$ from it, and appending $a$ at the end, and then using $\\operatorname{LIMO}$ on that instead. Applying $\\operatorname{PostLin}$ on that naive relinearization first can make the process better still.\n* Even with this, it is possible that $\\operatorname{LIMO}$ discovers an improvement in the new linearization that was actually already possible in the old one, but just was not known. In this case, it would be useful to \"backport\" that knowledge to the old linearization before comparing. To do so, we can take the resulting new cluster's linearization, remove $a$ and reinsert $d$ again, and then use $\\operatorname{Merge}$ to combine it with the original old linearization, possibly after another $\\operatorname{PostLin}$.\n\nEven in a non-replacement setting, $\\operatorname{LIMO}$ and $\\operatorname{PostLin}$ are useful for updating linearizations after a transaction is accepted.\n\n### 3.2. Improving linearizations\n\nBefore digging into the actual algorithms, it is useful to gain some insight into what exactly it is that determines whether changes to a linearization constitute an improvement.\n\n#### 3.2.1 Naively moving higher-feerate sets\n\nOne might think that moving any topological subset with a feerate higher than the first chunk of a linearization to the front would necessarily be an improvement to its feerate diagram.\n\nConsider this simpler example, where transactions are labelled \"name: fee\" (they all have size 1):\n\n```mermaid height=168,auto\ngraph RL\n   S[\"S: 1\"];\n   T[\"T: 1\"];\n   U[\"U: 1\"];\n   V[\"V: 2\"];\n   W[\"W: 3\"];\n   V --> T --> S;\n   V --> W;\n   U --> W; \n```\n\nThe linearization (S,T,W,V,U) has chunking [STWV,U], whose first chunk STWV has feerate 7/4=1.75:\n\n![improve_pre|395x500, 50%](upload://o4GxKGgFl77YDTrzCwsc9H74LPm.png)\n\nNow consider the topological subset WU, with feerate 4/2=2. Despite having a higher feerate than the first chunk STWV of (S,T,W,V,U), moving it to the front is *not* a strict improvement:\n\n![improve_post|395x500, 50%](upload://A3EQHzsP0W1LHuA81AtDbZPxqyk.png)\n\nBy moving WU to the front, W becomes a new chunk on its own, and U merges with what follows. The resulting USTV chunk is actually worse in some places than the old linearization.\n\nThus, when moving a topological subset to the front of a linearization, it clearly does not suffice that the moved set has a higher feerate than the first chunk. Another condition is needed to prevent the moved subset from splitting up and worsening further parts.\n\n#### 3.2.2 Guaranteed non-worsening\n\nImagine a very simple linearization $L$ whose chunking consists of just two chunks, $u$ and $(C \\setminus u)$, which necessarily implies that the feerate of $u$ is not below that of $(C \\setminus u)$. Now consider a topological subset $s$ of the cluster. We want to know under what conditions moving $s$ to the front of the linearization will be an improvement.\n\nIt turns out that sufficient conditions for non-worsening are:\n* 1. The feerate of $s$ needs to be at least as high as that of $u$.\n* 2. The intersection $s \\cap u$ needs to not exceed the feerate of $s$, or this intersection must be empty.\n\nIf additionally $s$ has a strictly higher feerate than $u$, moving it will be a strict improvement.\n\nThese are not necessary conditions (moving $s$ may be a non-worsening/improvement even if both do not hold), but they are sufficient, and moreover they will turn out to be easy to satisfy. A proof for a more general statement can be found [here](https://delvingbitcoin.org/t/limo-combining-the-best-parts-of-linearization-search-and-merging/825/8#h-13-the-set-gathering-theorem-4).\n\nAnd this gives a hint for the solution already: if we ever consider moving a higher-feerate set to the front of a linearization, but that set has an even-higher-feerate intersection with a prefix of what we already have, then we should move that intersection to the front instead. In our example there is indeed such an intersection (W), and if we were to move that intersection instead to the front, we do get a strictly improved chunking [W,STV,U]:\n\n![improve_opt|395x500, 50%](upload://fyHogkMQjSpvMNwIkVNvXHRjnbL.png)\n\n#### 3.2.3. Composing topological sets\n\nWhenever we are improving things, we will be building a new linearization while simultaneously guaranteeing that certain points on the feerate diagram which we know how to hit, are actually hit. In our original example, when considering these two linearizations:\n\n![prefixes|529x500, 50%](upload://erCaDpmQJGnPA14OtTg0hFWwxn8.png)\n\nAny linearization whose feerate diagram goes through or above all the labeled points (corresponding to the prefixes of both linearizations) in the diagram will necessarily be as good or better as both the (A,B,C,D,E) and (B,A,D,C,E) linearizations. It turns out that a strategy exists to do this generically: given any number of topologically valid sets $u_1, u_2, \\ldots, u_k$ which together include each cluster transaction at least once (in our example, the $u_i$ sets would be A, B, AB, ABC, ABD, ABCD, and ABCDE), we can construct a linearization whose diagram goes through or above all of their (size, fee) points as follows:\n\n* $\\operatorname{Compose}(C, \\{u_1, u_2, \\ldots, u_k\\})$, for cluster $C$ and topological subsets $u_i$ which together cover all of $C$:\n  * Let $R = []$, the returned linearization.\n  * While $C$ is not empty:\n    * Find a topological subset $s$ whose feerate is at least the maximum of all $u_i$'s feerates, and such that no $s \\cap u_i$ has an even higher feerate.\n    * Append $s$ to $R$ in any valid order, and remove it from $C$, and from all $u_i$.\n  * Return $R$.\n\nWays to find $s$ such that the necessary properties are fulfilled will follow later, we can already give an intuition for why this strategy works.\n\nConsider an equivalent algorithm that instead operates on linearizations $L_i$ that each consist of just two chunks $[u_i, C \\setminus u_i]$. In each iteration it finds a subset $s$ satisfying the conditions above, and constructs $k$ new linearizations by moving $s$ to the front of all old $L_i$ ones. Since the two conditions for non-worsening are satisfied, each new linearization is at least as good as its corresponding old linearization. The algorithm then continues doing the same on the non-$s$ part of the new linearizations. Ultimately, all the same sets $s$ have been moved to the front, so the final linearizations are all the same, and thus this common output linearization is at least as good as every input linearization. Since for each $u_i$ there was an input linearization whose feerate diagram goes through its (size, fee) point, the diagram of the resulting linearization goes through or above all those points too.\n\nThis explanation is not exact, because there may be sets $u_i$ for which it is impossible to construct a linearization $L_i$ that has $u_i$ as its first chunk (and everything else as one other chunk). Still, a [somewhat more rigorous](https://delvingbitcoin.org/t/cluster-mempool-definitions-theory/202/14) treatment shows that this $\\operatorname{Compose}$ approach works for any set of topological $u_i$ sets.\n\nTo find each $s$ set, we make use of what was hinted at already. Start with the highest-feerate $u_i$, but when that set has a non-empty higher-feerate intersection with any of the other $u_i$ sets, switch to considering that intersection. Then try intersecting that with all other $u_i$ sets again (even ones tried already), as another intersection may make it better yet, and keep doing that until no improvement is made anymore:\n\n* $\\operatorname{Compose}(C, \\{u_1, u_2, \\ldots, u_k\\})$, for cluster $C$ and topological subsets $u_i$ which together cover all of $C$:\n  * Let $R = []$, the returned linearization.\n  * While $C$ is not empty:\n    * Set $s$ to be the highest-feerate $u_i$.\n    * While $s$ is being improved:\n      * For every $u_i$, if $s \\cap u_i$ is non-empty and has a higher feerate than $s$, update $s = s \\cap u_i$, and restart the while loop.\n    * Append $s$ to $R$ in any valid order, and remove it from $C$, and from all $u_i$.\n  * Return $R$.\n\nTo determine the complexity of this, note that:\n* The other \"While $C$ is not empty\" loop may run up to $n$ times.\n* The \"While $s$ is being improved\" loop may run up to $\\operatorname{min}(n, k)$ times.\n* The \"For every $u_i$\" loop will run $k$ times.\n* By caching the feerates of intersections between $s$ and every $u_i$, the number of feerate operations done in the inner loop is limited to $nk$ per iteration of the \"While $C$ is not empty\" loop.\n\nThis means the overall algorithm complexity is $\\mathcal{O}(n^2k)$. The practical specializations of it that follow will do better, though.\n\n### 3.3. Practical algorithms\n\n#### 3.3.1. Merge\n\nThe goal when merging two linearizations is constructing a linearization whose feerate diagram is for every size on or above both input linearization diagrams. As shown in the previous section, this can be accomplished by running the generic $\\operatorname{Compose}$ algorithm with all prefixes of both input linearizations as $u_i$ sets. Doing so would have $\\mathcal{O}(n^3)$ complexity, and we can do better.\n\nInstead of using the general approach from the previous section for finding each $s$ set, we can use a more specialized variant with better efficiency. The culprit is the need to restart the search for better intersections with the $u_i$ sets every time $s$ improves, which turns out, can be avoided. All we need is a way to find an $s$ each time that satisfies the two conditions:\n* 1. The feerate of $s$ is at least as high as every prefix of the remainder of the two linearizations.\n* 2. The feerate of the intersection between $s$ and any linearization remainder prefix.\n\n![prefixes|529x500, 50%](upload://erCaDpmQJGnPA14OtTg0hFWwxn8.png)\n\nOne key insight is that many of the linearization prefixes are redundant. Any feerate diagram that hits or exceeds the convex hull vertices of the input linearization (B and ABCDE for the first; ABC and ABCDE for the second) will also hit or exceed every other point. In other words, it suffices to include in the $u_i$ sets only the *chunk* prefixes rather than the linearization prefixes.\n\nThis is not a full solution: we have reduced the problem to searching prefixes of what remains of the *original* chunks rather than all prefixes of what remains, but the restarting remains necessary. However, no harm is done by switching to the *actual* chunking of what remains of the linearization in the specific iteration of $\\operatorname{Compose}$ we are in instead, as that will be at least as good as the remainder of the $u_i$ sets after removing $s$. And with that, the loop is no longer needed:\n\n* $\\operatorname{Merge}(C, L_1, L_2)$:\n  * Let $R = []$\n  * While $C$ has transactions left:\n    * Compute the chunkings of what remains of both linearizations (using $\\operatorname{Chunking}$).\n    * Let $h$ be the highest-feerate first chunk of the two (remember that the first chunk of a linearization is its highest-feerate prefix).\n    * For every prefix $p$ of the *chunking* of the *other* linearization (the one which $h$ is not the first chunk of), from short to long:\n      * If the intersection $h \\cap p$ is non-empty and has higher-feerate than $h$ itself, set $s = h \\cap p$ and stop.\n    * If the loop above did not find any better intersection, set $s = h$.\n    * Append $s$ in any valid order to $R$, remove its transactions from $C$ and $L_1$ and $L_2$.\n  * Return $R$.\n\nThis works, because $h$ in each iteration starts off being already the shortest possible prefix of the chunkings of what remains of one of the two linearizations, so whatever happens with intersections with the other linearization, there is never a need to go back to trying intersections with the chunks of the first one. And there is no point continuing either with intersections with the other one either: $s$ is already a subset of all of the longer (later) ones, and the shorter (earlier) ones have been tried before, so cannot be an improvement (if one were better, it would already have been picked instead).\n\nRegarding complexity, the while loop can run at most $\\mathcal{O}(n)$ times, and each iteration performs a $\\operatorname{Chunking}$ which is also $\\mathcal{O}(n)$, as is the cost of iterating over the prefixes $p$. Together, this means an overall complexity of $\\mathcal{O}(n^2)$.\n\n**The existence of the merging algorithm proves that every cluster has an optimal linearization.** Take every valid linearization, and merge them one by one together to obtain a merging of all them. The result is necessarily as good as every linearization, and thus optimal.\n\n#### 3.3.2. LIMO\n\nGoing back to the naive linearization algorithm from earlier:\n* $\\operatorname{Lin}_f(C)$, for a transaction cluster $C$, and a function $f$ for finding high-feerate topological subsets:\n  * Set $R = []$, an empty list of transactions.\n  * While there are transactions left in $C$:\n    * Let $s = f(C)$.\n    * Append the elements of $s$ to $R$ in any valid order, and remove them from $C$.\n  * Return $R$.\n\nOur goal is constructing an (efficient) variant of this that also takes in an existing linearization, while also invoking $f$ to find good subsets to move to the front. Naively, this can be accomplished by instead of building an $R$ from scratch, starting from the existing linearization $L$ and then for every set $s$ found invoke $\\operatorname{Merge}$ to merge $L$ with a version of itself that has $s$ moved to the front. This is where the name LIMO comes from: \"Linearization through Incremental Merging of Optimizations\".\n\nThe same behavior can be achieved more efficiently however:\n\n* $\\operatorname{LIMO}_f(C, L)$, for a transaction cluster $C$, an existing linearization $L$, and a function $f$ that returns a non-empty topological subset, preferably with a high feerate:\n  * Set $R = []$, an empty list of transactions.\n  * While there are transactions left in $C$:\n    * Let $s$ be the highest-feerate prefix of what remains of $L$.\n    * If $f(C)$ has a higher feerate than $s$:\n      * Set $s = f(C)$.\n      * For every prefix $p$ of what remains of $L$ from short to long:\n        * If the intersection $s \\cap p$ has an even higher feerate: set $s$ to that and stop.\n    * Append the elements of $s$ to $R$ in any valid order, and remove them from $C$ and $L$.\n  * Return $R$.\n\nThis is effectively a variant of $\\operatorname{Compose}$ again where the $u_i$ sets are chosen to be the prefixes of the original linearization $L$, and with a specialized $s$-finding step that additionally guarantees a feerate not below that of whatever $f(C)$ is at that point. It satisfies the improvement properties for similar reasons as the $\\operatorname{Merge}$ algorithm: (1) $s$ starts off with a feerate not below the highest-feerate prefix, and (2) no further intersections with prefixes can improve it (shorter ones have been tried already, longer ones it is already a subset of). Therefore $R$ will be at least as good as $L$.\n\nIt is possible to restrict each initial $s$ as well as the intersections to prefixes at the chunk boundaries of $L$. Unlike for $\\operatorname{Merge}$, there is no second linearization whose intersections may trigger a restart with intersections of the first linearization, so it suffices to only consider the *original* chunk boundaries (from before any $s$ was removed from $L$). It would not hurt to re-chunk, but it is also not necessary here.\n\n* $\\operatorname{LIMO}_f(C, L)$, for a transaction cluster $C$, an existing linearization $L$, and a function $f$ that returns a non-empty topological subset, preferably with a high feerate:\n  * Set $R = []$, an empty list of transactions.\n  * Compute the chunking of $L$.\n  * While there are transactions left in $C$:\n    * Let $s$ be the highest-feerate prefix of what remains of $L$, at the (original) chunk boundaries.\n    * If $f(C)$ has a higher feerate than $s$:\n      * Set $s = f(C)$.\n      * For every prefix $p$ of what remains of $L$, at the (original) chunk boundaries, from short to long:\n        * If the intersection $s \\cap p$ has an even higher feerate: set $s$ to that and stop.\n    * Append the elements of $s$ to $R$ in any valid order, and remove them from $C$ and $L$ (and optionally recompute chunking of what remains of $L$).\n  * Return $R$.\n\nIf $f(C)$ is cheap, the algorithm's complexity is dominated by the inner $p$ loop, which may run up to $\\mathcal{O}(n^2)$ times. If $f(C)$ is expensive, its complexity will dominate.\n\n#### 3.3.3 Post-linearization\n\nConsider the following example, with transactions labelled \"name: fee/size\":\n\n```mermaid height=236,auto\ngraph BT\n   J[\"J: 2/3\"];\n   K[\"K: 1/4\"];\n   L[\"L: 2/1\"];\n   M[\"M: 2/1\"];\n   N[\"N: 1/4\"];\n   N --> J;\n   N --> L --> K;\n   N --> M --> K;\n```\n\nThe highest-feerate ancestor set in this graph is J (2/3=0.666), better than KL or KM (3/5=0.6), and better than JKLMN (8/13=0.615). Thus, in case the $f$ function for $\\operatorname{LIMO}_f$ is sufficiently restricted in search iterations, it is not unreasonable that it finds J as first $s$. In this case the resulting linearization is (J,K,L,M,N), with chunking [JKLM,N].\n\n![postproc|690x457, 50%](upload://dUueXn1G1HXvblZ8ubZaS1s9Sws.png)\n\nThere is something silly about this result: JKLM is not even connected. It consists of two disconnected components J and KLM. Disconnected components can obviously be swapped without breaking topology. And in this case, doing so improves the feerate diagram, as shown above, but $\\operatorname{LIMO}$ does not necessarily discover this.\n\nTo address this, recall the $\\operatorname{Chunking}$ algorithm:\n\n* $\\operatorname{Chunking}(C, L)$, for a cluster $C$ with linearization $L$:\n  * Set $R = []$, a list of sets of transactions.\n  * For $tx \\in L$:\n    * Append the singleton $\\{tx\\}$ to $R$.\n    * While $R$ has two or more elements, and the feerate of the last one is higher than that of the penultimate one:\n      * Replace the last two elements of $R$ with their union.\n  * Return $R$.\n\nIt only considers two options:\n1. Merging two sets (if the latter has a higher feerate).\n2. Deciding to leave two sets separate (if not).\n\nIf we add a third option, namely swapping the two sets (if the latter has higher feerate, but does not depend on the first one), while also swapping the corresponding transactions in the linearization, all resulting chunks will necessarily be connected (because no two sets get merged that have no dependency). The result is the $\\operatorname{PostLin}$ algorithm.\n\n* $\\operatorname{PostLin}(C, L)$, for a cluster $C$ with linearization $L$:\n  * Set $R = []$, a list of *lists* of transactions.\n  * For $tx \\in L$:\n    * Append the single-element list $[tx]$ to $R$.\n    * Let $cur$ be this new single-element list.\n    * While $cur$ is not the first list of $R$, and $cur$ has a higher feerate than the list before it:\n      * If $cur$ includes a transaction that depends on one in the list before it:\n        * Prepend this previous list to $cur$, and continue with it.\n      * Otherwise, swap $cur$ with this previous list, and continue with $cur$.\n  * Return the concatenation of all $R$ lists.\n\nThere may be up to $\\frac{n(n-1)}{2}$ swap operations in the above algorithm, which makes it in the worst case $\\mathcal{O}(n^2)$ (unlike $\\operatorname{Chunking}$ which is $\\mathcal{O}(n)$). For this reason we do not use it for chunking itself, but leave it as a separate post-processing step that can improve linearizations (note that it returns a list of transactions, not a chunking).\n\nThe algorithm is described here in a front-to-back manner, but it can also be done back-to-front. Both guarantee connected chunks, but provide other properties in addition to it. For example, the back-to-front variant has the following property. If we start from an existing linearization $L$, and move a leaf transaction (one with no children) of its cluster to the end of a linearization (and possibly increase its fee), then postlinearization in a back-to-front order guarantees an output linearization that is at least as good as $L$. This makes it particularly interesting before comparisons/mergings in RBF settings like the ones described in the \"Dealing with suboptimality\" section above.\n\n## 4. Implementations\n\nThe $\\operatorname{Merge}$ algorithm is implemented as `MergeLinearizations()` in [PR #30285](https://github.com/bitcoin/bitcoin/pull/30285). The cluster $C$ and linearizations $L_1$ and $L_2$ are not actually modified during the loop, but instead a set `todo` is kept with $C$ transactions that are left to do. Another optimization is that it avoids recomputing the chunking of linearizations when the $s$ exactly matches either first chunk; instead, that chunk can just be cut off.\n\nThe $\\operatorname{LIMO}_f$ algorithm is implemented in the `Linearize()` function in [PR #30126](https://github.com/bitcoin/bitcoin/pull/30126), where the $f$ function is instantiated as returning the highest-feerate subset among:\n1. The highest-feerate remaining ancestor set: this closely matches what the current Bitcoin Core block building algorithm uses (since version 0.13.0 with [PR #7600](https://github.com/bitcoin/bitcoin/pull/7600) in 2016), and is just an efficient good first start.\n2. The highest-feerate prefix of what remains of $L$ (at original chunk boundaries), as $\\operatorname{LIMO}$ needs to compute that anyway.\n3. The highest-feerate topological subset found through [search](https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303#h-2-finding-high-feerate-subsets-5) with an iteration limit to bound its cost (an efficient implementation of this search algorithm is in [PR #30286](https://github.com/bitcoin/bitcoin/pull/30286)). The best among (1) and (2) above is used as initial best in this algorithm, allowing it to avoid parts of the search tree which can be proven not to be able to produce better results than that.\n\nThe existing linearization is passed to the `Linearize()` function as `old_linearization`, which is allowed to be empty. If it is, `Linearize` simply implements the $\\operatorname{Lin}_f$ algorithm, with the same $f$ function (dropping option (2) above, as it does not apply). This way, `Linearize()` can be used both to construct an initial linearization or improve one.\n\nThe `PostLinearize()` function implemented in [PR #30285](https://github.com/bitcoin/bitcoin/pull/30285) performs the $\\operatorname{PostLin}$ operation twice: first in back-to-front order, and once again in front-to-back order.\n\n## Acknowledgements\n\nMany of the results presented here are the result of discussions with various people, and this writeup received many comments and suggestions from them, including Suhas Daftuar (@sdaftuar), Anthony Towns (@ajtowns), Greg Sanders (@instagibbs), Mark Erhardt (@murch), Gloria Zhao (@glozow), and others.\n\n## Code for diagram generation\n\n[details=\"Code to generate feerate diagrams\"]\n```python\nimport subprocess\n\ndef plot(filename, transactions, linearizations):\n    \"\"\"Generate PNG filename with fee-size diagrams for linearizations using transactions.\"\"\"\n    maxsize = 0\n    maxfee = 0\n    for line in linearizations:\n        if len(line) == 4:\n            maxsize = max(maxsize, sum(sum(transactions[txn][0] for txn in txnam) for txnam in line[1]))\n            maxfee = max(maxfee, sum(sum(transactions[txn][1] for txn in txnam) for txnam in line[1]))\n    width = (maxsize + 1) * 48 + 135\n    height = (maxfee + 1) * 48 + 103\n    labcount = 1\n\n    # Start gnuplot, and define function to send lines of code to it.\n    plot_process = subprocess.Popen(\"gnuplot\", stdin=subprocess.PIPE)\n    def send(code):\n        print(code)\n        plot_process.stdin.write((code + \"\\n\").encode())\n\n    # Common preamble\n    send(f\"set terminal pngcairo enhanced size {width},{height} font \\\"Verdana,15\\\"\")\n    send(f\"set output \\\"{filename}\\\"\")\n    send(\"set key bottom right\")\n    send(\"set size ratio -1\")\n    send(\"set grid x y\")\n    send(f\"set xtics 0, 1, {maxsize}\")\n    send(f\"set ytics 0, 1, {maxfee}\")\n    send(f\"set xrange [0:{maxsize + 1}]\")\n    send(f\"set yrange [0:{maxfee + 1}]\")\n    send(\"set xlabel \\\"cumulative size\\\"\")\n    send(\"set ylabel \\\"cumulative fee\\\"\")\n    send(f\"set label 1 \\\"\\\" at 0,0 point ps 2 pt 7 offset 0.4,-0.4\")\n\n    # Strings with arguments to plot command.\n    graphs = []\n    # At which positions dots have been placed already.\n    dots = set()\n    # Iterate over all linearizations\n    linnum = 0\n    for line in linearizations:\n        if len(line) == 4:\n            col, lin, ondots, offdots = line\n            # Compute the (size,fee,names) pairs of all chunks of the linearization.\n            chunks = [(0, 0, \"\")]\n            for txnam in lin:\n                nchunk = (sum(transactions[txn][0] for txn in txnam), sum(transactions[txn][1] for txn in txnam), txnam)\n                while len(chunks) > 1 and nchunk[1] * chunks[-1][0] > nchunk[0] * chunks[-1][1]:\n                    nchunk = (nchunk[0] + chunks[-1][0], nchunk[1] + chunks[-1][1], chunks[-1][2] + nchunk[2])\n                    chunks.pop()\n                chunks.append(nchunk)\n            # Compute the set of (size,fee) pairs of chunk prefixes.\n            diadots = set()\n            csize = 0\n            cfee = 0\n            for size, fee, txnams in chunks:\n                csize += size\n                cfee += fee\n                diadots.add((csize, cfee))\n            # Determine which dots to output.\n            cat = \"\"\n            accsize = 0\n            accfee = 0\n            for txnam in lin:\n                labcount += 1\n                cat += txnam\n                accsize += sum(transactions[txn][0] for txn in txnam)\n                accfee += sum(transactions[txn][1] for txn in txnam)\n                if (ondots and ((accsize, accfee) in diadots)) or (offdots and ((accsize, accfee) not in diadots)):\n                    if (accsize, accfee) not in dots:\n                        if accsize == maxsize:\n                            send(f\"set label {labcount} \\\"{cat}\\\" at {accsize},{accfee} right point ps 2 pt 7 offset -1.2,0.2\")\n                        else:\n                            send(f\"set label {labcount} \\\"{cat}\\\" at {accsize},{accfee} left point ps 2 pt 7 offset 0.4,-0.4\")\n                        dots.add((accsize, accfee))\n            # Create a data variable(s) for the diagram itself.\n            if isinstance(col, str):\n                send(f\"$data{linnum} << EOD\")\n                acsize = 0\n                acfee = 0\n                chunknames = []\n                for csize, cfee, cnams in chunks:\n                    acsize += csize\n                    acfee += cfee\n                    chunknames.append(cnams)\n                    send(f\"{acsize} {acfee}\")\n                send(f\"EOD\")\n                # Add a graph line.\n                if isinstance(lin, list):\n                    graphs.append(f\"$data{linnum} with line lw 2 lc rgb \\\"{col}\\\" title \\\"({','.join(lin)})\\\"\")\n                else:\n                    graphs.append(f\"$data{linnum} with line lw 2 lc rgb \\\"{col}\\\" title \\\"[{','.join(chunknames[1:])}]\\\"\")\n            elif isinstance(col, list):\n                cnum = 0\n                acsize = 0\n                acfee = 0\n                for csize, cfee, cnams in chunks[1:]:\n                    send(f\"$data{linnum}_{cnum} << EOD\")\n                    send(f\"{acsize} {acfee}\")\n                    acsize += csize\n                    acfee += cfee\n                    send(f\"{acsize} {acfee}\")\n                    send(\"EOD\")\n                    graphs.append(f\"$data{linnum}_{cnum} with line lw 2 lc rgb \\\"{col[cnum]}\\\" notitle\")\n                    cnum += 1\n            linnum += 1\n        elif len(line) == 2:\n            tsize = sum(transactions[txnam][0] for txnam in line[1])\n            tfee = sum(transactions[txnam][1] for txnam in line[1])\n            labcount += 1\n            send(f\"set label {labcount} \\\"{line[1]}\\\" at {tsize},{tfee} right point ps 2 pt 7 lc rgb \\\"{line[0]}\\\" offset -1.2,0.2 tc rgb \\\"{line[0]}\\\"\")\n    # Send a combined plot command with all graph lines.\n    send((\"plot \" + \", \".join(graphs)))\n    # Let gnuplot do its job.\n    plot_process.communicate()\n\ndef plots(diag):\n    \"\"\"Generate a number of feerate diagram PNGs described by diag.\"\"\"\n    for filename, linearizations in diag[\"diagrams\"].items():\n        plot(filename, diag[\"transactions\"], linearizations)\n        subprocess.run([\"pngcrush\", filename])\n\nDIAG1 = {\n    \"transactions\": {\n        \"A\": (2, 1),\n        \"B\": (1, 3),\n        \"C\": (1, 2),\n        \"D\": (3, 1),\n        \"E\": (1, 1),\n        \"F\": (1, 1),\n        \"G\": (1, 2),\n        \"H\": (3, 2),\n        \"I\": (1, 2),\n    },\n    \"diagrams\": {\n        \"diagram.png\": [\n            (\"dark-violet\", \"ABCDE\", True, True),\n        ],\n        \"incomparable.png\": [\n            (\"dark-violet\", \"ABCDE\", True, False),\n            (\"sea-green\", \"BADCE\", True, False),\n        ],\n        \"optimal.png\": [\n            (\"dark-grey\", \"ABCDE\", False, False),\n            (\"dark-grey\", \"BADCE\", False, False),\n            (\"red\", \"BACDE\", True, False)\n        ],\n        \"prefixes.png\": [\n            (\"dark-violet\", \"ABCDE\", True, True),\n            (\"sea-green\", \"BADCE\", True, True),\n        ],\n        \"full.png\": [\n            (\"red\", \"BACDE\", True, False),\n            (\"dark-yellow\", \"FGH\", True, False),\n            (\"purple\", \"I\", True, False),\n            ([\"red\", \"purple\", \"dark-yellow\", \"red\", \"dark-yellow\", \"red\"], \"BIFGACHDE\", True, False),\n            (\"blue\", \"BFG\"),\n        ],\n        \"partial.png\": [\n            (\"red\", \"BACDE\", True, False),\n            (\"dark-violet\", [\"BAC\", \"DE\"], True, True),\n            (\"cyan\", [\"BA\", \"CD\", \"E\"], True, True),\n            (\"sea-green\", [\"BACDE\"], True, True),\n        ],\n    },\n}\n\nDIAG2 = {\n    \"transactions\": {\n        \"S\": (4, 3),\n        \"T\": (5, 1),\n        \"U\": (1, 3),\n        \"V\": (1, 3),\n        \"W\": (4, 1)\n    },\n    \"diagrams\": {\n        \"improve_pre.png\": [\n            (\"dark-violet\", \"STWVU\", True, True),\n            (\"blue\", \"WU\"),\n        ],\n        \"improve_post.png\": [\n            (\"dark-violet\", \"STWVU\", False, False),\n            (\"sea-green\", \"WUSTV\", True, True),\n        ],\n        \"improve_opt.png\": [\n            (\"dark-grey\", \"STWVU\", False, False),\n            (\"dark-grey\", \"WUSTV\", False, False),\n            (\"red\", \"WSTVU\", True, True),\n        ],\n    },\n}\n\nDIAG3 = {\n    \"transactions\": {\n        \"J\": (3, 2),\n        \"K\": (4, 1),\n        \"L\": (1, 2),\n        \"M\": (1, 2),\n        \"N\": (4, 1),\n    },\n    \"diagrams\": {\n        \"postproc.png\": [\n            (\"dark-violet\", \"JKLMN\", True, False),\n            (\"sea-green\", \"KLMJN\", True, False),\n        ],\n    },\n}\n\nplots(DIAG1)\nplots(DIAG2)\nplots(DIAG3)\n```\n[/details]",
  "actions_summary": [
    {
      "id": 2,
      "count": 4
    }
  ],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 96,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "reactions": [
    {
      "id": "+1",
      "type": "emoji",
      "count": 2
    },
    {
      "id": "heart",
      "type": "emoji",
      "count": 2
    }
  ],
  "current_user_reaction": null,
  "reaction_users_count": 4,
  "current_user_used_main_reaction": false
}