{
  "id": 6011,
  "name": "ZmnSCPxj jxPCSnmZ",
  "username": "ZmnSCPxj",
  "avatar_template": "/letter_avatar_proxy/v4/letter/z/ee7513/{size}.png",
  "created_at": "2025-10-07T17:50:26.165Z",
  "cooked": "<p>Title: Persisting Mutable Storage Inside The \"T\"EE</p>\n<h1><a name=\"p-6011-introduction-1\" class=\"anchor\" href=\"#p-6011-introduction-1\"></a>Introduction</h1>\n<p>A Trusted Execution Environment is a scam where a hardware manufacturer,\nin coordination with a cloud service provider, convinces victims that\nthey can trust somebody else\u2019s computer to actually run normal code,\nwithout use of homomorphic encryption, exactly as the victim intends\nthe normal code to be run.\nThis is done by distracting the victim with a certificate authority\nattesting to a secret key hidden in the hardware, which (1) the hardware\nmanufacturer claims it does not know, despite having placed it in the\nhardware in the first place, and (2) the cloud service provider claims\nnot to have extracted the secret key from the hardware, despite having\ndirect physical possession of the hardware, <strong>and</strong> the ability to\nvirtualize anything, including execution of said hardware with its\nsupposedly-not-extracted secret key.</p>\n<p>All such existing \"T\"EE scams have an additional flaw that the scammers\nalso distract from their victims: they do not have persistent mutable\nstorage.\nAny persistent mutable storage <em><strong>MUST</strong></em> be external to any \"T\"EE, and\n<em><strong>MUST</strong></em> be trusted to not perform \u201crollback attacks\u201d.</p>\n<p>A \u201crollback attack\u201d is simply that a persistent mutable storage,\nrunning outside the \"T\"EE, can keep backup copies of old state, and\nrecover the old state.\nWhen the \"T\"EE application is restarted, the persistent mutable storage\nrecovers from the backup old state.</p>\n<p>Rollback attacks are disastrous for many Bitcoin applications:</p>\n<ol>\n<li>If the \"T\"EE application is a Lightning node, if it is given old\nstate, it can be convinced to publish a unilateral close on old\nstate, which is indistuingishable from a theft attempt.\nThe peer can then get all the funds in the channels, effectively\nstealing by claiming the victim <em>is</em> the real thief.</li>\n<li>If the \"T\"EE application is a signer for a statechain scheme,\nold states can effectively retain supposedly-deleted keys.\nThere is <em><strong>NO</strong></em> possible proof-of-ignorance that the statechain\nsigner can present to prove that there are no backup copies of\nold \u201cdeleted\u201d keys, because <em><strong>by itself</strong></em> it cannot even know\nif it is a victim of a rollback attack.</li>\n</ol>\n<p>Note that encrypting the stored data \u201cat rest\u201d <em><strong>DOES NOT</strong></em> protect\nagainst rollback attacks \u2014 the rollback simply requires that old\ndata be available to the attacker, not that the attacker can decrypt\nit.</p>\n<p>In this writeup, I will show how to synthesize a persistent mutable\nstorage that can be audited to not be rollback-capable, using\nmultiple \"T\"EEs running the same program <em>in addition to</em> the main\n\"T\"EE application they serve.</p>\n<h1><a name=\"p-6011-all-things-must-pass-2\" class=\"anchor\" href=\"#p-6011-all-things-must-pass-2\"></a>All Things Must Pass</h1>\n<p>The key insight is this:</p>\n<ul>\n<li>Even an HDD is ephemeral and not persistent.\nOne day, it, too, shall die and all its stored data lost \u2014\nforever.</li>\n<li>Yet a ZFS array containing that HDD is permanent.\nYou simply hotswap the dead HDD with a fresh one, and let ZFS\nkeep on going on, because ZFS is <em><strong>awesome</strong></em>.</li>\n</ul>\n<p>Thus, from ephemeral storage, persistence is achievable: this\nHDD, too, shall come to pass, but the end of the ZFS array is not\nyet.</p>\n<h2><a name=\"p-6011-a-persistent-array-of-tee-ephemeral-memories-3\" class=\"anchor\" href=\"#p-6011-a-persistent-array-of-tee-ephemeral-memories-3\"></a>A Persistent Array Of \"T\"EE Ephemeral Memories</h2>\n<p>We can consider that \u201ceven if an HDD is so big and strong and\ntough, one day it will stop being able to read anyway\u201d, then\nperhaps the dinky little ephemeral RAM of a \"T\"EE can serve\njust as well in an array of \"T\"EE ephemeral RAMs?</p>\n<p>The only requirement then is that we distribute the \"T\"EEs\nwidely, so that it is unlikely that so many of them go down\nthat permanent data loss is possible.\nWe then use suitable erasure coding so that we can recover\neven if some number of \"T\"EEs go down and lose the contents\nof their ephemeral RAMs.</p>\n<p>Note that part of the \"T\"EE scam is to convince victims that\nthe actual RAM used is <em>also</em> encrypted with the hardware\nattestation key.\nThus, there is no need to encrypt data at rest, as the \"T\"EE\nhardware claims it <em>already</em> encrypts the data in memory, and\nour \u201cpersistent\u201d storage is in fact the RAM inside the \"T\"EE.\nWe do still need to encrypt data in transit between the main\n\"T\"EE application and all of the \"T\"EE storage programs.</p>\n<p>Thus, we have multiple \"T\"EE storage programs, which the main\n\"T\"EE puts in a resilient array, achieving persistence on top\nof ephemeral storages.</p>\n<h3><a name=\"p-6011-a-digression-on-terminology-4\" class=\"anchor\" href=\"#p-6011-a-digression-on-terminology-4\"></a>A Digression On Terminology</h3>\n<p>Traditionally, discussions about redundant arrays of\ninexpensive storage devices use the term \u201cblock devices\u201d that\nstore and retrieve \u201cblocks\u201d of data.</p>\n<p>Unfortunately for our context \u2014 cryptocurrency network\napplications \u2014 \u201cblock\u201d is much too overloaded a\nword:</p>\n<ol>\n<li>Bitcoin blocks of transactions.</li>\n<li>Symmetric encryption schemes working on blocks of data.</li>\n<li>Network communications getting blocked.</li>\n<li>Using blocking vs nonblocking APIs on network sockets.</li>\n<li>IP (Internet Protocol) blocks.</li>\n<li>IP (Intellectual Property) blocks which prevent some\nerasure coding schemes from being adopted.</li>\n<li>Block, the company that funds Spiral, and which itself\nsells various Block devices such as Square terminals.</li>\n</ol>\n<p>Thus, I will use old, obsolete terminology that nobody uses\ntoday:</p>\n<ul>\n<li>We have \u201cdisks\u201d, which are storage devices, instead of\nthose newfangled \u201cblock devices\u201d.\n<ul>\n<li>It does <em><strong>NOT</strong></em> matter whether it is spinning rust,\ntrapped electrons in capacitors, or DRAM that we misuse\nas \u201cpersistent\u201d storage in the disk array.\nI will call them all \u201cdisks\u201d nevertheless.</li>\n</ul>\n</li>\n<li>The storage device is divided into \u201csectors\u201d, instead of\nthose newfangled \u201cblocks\u201d.</li>\n</ul>\n<h1><a name=\"p-6011-sector-size-5\" class=\"anchor\" href=\"#p-6011-sector-size-5\"></a>Sector Size</h1>\n<p>Traditionally, disks presented 512-byte sectors (even more\ntraditionally, 128-byte sectors in single-density floppies\n\u2014 is my age showing yet).\nHowever, modern disks now present 4096-byte sectors; some\nflash-based disks even want to present larger sectors, up to\n32768 bytes.</p>\n<p>Unfortunately, as we intend to use multiple \"T\"EEs as disks,\nwe have to connect them via Internet Protocol, most likely\nvia TCP or TLS (itself running on TCP).</p>\n<p>And generally, the common MSS for TCP is about ~1400 bytes.</p>\n<p>Both the 512-byte and the 4096-byte options just do not\nfit well with the MSS of ~1400 bytes.\n512 bytes would require us to <code>TCP_NODELAY</code> to deliver the\nsector data ASAP with a smaller-than-max packet, while 4096\nbytes would fragment the data into multiple packets.</p>\n<p>Ideally, our sector size would be 1024, as that just about\nfits the MSS for TCP, and can fit a little more data, such\nas MACs for transmission integrity as well as any IVs for\nencryption for in-transit data.</p>\n<p>Unfortunately, if we were emulating a disk in the main\napplication \"T\"EE (actually an array of multiple remote \"T\"EE\ndisks), that we then pass to a \u201creal\u201d filesystem like XFS (or\ndirectly to some database engine that works on disks instead\nof filesystem), then those will usually assume sector sizes\nof 4096 bytes (and might be configurable to use sector sizes\nof 512 bytes).\nEven if we work with 1024-byte sectors when communicating\nwith the remote \"T\"EE disks, the overlying file system will\nattempt to write 4096-byte sectors which would require\nmultiple packets anyway, or 512-byte sectors which would\nrequire read-modify-write operations on the main application\n\"T\"EE and add <em>more</em> overhead on each 512-byte write.</p>\n<p>So, we should stick with normal 4096-byte sector sizes,\nand just accept that it will take 3 packets in general to\ntransmit them plus any cryptography overhead for integrity\nand encrypted-in-transit requirements.</p>\n<h1><a name=\"p-6011-raid5-write-hole-6\" class=\"anchor\" href=\"#p-6011-raid5-write-hole-6\"></a>RAID5 Write Hole</h1>\n<p>The RAID5 write hole happens due to the fact that writing\nto multiple disks in parallel is <em><strong>NOT</strong></em> an atomic\noperation.</p>\n<p>It\u2019s possible that the application is able to send out\ndata to <em>some</em> disks, then crash, with only part of a\nstripe of data being written.\nThis leaves the on-disk data, <em>together with the extra\nparity data needed by erasure coding</em>, inconsistent,\nwith no real way to determine which data is \u201creal\u201d.\nPartial stripe writes are particularly bad because\nif a RAID5 write hole occurs and <em>then</em> a disk is lost,\nthe recovered data for the lost disk can be completely\ndifferent from <em>both</em> the \u201cold\u201d data and the \u201cnew\u201d data!</p>\n<p>This is called the \u201cRAID5\u201d write hole simply because\nit was <em>already</em> observed when using erasure coding\nschemes where at most one disk can be lost, i.e. RAID5\nXOR parities.\nHowever, it applies to <em>all</em> uses of erasure coding,\nincluding RAID6 with 2 parity disks or \u201cgeneralized\nRAID6\u201d where there are 3 or more parity disks.</p>\n<p>Fortunately, since this is a novel array scheme\nanyway, we can fix the RAID5 write hole by simply\nchanging the disk interface (this is not possible with\n\u201creal\u201d spinning-rust or electrons-in-capacitor disks,\nof course):</p>\n<ul>\n<li>There is no \u201cwrite to sector\u201d command.</li>\n<li>There are instead three commands:\n<ul>\n<li>\u201cprovisional write to sector\u201d, where the disk\nstores both the old version of the sector, and\nthe new data for the sector.</li>\n<li>\u201ccommit provisional write\u201d where the disk discards\nthe old version of the sector and keeps the new\ndata.</li>\n<li>\u201crollback provisional write\u201d where the disk\ndiscards the new version of the sector and keeps\nthe old data.</li>\n</ul>\n</li>\n<li>At most one provisional write can be in-flight.\nIf the disk is already storing a provisional write\nfor one sector, and another provisional write (to\nany sector, not just this one) is done, then the\ncommand errors.\n<ul>\n<li>This means we only need to sacrifice one sector of\nexpensive RAM for the provisional write scheme.</li>\n</ul>\n</li>\n<li>On reads, the disk reports both the old and new\ndata for sectors with a pending provisional write.</li>\n<li>There is also a status command to check if the disk\nhas a provisional write for a sector, as well as to\nreport the sector index for the sector in provisional\nwrite.</li>\n</ul>\n<p>When the array-management code wants to update <em>one</em>\nsector in the array:</p>\n<ol>\n<li>It reads all sectors in the same stripe (this can be\ncached).</li>\n<li>It calculates the new parity data for the new data.</li>\n<li>It sends out provisional write requests to the\ndestination storage disk and each of the parity disks.</li>\n<li>It waits until all disks have either responded, or\ntimed out.\n<ul>\n<li>In case of a time out, it permanently records that\ndisk as \u201cdead\u201d.\nThis record can be placed in a special sector of\nall disks, with a strict monotonic version number\nseen by the disk, and RAID1-replicated across all\ndisks, which is always directly replaced if the\nreplacement has a monotonically higher version\nnumber.\nIf the highest-versioned death record marks a disk\nas dead, then the array code never reads or writes\nto that disk ever again, treating all writes as\nsuccesses and all reads as recoverable using\nerasure code recovery of the remaining disks, and\nreporting the degraded state to the operator.</li>\n<li>In particular, if there are N storage disks and P\nparity disks, and we are updating ONE sector on a\nstorage disk, we are writing to P + 1 disks, and\nup to P of them can time out and be marked as\ndead, and the array will still consider itself as\n\u201cdegraded but hanging on\u201d.</li>\n</ul>\n</li>\n<li>It sends out commit commands to the destination\nstorage disk and each of the parity disks (at least\nthe live ones that did not time out in step 4).</li>\n<li>It waits until all disks have either responded, or\ntimed out.</li>\n</ol>\n<p>The array code may crash at any time, and on restart,\nhas to check for pending provisional writes.\nIt looks for the highest-versioned death record to\ndetermine which disks it still considers alive, and\nthen checks for pending provisional writes.</p>\n<ul>\n<li>If the array crashed between steps 3 and 4, some\nwrites may have been received by the disk, but some\ndisks might not have received them.</li>\n<li>If the array crashed between steps 4 and 6, some\ncommits may have been received by the disk, but\nsome disks might not have received them.</li>\n</ul>\n<p>The array thus needs to differentiate between the above\ntwo cases, by checking if taking the new version as\ntrue leads to a consistent parity.\nIf the new version leads to a consistent parity, it\nmeans it got past step 4 and can commit everything.\nIf the new version does not lead to a consistent parity,\nit means it was in between step 3 and 4 and has to\nroll back everything.</p>\n<h1><a name=\"p-6011-xor-optimization-7\" class=\"anchor\" href=\"#p-6011-xor-optimization-7\"></a>XOR Optimization</h1>\n<p>Some erasure coding schemes have an XOR optimization\nproperty:</p>\n<ul>\n<li>In case of a partial stripe write, instead of reading\nthe unwritten storage sectors of the stripe, we can\nread the <em>written</em> storage sector(s), XOR the old\nversion and the new version, set all the <em>unwritten</em>\nstorage sectors in-memory as zero, then run the\nnormal parity-calculate code.\nThe resulting parities can then be XORed with the\nexisting parities-to-be-overwritten to result in a\nconsistent stripe of the new version.</li>\n</ul>\n<p>The XOR optimization above means we do not have to\nread the entire stripe to write a few sectors;\nassuming N storages and we want to write W sectors\nof a stripe, if W &lt; N - W then doing the XOR\noptimization means reading fewer sectors.</p>\n<p>We can thus add a special provisional-write command\nthat, instead of taking the new version of the data,\ntakes a \u201cdelta\u201d sector.\nThis provisional-write-delta command then reads the\nold version sector data, XORs the delta sector,\nand the result is the \u201cnew\u201d sector for purposes of\nthe provisional write.</p>\n<p>The above provisional-write-delta command is used for\nwriting to parity sectors when using the XOR\noptimization.</p>\n<h1><a name=\"p-6011-generalized-raid6-8\" class=\"anchor\" href=\"#p-6011-generalized-raid6-8\"></a>Generalized RAID6</h1>\n<p>The free and open source world currently has a 2-parity\nerasure code scheme implemented in ZFS, with similar\ncode written in the Linux kernel (for <code>md</code>), using\n<code>PSHUFB</code> SIMD instruction.\nThe performance is generally pretty good <em><strong>if</strong></em> the\nprocessor has <code>PSHUFB</code>.\nHowever, for 3-parity and above, even the <code>PSHUFB</code>\nSIMD instruction cannot be used to speed up the\nparity calculation and erasure recovery code for the\nthird parity sector.\nRaidZ3 simply accepts the efficiency loss, while the\nLinux kernel <code>md</code> simply does not support more than 2\nparities.</p>\n<p>However, given how much more fragile our disks are in\nthis scheme, we really want to have a nice efficient\ngeneralized erasure coding scheme that supports much\nmore than 2 parities.\nUnfortunately, faster erasure coding schemes are\npatent-encumbered, i.e. blocked by IP law.</p>\n<p>We <em>do</em> have a generalized scheme, based on Reed\nSolomon codes and Galois Field math.\nThis is <em>usually</em> very slow since Galois Field math\noften requires moving individual bits around, and\nalso not very amenable to SIMD, but because of this,\nit was never patent-encumbered.</p>\n<p>And then there is this <a href=\"https://www.researchgate.net/publication/2643899_An_XOR-Based_Erasure-Resilient_Coding_Scheme\" rel=\"noopener nofollow ugc\">XOR-based EC paper</a>.</p>\n<p>The key insight of this paper is:</p>\n<ul>\n<li>Traditionally, we have been using bytes as\n<code>GF(2^8)</code> field elements.</li>\n<li>However, we can rotate our view:\n<ul>\n<li>One byte is actually 8 bits from 8 different\n<code>GF(2^8)</code> field elements.</li>\n<li>So for example, byte index 0 is actually the\n0th bit of 8 different <code>GF(2^8)</code> elements,\nbyte index 1 is actually 1st bit of the same\n8 different <code>GF(2^8)</code> elements\u2026 byte index\n7 is actually 7th bit of the same 8 different\n<code>GF(2^8)</code>elements.</li>\n<li>Thus, XORing a byte is actually an 8-wide\nSIMD instruction that works on individual bits\nof multiple <code>GF(2^8)</code> elements at a time.</li>\n</ul>\n</li>\n</ul>\n<p>The above rotation of our view means that even\n***non-***SIMD reads, XORs, and writes of bytes,\nwords, doublewords, or quadwords, are actually\nsemantically the same as SIMD reads, XORs, and\nwrites of individual bits of multiple <code>GF(2^8)</code>\nelements at a time.</p>\n<p>This creates a <em>generalized</em> Reed Solomon code\nusing Galois Field math that <strong>is not slow</strong>, and\nyet remains patent-unencumbered due to Reed Solomon\nbeing traditionally viewed as based on really\nslow-for-computers math.</p>\n<p>Scouring the githubs, I found an <a href=\"https://github.com/raid5atemyhomework/llrfs\" rel=\"noopener nofollow ugc\">abandoned project</a>\nthat implements the paper above.</p>\n<p>While abandoned, it seems to have completed the code\nthat calculates parity and erasure recovery (including\ntests), and that is what we really want anyway.</p>\n<p>It claims that in the 2-parity case, the ***non-***SIMD\nversion matches the <code>PSHUFB</code> SIMD implementation in\nZFS in performance.\nThe code apparently can be automatically vectorized by\nGCC to use simple SIMD instructions, and wide XORs\ninferred by GCC with <code>-ftree-vectorize</code> (and which are\nvery common in most processors these days, including\nARM) speed up the performance even more, beating the\nZFS <code>PSHUFB</code> implementation (which can be run on fewer\nprocessors) handily.</p>\n<p>(the two are incompatible in the sense that they\ngenerate different parities for the 2nd parity sector,\nso the same code cannot be used in ZFS without losing\nback compatibility, but both can still recover the\ndata perfectly fine if 2 sectors in a stripe are lost;\nthey just use different schemes for 2nd parity sector\nand beyond)</p>\n<p>The same basic XOR code is also used for 3-parity and\nbeyond, meaning that 3-parity does not have the same\nperformance loss that RaidZ3 has, <em>and</em> it generalizes\nall the way up to 128-parity.</p>\n<p>The scheme also works with the aforementioned XOR\noptimization in the previous section.</p>\n<p>That combination of flexibility combined with raw\nspeed on parity calculation and erasure recovery are\n<em>perfect</em> for this scheme, as we probably want more\nthan just 2, or even 3, parities, for this scheme.\nRemember, number of parities are number of disks you\ncan tolerate failing, and since a \"T\"EE shutting down\nis a total loss of all data in its RAM, you want to\nhave larger parities with this scheme compared to actual\nHDDs.</p>\n<h1><a name=\"p-6011-auditing-the-storage-tees-9\" class=\"anchor\" href=\"#p-6011-auditing-the-storage-tees-9\"></a>Auditing The Storage \"T\"EEs</h1>\n<p>The \u201cdisks\u201d in this scheme are really \"T\"EEs running\nvery simple programs that allocate pretty much the\nentire the available memory in the \"T\"EE for use as\nthe \u201cpersistent\u201d storage for the array.</p>\n<p>Because of the sheer simplicity, this can be easy to\naudit; we only need to check that it handles the\nin-memory storage correctly as per the rules:</p>\n<ul>\n<li>The special \u201cdeath record\u201d sector that has no\nprovisional-write capability, but requires strict\nmonotonic increase in a version counter stored in\nit, and which the array uses to record which storage\n\"T\"EEs it considers dead vs alive.</li>\n<li>The current provisional-write sector being held,\nif any.</li>\n<li>The operations to read sectors, set up provisional\nwrites (overwrite and delta modes), and commit and\nrollback provisional writes.</li>\n<li>The actual sector memory.</li>\n</ul>\n<p>(In practice you probably also want the host supplicant\nprogram to use <code>TCP_NODELAY</code> and <code>TCP_QUICKACK</code> as well,\nyou do <em>not</em> want 40ms delays inserted by the combination\nof Nagle and deferred ack)</p>\n<p>The simplicity makes it easier to audit the code.</p>\n<p>This allows auditing even of the persistent storage,\nand rollbacks can be protected against by ensuring\nthat the storage \"T\"EE can only rollback a provisional\nwrite and cannot rollback further.</p>\n<p>From there, the array code running in the main\napplication \"T\"EE builds up an array from multiple\ninstances of the storage \"T\"EEs, simulating a disk\ninside the main application \"T\"EE.\nThe main application can then use standard file\nsystem or database engine on top of this simulated\ndisk.\nFor example, key erasure needed in the statechain\nschemes can be assured by using a non-copy-on-write\nfilesystem, such as Ext4 without journaling, or by\nstoring keys in a dedicated partition of the\nsimulated disk, and overwriting them directly and\nwaiting for a sync from the array code, which would\nthen be propagated to all surviving storage \"T\"EEs,\nwhich you have audited to be actually operating\ncorrectly and overwriting memory backing that\nsector containing the to-be-deleted keys.</p>",
  "post_number": 1,
  "post_type": 1,
  "posts_count": 4,
  "updated_at": "2025-10-07T17:50:26.165Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 13,
  "reads": 42,
  "readers_count": 41,
  "score": 73.4,
  "yours": false,
  "topic_id": 2029,
  "topic_slug": "persisting-mutable-storage-inside-the-t-ee",
  "topic_title": "Persisting Mutable Storage Inside The \"T\"EE",
  "topic_html_title": "Persisting Mutable Storage Inside The &ldquo;T&rdquo;EE",
  "category_id": 8,
  "display_username": "ZmnSCPxj jxPCSnmZ",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "Title: Persisting Mutable Storage Inside The \"T\"EE\n\n# Introduction\n\nA Trusted Execution Environment is a scam where a hardware manufacturer,\nin coordination with a cloud service provider, convinces victims that\nthey can trust somebody else\u2019s computer to actually run normal code,\nwithout use of homomorphic encryption, exactly as the victim intends\nthe normal code to be run.\nThis is done by distracting the victim with a certificate authority\nattesting to a secret key hidden in the hardware, which (1) the hardware\nmanufacturer claims it does not know, despite having placed it in the\nhardware in the first place, and (2) the cloud service provider claims\nnot to have extracted the secret key from the hardware, despite having\ndirect physical possession of the hardware, **and** the ability to\nvirtualize anything, including execution of said hardware with its\nsupposedly-not-extracted secret key.\n\nAll such existing \"T\"EE scams have an additional flaw that the scammers\nalso distract from their victims: they do not have persistent mutable\nstorage.\nAny persistent mutable storage ***MUST*** be external to any \"T\"EE, and\n***MUST*** be trusted to not perform \u201crollback attacks\u201d.\n\nA \u201crollback attack\u201d is simply that a persistent mutable storage,\nrunning outside the \"T\"EE, can keep backup copies of old state, and\nrecover the old state.\nWhen the \"T\"EE application is restarted, the persistent mutable storage\nrecovers from the backup old state.\n\nRollback attacks are disastrous for many Bitcoin applications:\n\n1. If the \"T\"EE application is a Lightning node, if it is given old\n   state, it can be convinced to publish a unilateral close on old\n   state, which is indistuingishable from a theft attempt.\n   The peer can then get all the funds in the channels, effectively\n   stealing by claiming the victim *is* the real thief.\n2. If the \"T\"EE application is a signer for a statechain scheme,\n   old states can effectively retain supposedly-deleted keys.\n   There is ***NO*** possible proof-of-ignorance that the statechain\n   signer can present to prove that there are no backup copies of\n   old \u201cdeleted\u201d keys, because ***by itself*** it cannot even know\n   if it is a victim of a rollback attack.\n\nNote that encrypting the stored data \u201cat rest\u201d ***DOES NOT*** protect\nagainst rollback attacks \u2014 the rollback simply requires that old\ndata be available to the attacker, not that the attacker can decrypt\nit.\n\nIn this writeup, I will show how to synthesize a persistent mutable\nstorage that can be audited to not be rollback-capable, using\nmultiple \"T\"EEs running the same program *in addition to* the main\n\"T\"EE application they serve.\n\n# All Things Must Pass\n\nThe key insight is this:\n\n* Even an HDD is ephemeral and not persistent.\n  One day, it, too, shall die and all its stored data lost \u2014\n  forever.\n* Yet a ZFS array containing that HDD is permanent.\n  You simply hotswap the dead HDD with a fresh one, and let ZFS\n  keep on going on, because ZFS is ***awesome***.\n\nThus, from ephemeral storage, persistence is achievable: this\nHDD, too, shall come to pass, but the end of the ZFS array is not\nyet.\n\n## A Persistent Array Of \"T\"EE Ephemeral Memories\n\nWe can consider that \u201ceven if an HDD is so big and strong and\ntough, one day it will stop being able to read anyway\u201d, then\nperhaps the dinky little ephemeral RAM of a \"T\"EE can serve\njust as well in an array of \"T\"EE ephemeral RAMs?\n\nThe only requirement then is that we distribute the \"T\"EEs\nwidely, so that it is unlikely that so many of them go down\nthat permanent data loss is possible.\nWe then use suitable erasure coding so that we can recover\neven if some number of \"T\"EEs go down and lose the contents\nof their ephemeral RAMs.\n\nNote that part of the \"T\"EE scam is to convince victims that\nthe actual RAM used is *also* encrypted with the hardware\nattestation key.\nThus, there is no need to encrypt data at rest, as the \"T\"EE\nhardware claims it *already* encrypts the data in memory, and\nour \u201cpersistent\u201d storage is in fact the RAM inside the \"T\"EE.\nWe do still need to encrypt data in transit between the main\n\"T\"EE application and all of the \"T\"EE storage programs.\n\nThus, we have multiple \"T\"EE storage programs, which the main\n\"T\"EE puts in a resilient array, achieving persistence on top\nof ephemeral storages.\n\n### A Digression On Terminology\n\nTraditionally, discussions about redundant arrays of\ninexpensive storage devices use the term \u201cblock devices\u201d that\nstore and retrieve \u201cblocks\u201d of data.\n\nUnfortunately for our context \u2014 cryptocurrency network\napplications \u2014 \u201cblock\u201d is much too overloaded a\nword:\n\n1. Bitcoin blocks of transactions.\n2. Symmetric encryption schemes working on blocks of data.\n3. Network communications getting blocked.\n4. Using blocking vs nonblocking APIs on network sockets.\n5. IP (Internet Protocol) blocks.\n6. IP (Intellectual Property) blocks which prevent some\n   erasure coding schemes from being adopted.\n7. Block, the company that funds Spiral, and which itself\n   sells various Block devices such as Square terminals.\n\nThus, I will use old, obsolete terminology that nobody uses\ntoday:\n\n* We have \u201cdisks\u201d, which are storage devices, instead of\n  those newfangled \u201cblock devices\u201d.\n  * It does ***NOT*** matter whether it is spinning rust,\n    trapped electrons in capacitors, or DRAM that we misuse\n    as \u201cpersistent\u201d storage in the disk array.\n    I will call them all \u201cdisks\u201d nevertheless.\n* The storage device is divided into \u201csectors\u201d, instead of\n  those newfangled \u201cblocks\u201d.\n\n# Sector Size\n\nTraditionally, disks presented 512-byte sectors (even more\ntraditionally, 128-byte sectors in single-density floppies\n\u2014 is my age showing yet).\nHowever, modern disks now present 4096-byte sectors; some\nflash-based disks even want to present larger sectors, up to\n32768 bytes.\n\nUnfortunately, as we intend to use multiple \"T\"EEs as disks,\nwe have to connect them via Internet Protocol, most likely\nvia TCP or TLS (itself running on TCP).\n\nAnd generally, the common MSS for TCP is about \\~1400 bytes.\n\nBoth the 512-byte and the 4096-byte options just do not\nfit well with the MSS of \\~1400 bytes.\n512 bytes would require us to `TCP_NODELAY` to deliver the\nsector data ASAP with a smaller-than-max packet, while 4096\nbytes would fragment the data into multiple packets.\n\nIdeally, our sector size would be 1024, as that just about\nfits the MSS for TCP, and can fit a little more data, such\nas MACs for transmission integrity as well as any IVs for\nencryption for in-transit data.\n\nUnfortunately, if we were emulating a disk in the main\napplication \"T\"EE (actually an array of multiple remote \"T\"EE\ndisks), that we then pass to a \u201creal\u201d filesystem like XFS (or\ndirectly to some database engine that works on disks instead\nof filesystem), then those will usually assume sector sizes\nof 4096 bytes (and might be configurable to use sector sizes\nof 512 bytes).\nEven if we work with 1024-byte sectors when communicating\nwith the remote \"T\"EE disks, the overlying file system will\nattempt to write 4096-byte sectors which would require\nmultiple packets anyway, or 512-byte sectors which would\nrequire read-modify-write operations on the main application\n\"T\"EE and add *more* overhead on each 512-byte write.\n\nSo, we should stick with normal 4096-byte sector sizes,\nand just accept that it will take 3 packets in general to\ntransmit them plus any cryptography overhead for integrity\nand encrypted-in-transit requirements.\n\n# RAID5 Write Hole\n\nThe RAID5 write hole happens due to the fact that writing\nto multiple disks in parallel is ***NOT*** an atomic\noperation.\n\nIt\u2019s possible that the application is able to send out\ndata to *some* disks, then crash, with only part of a\nstripe of data being written.\nThis leaves the on-disk data, *together with the extra\nparity data needed by erasure coding*, inconsistent,\nwith no real way to determine which data is \u201creal\u201d.\nPartial stripe writes are particularly bad because\nif a RAID5 write hole occurs and *then* a disk is lost,\nthe recovered data for the lost disk can be completely\ndifferent from *both* the \u201cold\u201d data and the \u201cnew\u201d data!\n\nThis is called the \u201cRAID5\u201d write hole simply because\nit was *already* observed when using erasure coding\nschemes where at most one disk can be lost, i.e. RAID5\nXOR parities.\nHowever, it applies to *all* uses of erasure coding,\nincluding RAID6 with 2 parity disks or \u201cgeneralized\nRAID6\u201d where there are 3 or more parity disks.\n\nFortunately, since this is a novel array scheme\nanyway, we can fix the RAID5 write hole by simply\nchanging the disk interface (this is not possible with\n\u201creal\u201d spinning-rust or electrons-in-capacitor disks,\nof course):\n\n* There is no \u201cwrite to sector\u201d command.\n* There are instead three commands:\n  * \u201cprovisional write to sector\u201d, where the disk\n    stores both the old version of the sector, and\n    the new data for the sector.\n  * \u201ccommit provisional write\u201d where the disk discards\n    the old version of the sector and keeps the new\n    data.\n  * \u201crollback provisional write\u201d where the disk\n    discards the new version of the sector and keeps\n    the old data.\n* At most one provisional write can be in-flight.\n  If the disk is already storing a provisional write\n  for one sector, and another provisional write (to\n  any sector, not just this one) is done, then the\n  command errors.\n  * This means we only need to sacrifice one sector of\n    expensive RAM for the provisional write scheme.\n* On reads, the disk reports both the old and new\n  data for sectors with a pending provisional write.\n* There is also a status command to check if the disk\n  has a provisional write for a sector, as well as to\n  report the sector index for the sector in provisional\n  write.\n\nWhen the array-management code wants to update *one*\nsector in the array:\n\n1. It reads all sectors in the same stripe (this can be\n   cached).\n2. It calculates the new parity data for the new data.\n3. It sends out provisional write requests to the\n   destination storage disk and each of the parity disks.\n4. It waits until all disks have either responded, or\n   timed out.\n   * In case of a time out, it permanently records that\n     disk as \u201cdead\u201d.\n     This record can be placed in a special sector of\n     all disks, with a strict monotonic version number\n     seen by the disk, and RAID1-replicated across all\n     disks, which is always directly replaced if the\n     replacement has a monotonically higher version\n     number.\n     If the highest-versioned death record marks a disk\n     as dead, then the array code never reads or writes\n     to that disk ever again, treating all writes as\n     successes and all reads as recoverable using\n     erasure code recovery of the remaining disks, and\n     reporting the degraded state to the operator.\n   * In particular, if there are N storage disks and P\n     parity disks, and we are updating ONE sector on a\n     storage disk, we are writing to P + 1 disks, and\n     up to P of them can time out and be marked as\n     dead, and the array will still consider itself as\n     \u201cdegraded but hanging on\u201d.\n5. It sends out commit commands to the destination\n   storage disk and each of the parity disks (at least\n   the live ones that did not time out in step 4).\n6. It waits until all disks have either responded, or\n   timed out.\n\nThe array code may crash at any time, and on restart,\nhas to check for pending provisional writes.\nIt looks for the highest-versioned death record to\ndetermine which disks it still considers alive, and\nthen checks for pending provisional writes.\n\n* If the array crashed between steps 3 and 4, some\n  writes may have been received by the disk, but some\n  disks might not have received them.\n* If the array crashed between steps 4 and 6, some\n  commits may have been received by the disk, but\n  some disks might not have received them.\n\nThe array thus needs to differentiate between the above\ntwo cases, by checking if taking the new version as\ntrue leads to a consistent parity.\nIf the new version leads to a consistent parity, it\nmeans it got past step 4 and can commit everything.\nIf the new version does not lead to a consistent parity,\nit means it was in between step 3 and 4 and has to\nroll back everything.\n\n# XOR Optimization\n\nSome erasure coding schemes have an XOR optimization\nproperty:\n\n* In case of a partial stripe write, instead of reading\n  the unwritten storage sectors of the stripe, we can\n  read the *written* storage sector(s), XOR the old\n  version and the new version, set all the *unwritten*\n  storage sectors in-memory as zero, then run the\n  normal parity-calculate code.\n  The resulting parities can then be XORed with the\n  existing parities-to-be-overwritten to result in a\n  consistent stripe of the new version.\n\nThe XOR optimization above means we do not have to\nread the entire stripe to write a few sectors;\nassuming N storages and we want to write W sectors\nof a stripe, if W < N - W then doing the XOR\noptimization means reading fewer sectors.\n\nWe can thus add a special provisional-write command\nthat, instead of taking the new version of the data,\ntakes a \u201cdelta\u201d sector.\nThis provisional-write-delta command then reads the\nold version sector data, XORs the delta sector,\nand the result is the \u201cnew\u201d sector for purposes of\nthe provisional write.\n\nThe above provisional-write-delta command is used for\nwriting to parity sectors when using the XOR\noptimization.\n\n# Generalized RAID6\n\nThe free and open source world currently has a 2-parity\nerasure code scheme implemented in ZFS, with similar\ncode written in the Linux kernel (for `md`), using\n`PSHUFB` SIMD instruction.\nThe performance is generally pretty good ***if*** the\nprocessor has `PSHUFB`.\nHowever, for 3-parity and above, even the `PSHUFB`\nSIMD instruction cannot be used to speed up the\nparity calculation and erasure recovery code for the\nthird parity sector.\nRaidZ3 simply accepts the efficiency loss, while the\nLinux kernel `md` simply does not support more than 2\nparities.\n\nHowever, given how much more fragile our disks are in\nthis scheme, we really want to have a nice efficient\ngeneralized erasure coding scheme that supports much\nmore than 2 parities.\nUnfortunately, faster erasure coding schemes are\npatent-encumbered, i.e. blocked by IP law.\n\nWe *do* have a generalized scheme, based on Reed\nSolomon codes and Galois Field math.\nThis is *usually* very slow since Galois Field math\noften requires moving individual bits around, and\nalso not very amenable to SIMD, but because of this,\nit was never patent-encumbered.\n\nAnd then there is this [XOR-based EC paper](https://www.researchgate.net/publication/2643899_An_XOR-Based_Erasure-Resilient_Coding_Scheme).\n\nThe key insight of this paper is:\n\n* Traditionally, we have been using bytes as\n  `GF(2^8)` field elements.\n* However, we can rotate our view:\n  * One byte is actually 8 bits from 8 different\n    `GF(2^8)` field elements.\n  * So for example, byte index 0 is actually the\n    0th bit of 8 different `GF(2^8)` elements,\n    byte index 1 is actually 1st bit of the same\n    8 different `GF(2^8)` elements\u2026 byte index\n    7 is actually 7th bit of the same 8 different\n    `GF(2^8)`elements.\n  * Thus, XORing a byte is actually an 8-wide\n    SIMD instruction that works on individual bits\n    of multiple `GF(2^8)` elements at a time.\n\nThe above rotation of our view means that even\n\\*\\*\\*non-\\*\\*\\*SIMD reads, XORs, and writes of bytes,\nwords, doublewords, or quadwords, are actually\nsemantically the same as SIMD reads, XORs, and\nwrites of individual bits of multiple `GF(2^8)`\nelements at a time.\n\nThis creates a *generalized* Reed Solomon code\nusing Galois Field math that **is not slow**, and\nyet remains patent-unencumbered due to Reed Solomon\nbeing traditionally viewed as based on really\nslow-for-computers math.\n\nScouring the githubs, I found an [abandoned project](https://github.com/raid5atemyhomework/llrfs)\nthat implements the paper above.\n\nWhile abandoned, it seems to have completed the code\nthat calculates parity and erasure recovery (including\ntests), and that is what we really want anyway.\n\nIt claims that in the 2-parity case, the \\*\\*\\*non-\\*\\*\\*SIMD\nversion matches the `PSHUFB` SIMD implementation in\nZFS in performance.\nThe code apparently can be automatically vectorized by\nGCC to use simple SIMD instructions, and wide XORs\ninferred by GCC with `-ftree-vectorize` (and which are\nvery common in most processors these days, including\nARM) speed up the performance even more, beating the\nZFS `PSHUFB` implementation (which can be run on fewer\nprocessors) handily.\n\n(the two are incompatible in the sense that they\ngenerate different parities for the 2nd parity sector,\nso the same code cannot be used in ZFS without losing\nback compatibility, but both can still recover the\ndata perfectly fine if 2 sectors in a stripe are lost;\nthey just use different schemes for 2nd parity sector\nand beyond)\n\nThe same basic XOR code is also used for 3-parity and\nbeyond, meaning that 3-parity does not have the same\nperformance loss that RaidZ3 has, *and* it generalizes\nall the way up to 128-parity.\n\nThe scheme also works with the aforementioned XOR\noptimization in the previous section.\n\nThat combination of flexibility combined with raw\nspeed on parity calculation and erasure recovery are\n*perfect* for this scheme, as we probably want more\nthan just 2, or even 3, parities, for this scheme.\nRemember, number of parities are number of disks you\ncan tolerate failing, and since a \"T\"EE shutting down\nis a total loss of all data in its RAM, you want to\nhave larger parities with this scheme compared to actual\nHDDs.\n\n# Auditing The Storage \"T\"EEs\n\nThe \u201cdisks\u201d in this scheme are really \"T\"EEs running\nvery simple programs that allocate pretty much the\nentire the available memory in the \"T\"EE for use as\nthe \u201cpersistent\u201d storage for the array.\n\nBecause of the sheer simplicity, this can be easy to\naudit; we only need to check that it handles the\nin-memory storage correctly as per the rules:\n\n* The special \u201cdeath record\u201d sector that has no\n  provisional-write capability, but requires strict\n  monotonic increase in a version counter stored in\n  it, and which the array uses to record which storage\n  \"T\"EEs it considers dead vs alive.\n* The current provisional-write sector being held,\n  if any.\n* The operations to read sectors, set up provisional\n  writes (overwrite and delta modes), and commit and\n  rollback provisional writes.\n* The actual sector memory.\n\n(In practice you probably also want the host supplicant\nprogram to use `TCP_NODELAY` and `TCP_QUICKACK` as well,\nyou do *not* want 40ms delays inserted by the combination\nof Nagle and deferred ack)\n\nThe simplicity makes it easier to audit the code.\n\nThis allows auditing even of the persistent storage,\nand rollbacks can be protected against by ensuring\nthat the storage \"T\"EE can only rollback a provisional\nwrite and cannot rollback further.\n\nFrom there, the array code running in the main\napplication \"T\"EE builds up an array from multiple\ninstances of the storage \"T\"EEs, simulating a disk\ninside the main application \"T\"EE.\nThe main application can then use standard file\nsystem or database engine on top of this simulated\ndisk.\nFor example, key erasure needed in the statechain\nschemes can be assured by using a non-copy-on-write\nfilesystem, such as Ext4 without journaling, or by\nstoring keys in a dedicated partition of the\nsimulated disk, and overwriting them directly and\nwaiting for a sync from the array code, which would\nthen be propagated to all surviving storage \"T\"EEs,\nwhich you have audited to be actually operating\ncorrectly and overwriting memory backing that\nsector containing the to-be-deleted keys.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 104,
  "hidden": false,
  "trust_level": 2,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "Title: Persisting Mutable Storage Inside The &quot;T&quot;EE \n<a name=\"p-6011-introduction-1\" class=\"anchor\" href=\"#p-6011-introduction-1\"></a>Introduction\nA Trusted Execution Environment is a scam where a hardware manufacturer,\nin coordination with a cloud service provider, convinces victims that\nthey can trust somebody else\u2019s computer to actually run normal code,\nwithout use of homomorp&hellip;",
  "truncated": true,
  "post_url": "/t/persisting-mutable-storage-inside-the-t-ee/2029/1",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null,
  "can_vote": false
}