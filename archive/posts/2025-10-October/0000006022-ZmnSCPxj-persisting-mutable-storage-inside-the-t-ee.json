{
  "id": 6022,
  "name": "ZmnSCPxj jxPCSnmZ",
  "username": "ZmnSCPxj",
  "avatar_template": "/letter_avatar_proxy/v4/letter/z/ee7513/{size}.png",
  "created_at": "2025-10-08T11:24:33.282Z",
  "cooked": "<h1><a name=\"p-6022-raid5-write-hole-really-fixed-1\" class=\"anchor\" href=\"#p-6022-raid5-write-hole-really-fixed-1\"></a>RAID5 Write Hole, Really Fixed</h1>\n<p>This is an amendment.</p>\n<p>Unfortunately, the proposed fix for the RAID5 write hole does\nnot in fact work.</p>\n<p>To see why it does not work, let us review the RAID5 write hole.</p>\n<p>For simplicity, let us consider minimal disks that can only\nstore one bit.\nWe consider a setup with 2 storage disks, A and B, and 1 parity\ndisk, P, initially with the following data stored:</p>\n<pre><code class=\"lang-auto\">+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  0  |  |  0  |  |  0  |\n+-----+  +-----+  +-----+\n</code></pre>\n<p>Now, we want to change the bit stored in B from 0 to a 1.\nThat involves writing to both B and P, so that <code>P = A ^ B</code>\nas per the erasure coding we use.</p>\n<p>However, suppose we were able to only write to B, but we\ncrashed before we could write to P.</p>\n<pre><code class=\"lang-auto\">+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  0  |  |  1  |  |  0  |\n+-----+  +-----+  +-----+\n</code></pre>\n<p>And on recovery, it turns out the reason <em>why</em> we crashed\nwas because of a major electrical fault which totally fried\nthe disk A:</p>\n<pre><code class=\"lang-auto\">+-----+  +-----+  +-----+\n|XXXXX|  |  B  |  |  P  |\n|XXXXX|  +-----+  +-----+\n|XXXXX|  |  1  |  |  0  |\n+-----+  +-----+  +-----+\n</code></pre>\n<p>No biggie, we can recover, right?\nExcept that in order to recover, we calculate <code>A = B ^ P</code>\nas per the erasure coding we use, and end up with:</p>\n<pre><code class=\"lang-auto\">+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  1  |  |  1  |  |  0  |\n+-----+  +-----+  +-----+\n</code></pre>\n<p>The problem is that before the crash we only intended to\nwrite a 1 to B, but not to A.\nThus, the RAID5 write hole.</p>\n<p>The solution in the post, unfortunately, does <em>not</em> fix\nthe write hole above.</p>\n<p>Let us re-run the same scenario with our \"T\"EE storage\ndisks.\nWe start with this:</p>\n<pre><code class=\"lang-auto\">+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  0  |  |  0  |  |  0  |\n+-----+  +-----+  +-----+\n</code></pre>\n<p>We intend to change B to 1, and send a provisional write\nto B and P to change their bits to 1.\nUnfortunately, we are able to send the provisional write\nto B, but not to P, before we crash.</p>\n<pre><code class=\"lang-auto\">+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  0  |  |  0  |  |  0  |\n+-----+  +-----+  +-----+\n         |  1  |\n         |(new)|\n         +-----+\n</code></pre>\n<p>And it turns out the <em>reason</em> we crashed is because there was\nan AWS region outage, and it happens that A is on the same\nregion as we were, totally trashing A as well:</p>\n<pre><code class=\"lang-auto\">+-----+  +-----+  +-----+\n|XXXXX|  |  B  |  |  P  |\n|XXXXX|  +-----+  +-----+\n|XXXXX|  |  0  |  |  0  |\n+-----+  +-----+  +-----+\n         |  1  |\n         |(new)|\n         +-----+\n</code></pre>\n<p>Both the new and old states for B are plausible, in the absence\nof A, and thus this does not in fact completely close the RAID5\nwrite hole.\nBut if we select the new state for B, then we would get an\nincorrect recovery for A.</p>\n<p>We cannot, in general, determine if the state is before or\nafter all writes were posted to all modified stores, and that\nis the core problem of the RAID5 write hole.\nIn particular, we cannot simply always roll back the change in\nB \u2014 because what if instead the original situation had been:</p>\n<pre><code class=\"lang-auto\">+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  1  |  |  0  |  |  1  |\n+-----+  +-----+  +-----+\n</code></pre>\n<p>Then we post provisional writes successfully to both B and P:</p>\n<pre><code class=\"lang-auto\">+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  1  |  |  0  |  |  1  |\n+-----+  +-----+  +-----+\n         |  1  |  |  0  |\n         |(new)|  |(new)|\n         +-----+  +-----+\n</code></pre>\n<p>Then we tell B and P to commit, but only the commit command\nto P gets through before we crash:</p>\n<pre><code class=\"lang-auto\">+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  1  |  |  0  |  |  0  |\n+-----+  +-----+  +-----+\n         |  1  |\n         |(new)|\n         +-----+\n</code></pre>\n<p>And A is trashed because it was an AWS region outage:</p>\n<pre><code class=\"lang-auto\">+-----+  +-----+  +-----+\n|XXXXX|  |  B  |  |  P  |\n|XXXXX|  +-----+  +-----+\n|XXXXX|  |  0  |  |  0  |\n+-----+  +-----+  +-----+\n         |  1  |\n         |(new)|\n         +-----+\n</code></pre>\n<p>What we need to be able to do is to figure out what the\noriginal state actually was.</p>\n<h2><a name=\"p-6022-the-real-write-hole-solution-2\" class=\"anchor\" href=\"#p-6022-the-real-write-hole-solution-2\"></a>The Real Write Hole Solution</h2>\n<p>The correct solution is to ensure true atomicity.</p>\n<p>For example, we can use what in filesystems is traditionally\ncalled a journal, and in databases is traditionally called a\nwrite-ahead-log.\nBoth are actually the same thing.</p>\n<p>The array-management code reserves a journal area on all the\ndisks, and stores the journal RAID1 replicated to all disks.</p>\n<p>The journal area has a version number as well, so that if\nmultiple disks have different journals, the array treats the\none with highest version number as the winner.</p>\n<p>In the imagined scenario, the array code would write to the\njournal \u201cI will write 1 to B and 1 to P\u201d, together with the\nnext higher journal version, and replicate that across all\nthe disks.\nOnce at least <code>num_of_parities + 1</code> disks have responded to\nthe writes to the journal, the array code can report success\nto higher layers, but would still need to apply the\njournalled writes to the disks \u2014 this would delay future\nwrites until the previously-journalled writes were known to\nbe committed to the actual storage areas, but at least would\nallow parallelism between the higher layers and the array\ncode, if the higher layers have other things it needs to do\n(such as sending <code>revoke_and_ack</code> after synching to disk).</p>\n<p>On restarts, the array code would simply read the journal\narea and always apply the journal entry with highest\nversion before signalling that the array has been mounted.</p>\n<p>With this, the \"T\"EE storage disk program is now simpler,\nas it no longer has to maintain provisional writes \u2014 the\narray-management code gets the extra complexity of\nmaintaining a RAID1 journal in an area of the disks that\nit reserves.</p>\n<p>An optimization we can have is to add a \u201ccopy sector\u201d\ncommand, where the \"T\"EE storage disk program simply copies\nthe contents of one stored sector over the contents of a\ndifferent stored sector.\nThen, when applying the latest journal entry, the array\ndoes not need to read the journalled sector and then\nre-send it to the destination disk; if the disk has the\nlatest journal entry the array can simply tell the disk to\ncopy the journalled sector into the final destination\nsector, saving network bandwidth.</p>\n<p>Once applied, the journal entry does not need to be\ndeleted; on restart the array will see the journal\nentry again and re-apply it, but its application is safely\nidempotent.\nThis reduces the network bandwidth in the expected common\ncase where we are not crashing the array-management code\nall the time.</p>\n<p>Alternately, if this is being used for a statechain signer, we could have a \u201ctrim\u201d command that sets the sector to all 0s, and use that to delete the journalled sectors after the journal entry has been applied. This would assure us that data in the journal can be removed after the array knows the journal entry has been applied, and removes the possibility that old \u201cdeleted\u201d keys may have been stored in the journal area of one of the disks and not overwritten.</p>\n<p>As the journal gets overwritten, and involves multiple\nsectors of data (a journal entry stores the change in the\nstorage sector plus all the changes in the parity sectors),\nwe need to ensure atomicity of the journal entry getting\nwritten.\nThe journal area size would then have one sector for each\ndisk in the array (so that the journal has space to store a\nfull stripe write) plus an additional sector that stores\nthe version number of the journal entry and the destinations\nfor the journalled sectors.\nWe can make the additional, versioned, sector, the atomicity\nsector.\nIt would not only store the version number, as well as the\ndestinations for the journalled data sectors, but also\ninclude <em>checksums</em> (MACs) of the journalled data sectors.</p>\n<p>Then, when the array knows it has applied the previous\njournal entry, and wants to make a new one for a new write,\nthe array first writes the atomicity sector with the new\nversion, destination, and checksums, and then writes the\njournalled data sectors.\nThese processes can be done in parallel for each of the\ndisks (with the requirement that the journal first ensures\na response from the write to the atomicity sector before\nsending out write commands for the journalled data sectors).</p>\n<p>On restart, the array looks for the highest-versioned\natomicity sector.\nThen it checks for whether the corresponding journalled\ndata sectors match the checksums.\nIf the data sectors do not match, then it means the\nlatest journal entry was not successfully completely\nwritten, and the array should simply not apply the\njournal entry (the fact that it had a half-written\njournal entry at version V means that it already\nknew that the journal entry at version V - 1 was\nalready fully applied, and thus there is no need to\napply anything; the mismatching checksums imply that\nthe journal entry at V - 1 was deleted because it was\nalready known to be applied).\nIf the data sectors DO match the checksums, the journal\nentry was fully written and the array thus always\napplies it (this is benign as the latest journal entry\ngetting reapplied is idempotent).</p>",
  "post_number": 2,
  "post_type": 1,
  "posts_count": 4,
  "updated_at": "2025-10-08T11:29:24.545Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 0,
  "reads": 28,
  "readers_count": 27,
  "score": 5.6,
  "yours": false,
  "topic_id": 2029,
  "topic_slug": "persisting-mutable-storage-inside-the-t-ee",
  "topic_title": "Persisting Mutable Storage Inside The \"T\"EE",
  "topic_html_title": "Persisting Mutable Storage Inside The &ldquo;T&rdquo;EE",
  "category_id": 8,
  "display_username": "ZmnSCPxj jxPCSnmZ",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 2,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "# RAID5 Write Hole, Really Fixed\n\nThis is an amendment.\n\nUnfortunately, the proposed fix for the RAID5 write hole does\nnot in fact work.\n\nTo see why it does not work, let us review the RAID5 write hole.\n\nFor simplicity, let us consider minimal disks that can only\nstore one bit.\nWe consider a setup with 2 storage disks, A and B, and 1 parity\ndisk, P, initially with the following data stored:\n\n```\n+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  0  |  |  0  |  |  0  |\n+-----+  +-----+  +-----+\n```\n\nNow, we want to change the bit stored in B from 0 to a 1.\nThat involves writing to both B and P, so that `P = A ^ B`\nas per the erasure coding we use.\n\nHowever, suppose we were able to only write to B, but we\ncrashed before we could write to P.\n\n```\n+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  0  |  |  1  |  |  0  |\n+-----+  +-----+  +-----+\n```\n\nAnd on recovery, it turns out the reason *why* we crashed\nwas because of a major electrical fault which totally fried\nthe disk A:\n\n```\n+-----+  +-----+  +-----+\n|XXXXX|  |  B  |  |  P  |\n|XXXXX|  +-----+  +-----+\n|XXXXX|  |  1  |  |  0  |\n+-----+  +-----+  +-----+\n```\n\nNo biggie, we can recover, right?\nExcept that in order to recover, we calculate `A = B ^ P`\nas per the erasure coding we use, and end up with:\n\n```\n+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  1  |  |  1  |  |  0  |\n+-----+  +-----+  +-----+\n```\n\nThe problem is that before the crash we only intended to\nwrite a 1 to B, but not to A.\nThus, the RAID5 write hole.\n\nThe solution in the post, unfortunately, does *not* fix\nthe write hole above.\n\nLet us re-run the same scenario with our \"T\"EE storage\ndisks.\nWe start with this:\n\n```\n+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  0  |  |  0  |  |  0  |\n+-----+  +-----+  +-----+\n```\n\nWe intend to change B to 1, and send a provisional write\nto B and P to change their bits to 1.\nUnfortunately, we are able to send the provisional write\nto B, but not to P, before we crash.\n\n```\n+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  0  |  |  0  |  |  0  |\n+-----+  +-----+  +-----+\n         |  1  |\n         |(new)|\n         +-----+\n```\n\nAnd it turns out the *reason* we crashed is because there was\nan AWS region outage, and it happens that A is on the same\nregion as we were, totally trashing A as well:\n\n```\n+-----+  +-----+  +-----+\n|XXXXX|  |  B  |  |  P  |\n|XXXXX|  +-----+  +-----+\n|XXXXX|  |  0  |  |  0  |\n+-----+  +-----+  +-----+\n         |  1  |\n         |(new)|\n         +-----+\n```\n\nBoth the new and old states for B are plausible, in the absence\nof A, and thus this does not in fact completely close the RAID5\nwrite hole.\nBut if we select the new state for B, then we would get an\nincorrect recovery for A.\n\nWe cannot, in general, determine if the state is before or\nafter all writes were posted to all modified stores, and that\nis the core problem of the RAID5 write hole.\nIn particular, we cannot simply always roll back the change in\nB \u2014 because what if instead the original situation had been:\n\n```\n+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  1  |  |  0  |  |  1  |\n+-----+  +-----+  +-----+\n```\n\nThen we post provisional writes successfully to both B and P:\n\n```\n+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  1  |  |  0  |  |  1  |\n+-----+  +-----+  +-----+\n         |  1  |  |  0  |\n         |(new)|  |(new)|\n         +-----+  +-----+\n```\n\nThen we tell B and P to commit, but only the commit command\nto P gets through before we crash:\n\n```\n+-----+  +-----+  +-----+\n|  A  |  |  B  |  |  P  |\n+-----+  +-----+  +-----+\n|  1  |  |  0  |  |  0  |\n+-----+  +-----+  +-----+\n         |  1  |\n         |(new)|\n         +-----+\n```\n\nAnd A is trashed because it was an AWS region outage:\n\n```\n+-----+  +-----+  +-----+\n|XXXXX|  |  B  |  |  P  |\n|XXXXX|  +-----+  +-----+\n|XXXXX|  |  0  |  |  0  |\n+-----+  +-----+  +-----+\n         |  1  |\n         |(new)|\n         +-----+\n```\n\nWhat we need to be able to do is to figure out what the\noriginal state actually was.\n\n## The Real Write Hole Solution\n\nThe correct solution is to ensure true atomicity.\n\nFor example, we can use what in filesystems is traditionally\ncalled a journal, and in databases is traditionally called a\nwrite-ahead-log.\nBoth are actually the same thing.\n\nThe array-management code reserves a journal area on all the\ndisks, and stores the journal RAID1 replicated to all disks.\n\nThe journal area has a version number as well, so that if\nmultiple disks have different journals, the array treats the\none with highest version number as the winner.\n\nIn the imagined scenario, the array code would write to the\njournal \u201cI will write 1 to B and 1 to P\u201d, together with the\nnext higher journal version, and replicate that across all\nthe disks.\nOnce at least `num_of_parities + 1` disks have responded to\nthe writes to the journal, the array code can report success\nto higher layers, but would still need to apply the\njournalled writes to the disks \u2014 this would delay future\nwrites until the previously-journalled writes were known to\nbe committed to the actual storage areas, but at least would\nallow parallelism between the higher layers and the array\ncode, if the higher layers have other things it needs to do\n(such as sending `revoke_and_ack` after synching to disk).\n\nOn restarts, the array code would simply read the journal\narea and always apply the journal entry with highest\nversion before signalling that the array has been mounted.\n\nWith this, the \"T\"EE storage disk program is now simpler,\nas it no longer has to maintain provisional writes \u2014 the\narray-management code gets the extra complexity of\nmaintaining a RAID1 journal in an area of the disks that\nit reserves.\n\nAn optimization we can have is to add a \u201ccopy sector\u201d\ncommand, where the \"T\"EE storage disk program simply copies\nthe contents of one stored sector over the contents of a\ndifferent stored sector.\nThen, when applying the latest journal entry, the array\ndoes not need to read the journalled sector and then\nre-send it to the destination disk; if the disk has the\nlatest journal entry the array can simply tell the disk to\ncopy the journalled sector into the final destination\nsector, saving network bandwidth.\n\nOnce applied, the journal entry does not need to be\ndeleted; on restart the array will see the journal\nentry again and re-apply it, but its application is safely\nidempotent.\nThis reduces the network bandwidth in the expected common\ncase where we are not crashing the array-management code\nall the time.\n\nAlternately, if this is being used for a statechain signer, we could have a \u201ctrim\u201d command that sets the sector to all 0s, and use that to delete the journalled sectors after the journal entry has been applied. This would assure us that data in the journal can be removed after the array knows the journal entry has been applied, and removes the possibility that old \u201cdeleted\u201d keys may have been stored in the journal area of one of the disks and not overwritten.\n\nAs the journal gets overwritten, and involves multiple\nsectors of data (a journal entry stores the change in the\nstorage sector plus all the changes in the parity sectors),\nwe need to ensure atomicity of the journal entry getting\nwritten.\nThe journal area size would then have one sector for each\ndisk in the array (so that the journal has space to store a\nfull stripe write) plus an additional sector that stores\nthe version number of the journal entry and the destinations\nfor the journalled sectors.\nWe can make the additional, versioned, sector, the atomicity\nsector.\nIt would not only store the version number, as well as the\ndestinations for the journalled data sectors, but also\ninclude *checksums* (MACs) of the journalled data sectors.\n\nThen, when the array knows it has applied the previous\njournal entry, and wants to make a new one for a new write,\nthe array first writes the atomicity sector with the new\nversion, destination, and checksums, and then writes the\njournalled data sectors.\nThese processes can be done in parallel for each of the\ndisks (with the requirement that the journal first ensures\na response from the write to the atomicity sector before\nsending out write commands for the journalled data sectors).\n\nOn restart, the array looks for the highest-versioned\natomicity sector.\nThen it checks for whether the corresponding journalled\ndata sectors match the checksums.\nIf the data sectors do not match, then it means the\nlatest journal entry was not successfully completely\nwritten, and the array should simply not apply the\njournal entry (the fact that it had a half-written\njournal entry at version V means that it already\nknew that the journal entry at version V - 1 was\nalready fully applied, and thus there is no need to\napply anything; the mismatching checksums imply that\nthe journal entry at V - 1 was deleted because it was\nalready known to be applied).\nIf the data sectors DO match the checksums, the journal\nentry was fully written and the array thus always\napplies it (this is benign as the latest journal entry\ngetting reapplied is idempotent).",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 104,
  "hidden": false,
  "trust_level": 2,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "<a name=\"p-6022-raid5-write-hole-really-fixed-1\" class=\"anchor\" href=\"#p-6022-raid5-write-hole-really-fixed-1\"></a>RAID5 Write Hole, Really Fixed\nThis is an amendment. \nUnfortunately, the proposed fix for the RAID5 write hole does\nnot in fact work. \nTo see why it does not work, let us review the RAID5 write hole. \nFor simplicity, let us consider minimal disks that can only\nstore one bit.\nWe consider a setup with&hellip;",
  "truncated": true,
  "post_url": "/t/persisting-mutable-storage-inside-the-t-ee/2029/2",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}