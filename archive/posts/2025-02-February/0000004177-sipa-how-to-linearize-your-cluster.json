{
  "id": 4177,
  "name": "Pieter Wuille",
  "username": "sipa",
  "avatar_template": "/user_avatar/delvingbitcoin.org/sipa/{size}/102_2.png",
  "created_at": "2025-02-04T15:41:50.793Z",
  "cooked": "<p>I have posted a topic about the algorithm we had been working on before these min-cut based approaches were discovered: <a href=\"https://delvingbitcoin.org/t/spanning-forest-cluster-linearization/1419\" class=\"inline-onebox\">Spanning-forest cluster linearization</a>. Perhaps some of the insights there carry over still.</p>\n<hr>\n<aside class=\"quote no-group quote-modified\" data-username=\"stefanwouldgo\" data-post=\"23\" data-topic=\"303\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/stefanwouldgo/48/664_2.png\" class=\"avatar\"> stefanwouldgo:</div>\n<blockquote>\n<p>This means that as we look at the series of potential feerates \\lambda\u03bb\\lambda from the highest to the lowest, the min-cuts are only ever increasing in the sense that the highest weight closure for a lower <span class=\"math\">\\lambda</span> includes all the highest weight closures for higher <span class=\"math\">\\lambda</span>.</p>\n</blockquote>\n</aside>\n<p>That\u2019s an amazing insight, and with it, it indeed sounds plausible that with the same overall complexity all chunks can be found. My belief was that since there can exist <span class=\"math\">O(2^n)</span> different-feerate chunks, an algorithm like this needs extra work to remove previous chunks to avoid the blowup, but what you\u2019re saying is that maximizing weight for a given minimum <span class=\"math\">\\lambda</span> already accomplishes that?</p>\n<aside class=\"quote no-group quote-modified\" data-username=\"stefanwouldgo\" data-post=\"23\" data-topic=\"303\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/stefanwouldgo/48/664_2.png\" class=\"avatar\"> stefanwouldgo:</div>\n<blockquote>\n<p>So calling any min-cut algorithm at these \\lambda\u03bb\\lambda will at least let us know if the new cluster improves on the old one. But maybe this is premature optimization.</p>\n</blockquote>\n</aside>\n<p>Determining if an RBF is an improvement isn\u2019t just a question of whether the first chunk is better, but whether the diagram is better everywhere (RBFs can conflict with transactions in other clusters even, and we care about the combined diagram in this case).</p>\n<aside class=\"quote no-group\" data-username=\"stefanwouldgo\" data-post=\"23\" data-topic=\"303\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/stefanwouldgo/48/664_2.png\" class=\"avatar\"> stefanwouldgo:</div>\n<blockquote>\n<p>However, their algorithm seems to perform best on all their instances, they are all MIT licensed C++ implementations and it seems to me they solve exactly our full problem</p>\n</blockquote>\n</aside>\n<p>This is a very preliminary comment, as I haven\u2019t looked at the actual implementation, but I\u2019m skeptical that any existing implementation will do. We\u2019re working with extremely tight timeframes (sub-millisecond, preferably less), which in my experience means that even just the cost of converting the problem to a data structure an existing implementation accepts may be non-trivial already. If the approach is restricted to a background thread that re-linearizes any hard things that weren\u2019t linearized optimally at relay time, such concerns are less relevant, but still, ideally we have just a single codebase that can handle all cases.</p>\n<aside class=\"quote no-group\" data-username=\"stefanwouldgo\" data-post=\"23\" data-topic=\"303\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/stefanwouldgo/48/664_2.png\" class=\"avatar\"> stefanwouldgo:</div>\n<blockquote>\n<p>So it would probably indeed be helpful to have some test cases/benchmarks.</p>\n</blockquote>\n</aside>\n<p>Unfortunately, because we work in an adverserial setting, the real question isn\u2019t (just) about real-life clusters we see today, but also worst-case clusters that attackers can construct. Obviously it doesn\u2019t matter that attacker clusters are optimally linearized, but attackers may in some scenarios be able to attach their transactions to honest users\u2019 transactions, and ideally, even in these settings simple use cases keep working (e.g. CPFP).</p>\n<p>As an example, imagine an honest user performing a CPFP, which causes a transaction to be fee-bumped slightly. Simultaneously, an attacker manages to attach to this simple CPFP cluster a bunch of transactions of their own, which involve significant potential gains from good linearization. A linearization algorithm that spends all its time optimizing the attacker side, but then runs out of time and is interrupted before it ever considers finding the honest users\u2019 fee-bump, would break things.</p>\n<p>In the currently-merged code, this is addressed by always including an ancestor-set finding in the result: each successive chunk has a feerate that is at least as high as the best ancestor set (single child together with all its ancestors) among the transactions that remain. It may be possible to keep using that strategy here; please have a look at the <a href=\"https://delvingbitcoin.org/t/limo-combining-the-best-parts-of-linearization-search-and-merging/825\">LIMO algorithm</a> which may still apply. There is no guarantee that that is sufficient for any particular use case, but it\u2019s probably not far from best we can do within <span class=\"math\">O(n^2)</span> time algorithms, and anything worse than <span class=\"math\">O(n^2)</span> is probably infeasible in the worst case for the cluster sizes we want to support within our time limits.</p>\n<p>But all of this means is that what we\u2019re actually aiming for isn\u2019t all that well defined. Still:</p>\n<ul>\n<li>We don\u2019t necessarily care about the time it takes to find an optimal linearization, but more about how much improvement to the linearization is made per time unit.</li>\n<li>It\u2019s not necessarily actual clusters we see today that matter, but also worst cases attackers can produce.</li>\n<li>We probably need an algorithm that can run with a time limit, and produce a valid (possibly non-optimal) linearization still.</li>\n<li>It would be nice if the algorithm can incorporate \u201cexternally\u201d found good topologically-valid sets, like LIMO can with ancestor sets, guaranteeing them as a minimum quality level.</li>\n<li>When the algorithm runs with a time limit that results in the optimal not being found, ideally the work it did is spread out over all transactions. This may mean some degree of randomization is needed to prevent deterministic behavior that lets an attacker direct where work is performed. It may even be worth doing so if the randomization worsens the worst-case complexity.</li>\n<li>Probably obvious, but still worth stating: for small problems like ours, constant factors matter a lot, and may matter more than asymptotic complexity. In particular, things like bitvectors to represent sets of transactions are possible, which in theory have <span class=\"math\">O(n)</span> cost set operation, but with very significantly lower constant factors than say an <span class=\"math\">O(\\log n)</span> tree structure which a purely theoretical complexity analysis would likely suggest.</li>\n</ul>\n<aside class=\"quote no-group\" data-username=\"Lagrang3\" data-post=\"22\" data-topic=\"303\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/lagrang3/48/377_2.png\" class=\"avatar\"> Lagrang3:</div>\n<blockquote>\n<p>I could help with the implementation of a min-cut algorithm from scratch. Given the fact that clusters are expected to be small, I would stick to simpler implementations so the Bisection search would be my pick (E.L. Lawler mentioned above). Also a tailored implementation could exploit the problem\u2019s specific characteristics, eg. that all arcs besides those connecting s and t, have unlimited capacity and that the cluster doesn\u2019t have cycles.</p>\n</blockquote>\n</aside>\n<p>That\u2019s great! I do think we need time to experiment with these algorithms, because as stated above, actual performance will matter a lot. Once I understand the min-cut algorithms and this paper better I will probably try writing a from-scratch implementation too.</p>\n<aside class=\"quote no-group\" data-username=\"Lagrang3\" data-post=\"22\" data-topic=\"303\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/lagrang3/48/377_2.png\" class=\"avatar\"> Lagrang3:</div>\n<blockquote>\n<p>It would be nice to have some test cases, like the typical worst case clusters one might expect and such, for benchmarks.</p>\n</blockquote>\n</aside>\n<p>Worst cases really depend on the algorithm. This isn\u2019t just theoretical: the currently-merged (exponential) linearization algorithm seems to have clusters with <span class=\"math\">O(1)</span> dependency per transaction as worst case, while the spanning-forest algorithm I had been working on (see above) seems to have clusters with <span class=\"math\">O(n)</span> dependencies per transaction as worst case.</p>\n<p>It is entirely possible that a well-optimized min-cut based implementation works in pretty much negligible time for any real clusters we see today, which makes it hard to source benchmarks there.</p>\n<p>That said, there are benchmarks for the currently-merged algorithm: <a href=\"https://github.com/bitcoin/bitcoin/blob/94ca99ac51dddbee79d6409ebcc43b1119b0aca9/src/bench/cluster_linearize.cpp\" class=\"inline-onebox\">bitcoin/src/bench/cluster_linearize.cpp at 94ca99ac51dddbee79d6409ebcc43b1119b0aca9 \u00b7 bitcoin/bitcoin \u00b7 GitHub</a></p>",
  "post_number": 24,
  "post_type": 1,
  "updated_at": "2025-02-04T16:25:02.440Z",
  "reply_count": 0,
  "reply_to_post_number": 23,
  "quote_count": 2,
  "incoming_link_count": 0,
  "reads": 9,
  "readers_count": 8,
  "score": 1.8,
  "yours": false,
  "topic_id": 303,
  "topic_slug": "how-to-linearize-your-cluster",
  "topic_title": "How to linearize your cluster",
  "topic_html_title": "How to linearize your cluster",
  "category_id": 8,
  "display_username": "Pieter Wuille",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 4,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "I have posted a topic about the algorithm we had been working on before these min-cut based approaches were discovered: https://delvingbitcoin.org/t/spanning-forest-cluster-linearization/1419. Perhaps some of the insights there carry over still.\n\n---\n[quote=\"stefanwouldgo, post:23, topic:303\"]\nThis means that as we look at the series of potential feerates \\lambda\u03bb\\lambda from the highest to the lowest, the min-cuts are only ever increasing in the sense that the highest weight closure for a lower $\\lambda$ includes all the highest weight closures for higher $\\lambda$.\n[/quote]\n\nThat's an amazing insight, and with it, it indeed sounds plausible that with the same overall complexity all chunks can be found. My belief was that since there can exist $O(2^n)$ different-feerate chunks, an algorithm like this needs extra work to remove previous chunks to avoid the blowup, but what you're saying is that maximizing weight for a given minimum $\\lambda$ already accomplishes that?\n\n[quote=\"stefanwouldgo, post:23, topic:303\"]\nSo calling any min-cut algorithm at these \\lambda\u03bb\\lambda will at least let us know if the new cluster improves on the old one. But maybe this is premature optimization.\n[/quote]\n\nDetermining if an RBF is an improvement isn't just a question of whether the first chunk is better, but whether the diagram is better everywhere (RBFs can conflict with transactions in other clusters even, and we care about the combined diagram in this case).\n\n\n\n[quote=\"stefanwouldgo, post:23, topic:303\"]\nHowever, their algorithm seems to perform best on all their instances, they are all MIT licensed C++ implementations and it seems to me they solve exactly our full problem\n[/quote]\n\nThis is a very preliminary comment, as I haven't looked at the actual implementation, but I'm skeptical that any existing implementation will do. We're working with extremely tight timeframes (sub-millisecond, preferably less), which in my experience means that even just the cost of converting the problem to a data structure an existing implementation accepts may be non-trivial already. If the approach is restricted to a background thread that re-linearizes any hard things that weren't linearized optimally at relay time, such concerns are less relevant, but still, ideally we have just a single codebase that can handle all cases.\n\n[quote=\"stefanwouldgo, post:23, topic:303\"]\nSo it would probably indeed be helpful to have some test cases/benchmarks.\n[/quote]\n\nUnfortunately, because we work in an adverserial setting, the real question isn't (just) about real-life clusters we see today, but also worst-case clusters that attackers can construct. Obviously it doesn't matter that attacker clusters are optimally linearized, but attackers may in some scenarios be able to attach their transactions to honest users' transactions, and ideally, even in these settings simple use cases keep working (e.g. CPFP).\n\nAs an example, imagine an honest user performing a CPFP, which causes a transaction to be fee-bumped slightly. Simultaneously, an attacker manages to attach to this simple CPFP cluster a bunch of transactions of their own, which involve significant potential gains from good linearization. A linearization algorithm that spends all its time optimizing the attacker side, but then runs out of time and is interrupted before it ever considers finding the honest users' fee-bump, would break things.\n\nIn the currently-merged code, this is addressed by always including an ancestor-set finding in the result: each successive chunk has a feerate that is at least as high as the best ancestor set (single child together with all its ancestors) among the transactions that remain. It may be possible to keep using that strategy here; please have a look at the [LIMO algorithm](https://delvingbitcoin.org/t/limo-combining-the-best-parts-of-linearization-search-and-merging/825) which may still apply. There is no guarantee that that is sufficient for any particular use case, but it's probably not far from best we can do within $O(n^2)$ time algorithms, and anything worse than $O(n^2)$ is probably infeasible in the worst case for the cluster sizes we want to support within our time limits.\n\nBut all of this means is that what we're actually aiming for isn't all that well defined. Still:\n* We don't necessarily care about the time it takes to find an optimal linearization, but more about how much improvement to the linearization is made per time unit.\n* It's not necessarily actual clusters we see today that matter, but also worst cases attackers can produce.\n* We probably need an algorithm that can run with a time limit, and produce a valid (possibly non-optimal) linearization still.\n* It would be nice if the algorithm can incorporate \"externally\" found good topologically-valid sets, like LIMO can with ancestor sets, guaranteeing them as a minimum quality level.\n* When the algorithm runs with a time limit that results in the optimal not being found, ideally the work it did is spread out over all transactions. This may mean some degree of randomization is needed to prevent deterministic behavior that lets an attacker direct where work is performed. It may even be worth doing so if the randomization worsens the worst-case complexity.\n* Probably obvious, but still worth stating: for small problems like ours, constant factors matter a lot, and may matter more than asymptotic complexity. In particular, things like bitvectors to represent sets of transactions are possible, which in theory have $O(n)$ cost set operation, but with very significantly lower constant factors than say an $O(\\log n)$ tree structure which a purely theoretical complexity analysis would likely suggest.\n\n[quote=\"Lagrang3, post:22, topic:303\"]\nI could help with the implementation of a min-cut algorithm from scratch. Given the fact that clusters are expected to be small, I would stick to simpler implementations so the Bisection search would be my pick (E.L. Lawler mentioned above). Also a tailored implementation could exploit the problem\u2019s specific characteristics, eg. that all arcs besides those connecting s and t, have unlimited capacity and that the cluster doesn\u2019t have cycles.\n[/quote]\n\nThat's great! I do think we need time to experiment with these algorithms, because as stated above, actual performance will matter a lot. Once I understand the min-cut algorithms and this paper better I will probably try writing a from-scratch implementation too.\n\n[quote=\"Lagrang3, post:22, topic:303\"]\nIt would be nice to have some test cases, like the typical worst case clusters one might expect and such, for benchmarks.\n[/quote]\n\nWorst cases really depend on the algorithm. This isn't just theoretical: the currently-merged (exponential) linearization algorithm seems to have clusters with $O(1)$ dependency per transaction as worst case, while the spanning-forest algorithm I had been working on (see above) seems to have clusters with $O(n)$ dependencies per transaction as worst case.\n\nIt is entirely possible that a well-optimized min-cut based implementation works in pretty much negligible time for any real clusters we see today, which makes it hard to source benchmarks there.\n\nThat said, there are benchmarks for the currently-merged algorithm: https://github.com/bitcoin/bitcoin/blob/94ca99ac51dddbee79d6409ebcc43b1119b0aca9/src/bench/cluster_linearize.cpp",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 96,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": "Github link was replaced with a permanent link",
  "can_view_edit_history": true,
  "wiki": false,
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false
}