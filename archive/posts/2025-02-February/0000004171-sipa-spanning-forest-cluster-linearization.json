{
  "id": 4171,
  "name": "Pieter Wuille",
  "username": "sipa",
  "avatar_template": "/user_avatar/delvingbitcoin.org/sipa/{size}/102_2.png",
  "created_at": "2025-02-04T14:31:21.363Z",
  "cooked": "<h1><a name=\"p-4171-spanning-forest-cluster-linearization-1\" class=\"anchor\" href=\"#p-4171-spanning-forest-cluster-linearization-1\"></a>Spanning-forest cluster linearization</h1>\n<div data-theme-toc=\"true\"> </div>\n<p>This is a write-up about a work in progress cluster linearization algorithm that appeared (and may still be) promising as a replacement for the ones described in the currently-implemented algorithms (Bitcoin Core PRs <a href=\"https://github.com/bitcoin/bitcoin/pull/30286\">30126</a>, <a href=\"https://github.com/bitcoin/bitcoin/pull/30285\">30285</a>, <a href=\"https://github.com/bitcoin/bitcoin/pull/30286\">30286</a> and Delving posts <a href=\"https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303\">on linearization</a> and <a href=\"https://delvingbitcoin.org/t/introduction-to-cluster-linearization/1032\">related algorithms</a>).</p>\n<p>While this spanning-forest algorithm looks promising, the recent <a href=\"https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303/9\">discovery</a> by <a class=\"mention\" href=\"/u/stefanwouldgo\">@stefanwouldgo</a> that well-known <a href=\"https://en.wikipedia.org/wiki/Minimum_cut\">minimal-cut</a>-based algorithms with good asymptotic complexity exist for the problem we call cluster linearization (the established term is the \u201cmaximum-ratio <a href=\"https://en.wikipedia.org/wiki/Closure_problem\">closure problem</a>\u201d) is even more promising.</p>\n<p>I am still creating this write-up, because despite lacking known complexity bounds, it appears to be very elegant, fast, and practical (I have a prototype implementation). It is possible that good bounds for it still get found, or that its insights help others build on top, or that it ultimately appears more practical for the real-life problems we face than more advanced algorithms. Or perhaps we discover it\u2019s actually equivalent to a well-known algorithm. I don\u2019t know, but before switching over to studying another approach, I want to have it written down in some form, also to have my own thoughts ordered.</p>\n<h2><a name=\"p-4171-h-1-background-cluster-linearization-as-an-lp-problem-2\" class=\"anchor\" href=\"#p-4171-h-1-background-cluster-linearization-as-an-lp-problem-2\"></a>1. Background: cluster linearization as an LP problem</h2>\n<p>(Skip this section if you\u2019re not interested in the background that led to this idea)</p>\n<p>Cluster linearization can be done by repeatedly finding the highest-feerate topologically-valid subset, moving it to the front of a linearization, and then continuing with what remains of a cluster. This reduces the problem of optimal linearization to the problem of finding the highest-feerate topologically-valid subset. This problem can be formulated as a linear programming problem (credit to Dongning Guo and Aviv Zohar):</p>\n<ul>\n<li>For every transaction <span class=\"math\">i</span>, have a variable <span class=\"math\">t_i \\in \\mathbb{R}_{\\geq 0}</span>. <span class=\"math\">t_i &gt; 0</span> means including transaction <span class=\"math\">i</span> in the solution set; <span class=\"math\">t_i = 0</span> means not including it. Using <strong>real numbers</strong> instead of booleans here may seem strange, but it will turn out that all non-zero <span class=\"math\">t_i</span> will have the same value anyway.</li>\n<li>Enforce the topology constraints by adding an <strong>inequality per dependency</strong>: for each parent-child pair <span class=\"math\">(p,c)</span>,  require <span class=\"math\">t_p \\geq t_c</span>. Thus, if a child is included (nonzero), so is the parent.</li>\n<li>Use as <strong>goal function</strong> <span class=\"math\">g = \\sum_i t_i \\operatorname{fee}(i) / \\sum_i t_i \\operatorname{size}(i)</span>, which is the feerate of the solution set. It doesn\u2019t matter that the non-zero <span class=\"math\">t_i</span> may take on values different from <span class=\"math\">1</span>, because they contribute equally to numerator and denominator anyway, meaning that multiplying all of them with the same value has no effect.</li>\n<li>To make the goal function linear (it is a fraction so far), add one additional <strong>normalization constraint</strong> that the numerator is <span class=\"math\">1</span>: <span class=\"math\">\\sum_i t_i \\operatorname{size}(i) = 1</span>, and thus the goal becomes just <span class=\"math\">g = \\sum_i t_i \\operatorname{fee}(i)</span>.</li>\n</ul>\n<p>It can be proven that any optimal solution to this problem is an optimal solution to the \u201cfind highest-feerate topologically-valid subset\u201d problem.</p>\n<p>This immediately has two implications:</p>\n<ul>\n<li>Cluster linearization can be done in <strong>worst-case polynomial time</strong>, through polynomial-time LP solving algorithms, including ones based on <a href=\"https://en.wikipedia.org/wiki/Interior-point_method\">Interior Point Methods</a>. These are not necessarily the best approach for our problem, but their existence conclusively settles the fact that the problem is not NP-hard for example.</li>\n<li>The entire scientific domain of linear programming solution techniques becomes applicable to our problem.</li>\n</ul>\n<h3><a name=\"p-4171-h-11-the-simplex-algorithm-for-finding-high-feerate-sets-3\" class=\"anchor\" href=\"#p-4171-h-11-the-simplex-algorithm-for-finding-high-feerate-sets-3\"></a>1.1. The simplex algorithm for finding high-feerate sets.</h3>\n<p>Probably the best-known, and in some variation, most commonly used algorithm for solving linear programming problems is the <a href=\"https://en.wikipedia.org/wiki/Simplex_algorithm\"><strong>simplex</strong> algorithm</a>.</p>\n<p>Internally, the simplex algorithm requires that the problem, and the goal, be stated as equations over a number of non-negative real variables, with no inequalities besides non-negativity. Our <span class=\"math\">t_i</span> variables are already non-negative, but we have inequalities left. We can introduce slack variables to get rid of those:</p>\n<ul>\n<li>Introduce a <span class=\"math\">d_j \\in \\mathbb{R}_{\\geq 0}</span> for every dependency <span class=\"math\">(p_j, c_j)</span>: <span class=\"math\">d_j = t_{p_j} - t_{c_j}</span>, replacing the dependency inequalities.</li>\n</ul>\n<p>In a problem with <span class=\"math\">n</span> transactions and <span class=\"math\">m</span> dependencies, this means <span class=\"math\">n + m + 1</span> variables (<span class=\"math\">t_{1 \\ldots n}</span>, <span class=\"math\">d_{1 \\ldots m}</span>, and <span class=\"math\">g</span>), and <span class=\"math\">m + 2</span> equations:</p>\n<ul>\n<li><span class=\"math\">d_j = t_{p_j} - t_{c_j}</span>, for <span class=\"math\">j = 1 \\ldots m</span> (the dependencies)</li>\n<li><span class=\"math\">g = \\sum_i t_i \\operatorname{fee}(i)</span> (the goal)</li>\n<li><span class=\"math\">\\sum_i t_i \\operatorname{size}(i) = 1</span> (the normalization)</li>\n</ul>\n<p>The simplex algorithm then maintains a set of <span class=\"math\">n-1</span> <strong><em>free</em> variables</strong>, which are chosen to be zero. All other variables (the <strong><em>basic</em> variables</strong>) can be computed from the free variables using the <span class=\"math\">n+m+1</span> equations. In every iteration of the algorithm, one free variable is chosen to become basic, while a basic algorithm is chosen to become free. In this context:</p>\n<ul>\n<li>A free <span class=\"math\">t_i</span> variable means transaction <span class=\"math\">i</span> is excluded from the solution.</li>\n<li>A free <span class=\"math\">d_j</span> variable means that transactions <span class=\"math\">p_j</span> and <span class=\"math\">c_j</span> are either both included, or both excluded.</li>\n</ul>\n<p>There are rules that control which variables are picked to become free/basic, which generally boil down to not worsening the existing solution, and keeping the solution valid. In addition, various policies exist that attempt to maximize progress in every step.</p>\n<h3><a name=\"p-4171-h-12-interpreting-simplex-steps-in-our-problem-space-4\" class=\"anchor\" href=\"#p-4171-h-12-interpreting-simplex-steps-in-our-problem-space-4\"></a>1.2. Interpreting simplex steps in our problem space</h3>\n<p>Note that groups of transactions can exist that are reachable from one another through free <span class=\"math\">d_j</span> variables, and thus they must all be included in the solution set, or all excluded from it. Let\u2019s call these groups \u201cchunks\u201d, for reasons that will become clear later.</p>\n<p>Because of further rules on the choices taken by the simplex algorithm, further properties hold:</p>\n<ul>\n<li>No \u201ccycle\u201d of free <span class=\"math\">d_j</span> variables can exist within a chunk (because at least one <span class=\"math\">d_j = 0</span> would be redundant).</li>\n<li>Within every chunk, there can at most be 1 free <span class=\"math\">t_i</span> variable (a second one would be redundant).</li>\n<li>By a counting argument (exactly <span class=\"math\">n-1</span> free variables) it follows that all chunks, except for one, will contain a free <span class=\"math\">t_i</span> variable, and thus be excluded.</li>\n</ul>\n<p>Overall this means that every state of the simplex algorithm corresponds to a <strong>partitioning of the transaction graph into chunks</strong>, each of which is connected internally by a <strong>spanning tree</strong> (because no cycles) of free <span class=\"math\">d_j</span> variables, and among them <strong>one is the solution set</strong>. In every iteration of the algorithm:</p>\n<ul>\n<li>One free variable is made basic, which is either:\n<ul>\n<li>A <span class=\"math\">t_i</span> variable: an excluded chunk whose feerate is higher than the included chunk\u2019s feerate is made not-excluded. It may become included, or end up being merged with another (included or excluded) chunk, see below.</li>\n<li>A <span class=\"math\">d_j</span> variable: a chunk breaks apart into two chunks, as the dependency between them is made basic (in a spanning tree, removing any link breaks the tree in two), where the \u201ctop\u201d chunk (the one containing the parent of the <span class=\"math\">d_j</span> variable) has higher feerate than the included chunk, or the \u201cbottom\u201d chunk has lower feerate than the included chunk.</li>\n</ul>\n</li>\n<li>One basic variable is made free, which is either:\n<ul>\n<li>A <span class=\"math\">t_i</span> variable: the previously-included chunk is made excluded, if the chunk made non-excluded or split off above has no (basic) dependencies on another chunk (i.e., it is topological).</li>\n<li>A <span class=\"math\">d_j</span> variable otherwise: the non-excluded or newly-split off chunk merges with another chunk it depends on.</li>\n</ul>\n</li>\n</ul>\n<h3><a name=\"p-4171-h-13-from-high-feerate-subsets-to-full-linearization-5\" class=\"anchor\" href=\"#p-4171-h-13-from-high-feerate-subsets-to-full-linearization-5\"></a>1.3. From high-feerate subsets to full linearization</h3>\n<p>There appear to be several superfluous details to the algorithm above. It keeps track of (up to) one specific free <span class=\"math\">t_i</span> variable within each chunk, but the choice of <em>which</em> transaction <span class=\"math\">i</span> that is for each excluded chunk is almost entirely irrelevant to the algorithm. Furthermore, one can wonder what the point is of having to keep track of which chunk is included explicitly, as it will always just be the highest-feerate one among the chunks. We choose to make a few simplifications to the algorithm:</p>\n<ul>\n<li>Get rid of the <span class=\"math\">t_i</span> variables. The included chunk is implicitly the one with the highest-feerate; the rest is excluded.</li>\n<li>Avoid comparing with the chunk feerate of the included set:\n<ul>\n<li>To determine which <span class=\"math\">d_j</span> to make basic, do not compare the top or bottom would-be chunk with the included chunk, but with each other. The rule becomes that one can split a chunk in two along a dependency, if doing so results in a top chunk with higher feerate than the bottom one.</li>\n<li>A <span class=\"math\">d_j</span> variable can be made free if the chunk the child is in has a higher feerate than the top chunk (i.e., merging a chunk with a parent if it is not topological).</li>\n<li>By doing this, we\u2019re effectively not just optimizing the included chunk, but <strong>all chunks</strong>. And this is why they\u2019re called chunks: they become the chunks of the full linearization when the algorithm ends, with credit to <a class=\"mention\" href=\"/u/murch\">@murch</a> for noting the similarity between the algorithm and the <a href=\"https://delvingbitcoin.org/t/introduction-to-cluster-linearization/1032#h-22-feerate-diagrams-and-chunking-5\">chunking algorithm</a>.</li>\n</ul>\n</li>\n</ul>\n<p>Thus, overall, we obtain an algorithm whose state is a <a href=\"https://en.wikipedia.org/wiki/Spanning_tree#Spanning_forests\">spanning forest</a> (a collection of spanning trees covering the whole graph) for the overall cluster, and which aims to find the overall linearization of the whole thing, not just the first chunk.</p>\n<h2><a name=\"p-4171-h-2-the-algorithm-6\" class=\"anchor\" href=\"#p-4171-h-2-the-algorithm-6\"></a>2. The algorithm</h2>\n<p>Restated from scratch, without the whole LP/Simplex background that led to it, the spanning forest linearization algorithm is:</p>\n<ul>\n<li>Input:\n<ul>\n<li>A transaction graph for a cluster, with fees, sizes, and dependencies between the transactions.</li>\n</ul>\n</li>\n<li>Output:\n<ul>\n<li>An optimal linearization for the graph.</li>\n</ul>\n</li>\n<li>State:\n<ul>\n<li>For every dependency in the graph, maintain a boolean, \u201c<strong>active</strong>\u201d. Initially all dependencies are inactive (active dependency corresponds to \u201cfree <span class=\"math\">d_j</span>\u201d in the previous section, but \u201cactive\u201d seems like a more appropriate name when dropping the simplex context).</li>\n<li>The set of active dependencies implicitly partition the graph into <strong>chunks</strong>. Two transactions are in the same chunk if there is a path of active dependencies (each in either the parent-of or child-of direction) from one to another. If there is no such path, the transactions are in different chunks. This means that the initial state is every transaction in its own singleton chunk.</li>\n<li>An invariant of the algorithm is that no cycles can exist within the active dependencies (again, ignoring direction). In other words, the active dependencies within each chunk form a spanning tree, and the active dependencies for the entire cluster form a <strong>spanning forest</strong> for it.</li>\n</ul>\n</li>\n<li>Algorithm:\n<ul>\n<li>Perform one of these permitted steps as long as any apply:\n<ol>\n<li><strong>Merging chunks</strong>. If there is an inactive dependency, where the parent and child are in distinct chunks, and the child chunk has higher feerate than the parent chunk, make the dependency active, merging the two chunks.</li>\n<li><strong>Splitting chunks</strong>. If there is an active dependency, consider the two chunks that would be created by making it inactive (due to the spanning-forest property, this is the case for every active dependency). If the would-be chunk with the parent of the dependency in has a higher feerate than the child chunk, make the dependency inactive, splitting the chunk in two.</li>\n</ol>\n</li>\n<li>Output the final chunks, sorted from high feerate to low feerate, tie-breaking by topology, each chunk internally sorted by an arbitrary for valid topological order (e.g., by increasing number of ancestors).</li>\n</ul>\n</li>\n</ul>\n<h3><a name=\"p-4171-h-21-correctness-7\" class=\"anchor\" href=\"#p-4171-h-21-correctness-7\"></a>2.1. Correctness</h3>\n<p>Since several changes were made to the simplex algorithm along the way to obtain the spanning forest algorithm, it is not necessarily the case anymore that it is correct.</p>\n<p>The algorithm above, <strong>if it terminates</strong>, outputs an optimal linearization, or put otherwise: the successively output chunks are each a highest-feerate topological subset of what remains of the cluster. The <a href=\"https://delvingbitcoin.org/t/cluster-mempool-definitions-theory/202\">theory thread</a> proves that this suffices for an optimal linearization. To see the output chunks are optimal:</p>\n<ul>\n<li>There cannot be any higher-feerate chunk that depends on a lower-feerate chunk, because if there was, the chunk merging rule above would still apply, and the algorithm has not terminated. Thus, the chunks that come out, from high feerate to low feeate, form a <strong>valid linearization</strong>.</li>\n<li>As for optimality, it holds that the highest-feerate chunk is actually the highest-feerate topologically-valid subset of the <strong>active subgraph</strong> of the cluster. Since fewer dependencies means fewer constraints, the highest-feerate set for just the active subgraph is certainly no worse than the highest-feerate set for the actual, more constrained, problem. To see why the highest-feerate chunk is the highest-feerate topologically-valid subset of the active subgraph:\n<ul>\n<li>It is certainly topologically-valid, because it is topologically-valid for the actual problem (see above).</li>\n<li>Imagine a higher-feerate topologically-valid subset of the active subgraph existed. Consider its intersections with the final chunks in the algorithm. At least one of those must have a feerate above the highest-feerate chunk itself (if not, their combined feerate, which is a weighted average, would not be higher either). That intersection must be obtainable by cutting off certain branches of the spanning tree, and some of those branches must have a feerate lower than the chunk feerate (otherwise cutting them off would not be an improvement). However, because the splitting rule of the algorithm is not applicable (or the algorithm wouldn\u2019t have terminated), no such branch can exist (if it did, the chunk would have been split in two right there).</li>\n</ul>\n</li>\n<li>All the properties above remain valid after the highest-feerate chunk is removed from the graph, which is exactly the state obtained before the next chunk is output.</li>\n</ul>\n<p>Note that all of these properties are conditional on the assumption that the algorithm terminates. It is less clear under what conditions that is the case (see below).</p>\n<h3><a name=\"p-4171-h-22-refining-the-merge-and-split-choices-8\" class=\"anchor\" href=\"#p-4171-h-22-refining-the-merge-and-split-choices-8\"></a>2.2. Refining the merge and split choices</h3>\n<p>So far, we have left unspecified <em>which</em> dependencies are to be made active/inactive. From casual fuzzing, it appears that just making random choices is sufficient to obtain the optimal result, but random choices <strong>can result in repeated states</strong>, which means it can include pointless work which we\u2019d like to avoid.</p>\n<p>Thanks to <a class=\"mention\" href=\"/u/ajtowns\">@ajtowns</a> and <a class=\"mention\" href=\"/u/clarashk\">@ClaraShk</a> for the discussions that led to the conclusions in this section.</p>\n<h4><a name=\"p-4171-h-221-prioritize-merge-over-split-9\" class=\"anchor\" href=\"#p-4171-h-221-prioritize-merge-over-split-9\"></a>2.2.1. Prioritize merge over split</h4>\n<p>One easy refinement, which will simplify further analysis, is to prioritize merging over splitting. The idea is that merging is making the state <em>topological</em>, while splitting is about making the state <em>better</em>. If we, after every improving split step, perform merges as long as possible, we make the state topological as soon as possible again. This has the practical advantage that if we want to stop the algorithm (due to running out of time), we end up with something that\u2019s at least valid.</p>\n<h4><a name=\"p-4171-h-222-merge-by-highest-feerate-difference-10\" class=\"anchor\" href=\"#p-4171-h-222-merge-by-highest-feerate-difference-10\"></a>2.2.2. Merge by highest feerate difference</h4>\n<p>When performing merges, there may be multiple inactive dependencies where the child chunk has a higher feerate than the parent, in which case it is unclear which one to pick. From casual fuzzing, it appears that prioritizing the one where the feerate <em>difference</em> between the child and parent is maximal has a number of advantages:</p>\n<ul>\n<li>The same state is never repeated, which - if true - must imply that some (possibly minute) strict improvement happens every iteration.</li>\n<li>It appears to be a good choice for minimizing the number of iterations needed in the worst case.</li>\n</ul>\n<p>Furthermore, it simplifies reasoning about termination. Call an \u201c<strong>improvement step</strong>\u201d to be one application of the splitting rule (using whatever criterion to select which one), followed by as many applications of the merging rule (using maximum feerate difference first as selection strategy) as possible. If the state was topological (= no merge steps applicable) before an improvement step, we can compare the feerate diagrams of the linearizations that would be output before and after the improvement step.</p>\n<p>Think of the state of the chunks implicitly as an ordered list, from higher-feerate to lower-feerate, plus the fact that chunks can only depend on chunks that come before it. When a split happens, there are two possibilities:</p>\n<ul>\n<li>The higher-feerate split-off chunk (the parent chunk) depends (through another inactive dependency) on the lower-feerate one (the child chunk). Since the produced child has lower feerate than any other chunk it may depend on, the maximum-feerate-difference rule in this case says that it must <strong>merge with itself again</strong>, even if other merges are possible. In this case, the feerate diagram remains unchanged, because we end up with the same chunks as before the improvement step, though with a different spanning tree in the re-merged chunk. Whether this can end up in an infinite loop, and if not, how many iterations improving the same chunk this way can happen is an open question.</li>\n<li>If such a self-merge does not happen (because the split-off parent chunk does not depend on the split-off child), imagine that the two new chunks initially take on adjacent positions in the sorted chunk list (which is now no longer properly sorted), where the split chunk used to, with the parent one first. We can look at what happens to the parent and child chunk separately:\n<ul>\n<li>The higher-feerate split-off chunk can in this case only depends on chunks that used to precede it, but it may have a higher feerate than those. The maximum-feerate-difference rule now effectively boils down to this new chunk \u201c<strong>bubbling up</strong>\u201d (think: bubble sort) until it either ends up in a position where its feerate is no longer higher than what precedes it, or it finds a lower-feerate chunk it depends on, merging with it, and then continuing the process bubbling this merged chunk further up possibly. This is very similar to the behavior inside the <a href=\"https://delvingbitcoin.org/t/linearization-post-processing-o-n-2-fancy-chunking/201\">post-linearization algorithm</a> described earlier.</li>\n<li>The same process, but reversed, happens to the split-off child. Its feerate dropped compared to the earlier chunk, so now chunks that used to succeed it may now have higher feerate than it. So the split-off child chunk starts <strong>bubbling down</strong>, until it finds a position in the sort chunk list where it has higher feerate than what follows, or when it encounters a higher-feerate chunk that depends on it, where it merges, and possibly continuing further.</li>\n</ul>\n</li>\n</ul>\n<p>Whenever a self-merge does not happen, every improvement step strictly improves the feerate diagram, by moving a higher-feerate part up, and a lower-feerate part down, each potentially merging with other chunks. So the question of termination is just about improvement steps that result in a self-merge.</p>\n<h4><a name=\"p-4171-h-223-split-by-maximizing-feesubparentsub-sizesubchildsub-feesubchildsub-sizesubparentsub-11\" class=\"anchor\" href=\"#p-4171-h-223-split-by-maximizing-feesubparentsub-sizesubchildsub-feesubchildsub-sizesubparentsub-11\"></a>2.2.3. Split by maximizing <em>(fee<sub>parent</sub> size<sub>child</sub> - fee<sub>child</sub> size<sub>parent</sub>)</em></h4>\n<p>It remains to be discussed how to decide what split to perform within an improvement step when there are multiple possible options. If it is the case that every improvement step is guaranteed to make some kind of progress, it may be acceptable to just <strong>make improvement steps randomly</strong>, but perhaps we can do better.</p>\n<p>Going back to the LP formulation of the problem, it is a requirement that the derivative of the goal variable <span class=\"math\">g</span> w.r.t. the variable being made free (in the case of a split, the <span class=\"math\">d_j</span> variable) is positive (this translated to needing a would-be parent chunk to having higher feerate than the child). A natural choice is picking the one with the <strong>highest derivative</strong> in the simplex phrasing. Working that out corresponds to maximizing the function</p>\n<div class=\"math\">\n\\begin{equation}\n\\begin{split}\nq(A, B) &amp; = \\,\\, &amp; \\left(\\operatorname{feerate}(A) - \\operatorname{feerate}(B)\\right) \\operatorname{size}(A) \\operatorname{size}(B) \\\\ \n&amp; = &amp; \\left(\\frac{\\operatorname{fee}(A)}{\\operatorname{size}(A)} - \\frac{\\operatorname{fee}(B)}{\\operatorname{size}(B)}\\right)\\operatorname{size}(A)\\operatorname{size}(B) \\\\\n&amp; = &amp; \\operatorname{fee}(A)\\operatorname{size}(B) - \\operatorname{fee}(B)\\operatorname{size}(A)\n\\end{split}\n\\end{equation}\n</div>\n<p>for A the would-be parent chunk and B the would-be child chunk. And from casual fuzzing, this indeed appears to be a good choice, with apparently relatively low worst-case numbers of iterations. Furthermore, it appears to result in <strong>never repeating the same <em>split</em></strong> (in the sense that the same chunk is never split in the same parent and child chunk, ignoring the exact spanning tree it has. Other rules (e.g. maximizing feerate difference directly when splitting) don\u2019t seem to have this property: while they don\u2019t repeat the exact same state, they do appear to result in sometimes repeating the same splits a finite number of times.</p>\n<p>Another insight is that <em>if</em> a split step does not result in a self-merge (see above), <span class=\"math\">q(A,B)/2</span> is a lower bound on <strong>increase in surface area</strong> (integral) under the feerate diagram curve, which is directly related to our overall goal (because the optimal linearization necessarily has the highest-possible surface area under the curve).</p>\n<p>The <span class=\"math\">q()</span> function has other interesting properties too, which may or may not matter:</p>\n<ul>\n<li><span class=\"math\">q(A,B) = \\sum_{i \\in A} \\sum_{j \\in B} q(i,j)</span> (i.e., it is a <a href=\"https://en.wikipedia.org/wiki/Bilinear_map\">bilinear map</a>).</li>\n<li><span class=\"math\">q(A,B) = q(A, A \\cup B) = q(A \\cup B, B)</span></li>\n</ul>\n<h3><a name=\"p-4171-h-23-initial-state-12\" class=\"anchor\" href=\"#p-4171-h-23-initial-state-12\"></a>2.3. Initial state</h3>\n<p>We started with stating that the initial state is all dependencies inactive (i.e., all transactions in their own chunk). But that state is typically not topologically valid, which is a requirement for the analysis of the improvement steps above.</p>\n<p>It is possible to just start with a general merging step to avoid this: even just merging everything randomly would suffice. But we can do better. In practice, we always <strong>start with a known linearization</strong> for a cluster, and the goal is improving it, rather than finding a new linearization from scratch.</p>\n<p>With that, we can use the following approach:</p>\n<ul>\n<li>Iterate over all transactions in the input linearization, from front to back. And for each:\n<ul>\n<li>Perform a bubbling-up merge on the chunk that transaction is in (initially a singleton).</li>\n</ul>\n</li>\n</ul>\n<p>The result is actually <strong>exactly the <a href=\"https://delvingbitcoin.org/t/linearization-post-processing-o-n-2-fancy-chunking/201\">post-linearization algorithm</a></strong>, but in the spanning-forest setting. This means that the chunks that come out, even without any further improvement steps, will form a linearization at least as good as the input linearization. Any work done on top just makes it even better.</p>\n<p>In other words, we can see this as natural linearization-improvement algorithm, avoiding the need for <a href=\"https://delvingbitcoin.org/t/limo-combining-the-best-parts-of-linearization-search-and-merging/825\">LIMO</a> or explicit <a href=\"https://delvingbitcoin.org/t/merging-incomparable-linearizations/209\">linearization merging</a>. A downside is that it is unclear how to incorporate a \u201cmake sure every next chunk is at least as good as the highest-feerate ancestor set among what remains\u201d, without explicitly computing such a linearization and merging with it. It is unclear how big of a problem that is.</p>\n<h3><a name=\"p-4171-h-24-resulting-combined-algorithm-13\" class=\"anchor\" href=\"#p-4171-h-24-resulting-combined-algorithm-13\"></a>2.4. Resulting combined algorithm</h3>\n<p>Putting all the pieces above together, we get:</p>\n<ul>\n<li>SpanningTreeLinearize(C, L)\n<ul>\n<li>Helper function: MergeUpwards(T)\n<ul>\n<li>Loop:\n<ul>\n<li>Find the chunk H which transaction T is in.</li>\n<li>Among all other chunks H\u2019, which H has an (inactive) dependency on, find the lowest-feerate one.</li>\n<li>If no H\u2019 is found, stop loop.</li>\n<li>Activate a dependency between H and H\u2019.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Helper function: MergeDownwards(T):\n<ul>\n<li>Loop:\n<ul>\n<li>Find the chunk H which transaction T is in.</li>\n<li>Among all other chunks H\u2019 which have an (inactive) dependency on H, find the highest-feerate one.</li>\n<li>If no H\u2019 is found, stop loop.</li>\n<li>Activate a dependency between H and H\u2019.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Helper function: Improve(D)\n<ul>\n<li>Deactivate D, resulting in chunks Hp and Hc.</li>\n<li>If Hc has a dependency on Hp, activate it and stop.</li>\n<li>Otherwise:\n<ul>\n<li>MergeUpwards(Hp)</li>\n<li>MergeDownwards(Hc)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Initialize state with all dependencies of C as inactive.</li>\n<li>For each transaction T in L:\n<ul>\n<li>MergeUpwards(T)</li>\n</ul>\n</li>\n<li>Loop:\n<ul>\n<li>Among all active dependencies, find D, the one with the highest <em>q = fee(Hp)size(Hc) - fee(Hc)size(Hp)</em>, where <em>Hp</em> is the would-be parent chunk if it were deactivated, and <em>Hc</em> the would-be child chunk.</li>\n<li>If <em>q &lt;= 0</em>, or no active dependency exists, stop.</li>\n<li>Improve(D)</li>\n</ul>\n</li>\n<li>For all chunks H, in decreasing chunk feerate order, tie-breaking by topology:\n<ul>\n<li>Output H in arbitrary topologically-valid order.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h2><a name=\"p-4171-h-3-speculation-about-complexity-14\" class=\"anchor\" href=\"#p-4171-h-3-speculation-about-complexity-14\"></a>3. Speculation about complexity</h2>\n<p>In my prototype implementation, I have used a number of techniques to make this efficient.</p>\n<p>The main one is to maintain at all times the precomputed <strong>would-be parent chunk fees and sizes</strong> for each active dependency. This is fairly expensive, but for each update (merge or split) can be computed at once for the entire chunk at roughly the same cost as would to compute it for just one dependency individually. To update after a merge:</p>\n<ul>\n<li>Let T and B be the top and bottom chunks being merged.</li>\n<li>Travel from the activated dependency outward, along active dependencies, and:\n<ul>\n<li>For each dependency D traversed downwards (reached the parent transaction before the child):\n<ul>\n<li>If D is inside T, add B\u2019s fee/size to D\u2019s would-be-parent fee/size.</li>\n<li>If D is inside B, add T\u2019s fee/size to D\u2019s would-be-parent fee/size.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>And similar for splits, except subtracting instead of adding.</p>\n<p>This has cost <span class=\"math\">O(d)</span> per update, where <span class=\"math\">d</span> is the number of active and inactive dependencies that are inside or on the border of the chunk. As this may include all dependencies in the graph, <span class=\"math\">d</span> may be up to <span class=\"math\">O(m)</span>, but by keeping track within each transaction which dependencies are active and inactive, it may be possible to bring it to <span class=\"math\">O(n)</span> (as at most <span class=\"math\">n-1</span> dependencies can be active at a given point in time).</p>\n<p>Every split step requires looking over all active dependencies, comparing their would-be parent chunk feerate with the would-be child chunk feerate, to determine which to split, which is <span class=\"math\">O(n)</span>, as there are at most <span class=\"math\">n-1</span> active dependencies. Let <span class=\"math\">s</span> be the number of improvement steps performed. The number of splits is of course <span class=\"math\">s</span>: one per improvement. The number of merges may be up to <span class=\"math\">s+n-1</span>, as we can start with up to <span class=\"math\">n</span> chunks, end with at least <span class=\"math\">1</span>, and each merge reduces the number of chunks by one, while each split increases the number by one. Lastly, each merge is <span class=\"math\">O(m)</span> to figure out what to merge with, but it may be possible to bring this down to <span class=\"math\">O(n)</span> by observing that no two dependencies from the currently-being-bubbled chunk to the same chunk are worth considering. So overall, I believe this leads to a cost of <span class=\"math\">O(m(s+n))</span>, and it may be possible to reduce it to <span class=\"math\">O(n(s+n)) = O(ns + n^2)</span>.</p>\n<p>The big question is of course what <span class=\"math\">s</span> is. Basic on fuzzing results, it seems to mostly depend on <span class=\"math\">m</span>, and be somewhat super-linear in it, perhaps <span class=\"math\">m^{3/2}</span> or <span class=\"math\">m^2</span>. Given the fact that <span class=\"math\">m</span> may be up to <span class=\"math\">n^2/4</span> itself, this leads to an overall complexity of maybe (and again, this is just based on very basic fuzzing and extrapolating the worst case numbers seen) of somewhere between <span class=\"math\">O(n^4)</span> and <span class=\"math\">O(n^6)</span>. It is also entirely possible that the worst case is exponential and I just haven\u2019t been able to discover the cases that exhibit this behavior, or it may even be the case that non-terminating examples exist.</p>\n<h3><a name=\"p-4171-h-31-goals-15\" class=\"anchor\" href=\"#p-4171-h-31-goals-15\"></a>3.1. Goals</h3>\n<p>Note however that while the best possible complexity is certainly an interesting research question, what matters practically is rather:</p>\n<ul>\n<li>What is the largest cluster size for which we can guarantee a \u201cgood enough\u201d linearization in a very short timeframe (think ~50 \u00b5s). So far, we have defined \u201cgood enough\u201d as \u201ceach chunk is at least as good as the best remaining ancestor set\u201d, as that is both efficient to compute (<span class=\"math\">O(n^2)</span>), and roughly matches the existing CPFP-aware mining algorithm in Bitcoin Core. Since this is a whole-linearization algorithm, and not an individual find-good-subset algorithm, it may be hard to incorporate actual ancestor set finding in it, but maybe an argument can be made that <span class=\"math\">k</span> iterations of spanning-forest improvement steps is qualitatively as good as ancestor set, for some function <span class=\"math\">k</span>.</li>\n<li>Beyond the minimal \u201cgood enough\u201d above, I think the real question is which algorithm has the best worst-case \u201cimprovement per unit of time\u201d, for cluster sizes within the bound established above.</li>\n</ul>",
  "post_number": 1,
  "post_type": 1,
  "updated_at": "2025-02-05T01:08:43.125Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 10,
  "reads": 19,
  "readers_count": 18,
  "score": 83.8,
  "yours": false,
  "topic_id": 1419,
  "topic_slug": "spanning-forest-cluster-linearization",
  "topic_title": "Spanning-forest cluster linearization",
  "topic_html_title": "Spanning-forest cluster linearization",
  "category_id": 8,
  "display_username": "Pieter Wuille",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 2,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "# Spanning-forest cluster linearization\n\n<div data-theme-toc=\"true\"> </div>\n\nThis is a write-up about a work in progress cluster linearization algorithm that appeared (and may still be) promising as a replacement for the ones described in the currently-implemented algorithms (Bitcoin Core PRs [30126](https://github.com/bitcoin/bitcoin/pull/30286), [30285](https://github.com/bitcoin/bitcoin/pull/30285), [30286](https://github.com/bitcoin/bitcoin/pull/30286) and Delving posts [on linearization](https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303) and [related algorithms](https://delvingbitcoin.org/t/introduction-to-cluster-linearization/1032)).\n\nWhile this spanning-forest algorithm looks promising, the recent [discovery](https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303/9) by @stefanwouldgo that well-known [minimal-cut](https://en.wikipedia.org/wiki/Minimum_cut)-based algorithms with good asymptotic complexity exist for the problem we call cluster linearization (the established term is the \"maximum-ratio [closure problem](https://en.wikipedia.org/wiki/Closure_problem)\") is even more promising.\n\nI am still creating this write-up, because despite lacking known complexity bounds, it appears to be very elegant, fast, and practical (I have a prototype implementation). It is possible that good bounds for it still get found, or that its insights help others build on top, or that it ultimately appears more practical for the real-life problems we face than more advanced algorithms. Or perhaps we discover it's actually equivalent to a well-known algorithm. I don't know, but before switching over to studying another approach, I want to have it written down in some form, also to have my own thoughts ordered.\n\n## 1. Background: cluster linearization as an LP problem\n\n(Skip this section if you're not interested in the background that led to this idea)\n\nCluster linearization can be done by repeatedly finding the highest-feerate topologically-valid subset, moving it to the front of a linearization, and then continuing with what remains of a cluster. This reduces the problem of optimal linearization to the problem of finding the highest-feerate topologically-valid subset. This problem can be formulated as a linear programming problem (credit to Dongning Guo and Aviv Zohar):\n\n* For every transaction $i$, have a variable $t_i \\in \\mathbb{R}_{\\geq 0}$. $t_i > 0$ means including transaction $i$ in the solution set; $t_i = 0$ means not including it. Using **real numbers** instead of booleans here may seem strange, but it will turn out that all non-zero $t_i$ will have the same value anyway.\n* Enforce the topology constraints by adding an **inequality per dependency**: for each parent-child pair $(p,c)$,  require $t_p \\geq t_c$. Thus, if a child is included (nonzero), so is the parent.\n* Use as **goal function** $g = \\sum_i t_i \\operatorname{fee}(i) / \\sum_i t_i \\operatorname{size}(i)$, which is the feerate of the solution set. It doesn't matter that the non-zero $t_i$ may take on values different from $1$, because they contribute equally to numerator and denominator anyway, meaning that multiplying all of them with the same value has no effect.\n* To make the goal function linear (it is a fraction so far), add one additional **normalization constraint** that the numerator is $1$: $\\sum_i t_i \\operatorname{size}(i) = 1$, and thus the goal becomes just $g = \\sum_i t_i \\operatorname{fee}(i)$.\n\nIt can be proven that any optimal solution to this problem is an optimal solution to the \"find highest-feerate topologically-valid subset\" problem.\n\nThis immediately has two implications:\n* Cluster linearization can be done in **worst-case polynomial time**, through polynomial-time LP solving algorithms, including ones based on [Interior Point Methods](https://en.wikipedia.org/wiki/Interior-point_method). These are not necessarily the best approach for our problem, but their existence conclusively settles the fact that the problem is not NP-hard for example.\n* The entire scientific domain of linear programming solution techniques becomes applicable to our problem.\n\n### 1.1. The simplex algorithm for finding high-feerate sets.\n\nProbably the best-known, and in some variation, most commonly used algorithm for solving linear programming problems is the [**simplex** algorithm](https://en.wikipedia.org/wiki/Simplex_algorithm).\n\nInternally, the simplex algorithm requires that the problem, and the goal, be stated as equations over a number of non-negative real variables, with no inequalities besides non-negativity. Our $t_i$ variables are already non-negative, but we have inequalities left. We can introduce slack variables to get rid of those:\n* Introduce a $d_j \\in \\mathbb{R}_{\\geq 0}$ for every dependency $(p_j, c_j)$: $d_j = t_{p_j} - t_{c_j}$, replacing the dependency inequalities.\n\nIn a problem with $n$ transactions and $m$ dependencies, this means $n + m + 1$ variables ($t_{1 \\ldots n}$, $d_{1 \\ldots m}$, and $g$), and $m + 2$ equations:\n* $d_j = t_{p_j} - t_{c_j}$, for $j = 1 \\ldots m$ (the dependencies)\n* $g = \\sum_i t_i \\operatorname{fee}(i)$ (the goal)\n* $\\sum_i t_i \\operatorname{size}(i) = 1$ (the normalization)\n\nThe simplex algorithm then maintains a set of $n-1$ ***free* variables**, which are chosen to be zero. All other variables (the ***basic* variables**) can be computed from the free variables using the $n+m+1$ equations. In every iteration of the algorithm, one free variable is chosen to become basic, while a basic algorithm is chosen to become free. In this context:\n* A free $t_i$ variable means transaction $i$ is excluded from the solution.\n* A free $d_j$ variable means that transactions $p_j$ and $c_j$ are either both included, or both excluded.\n\nThere are rules that control which variables are picked to become free/basic, which generally boil down to not worsening the existing solution, and keeping the solution valid. In addition, various policies exist that attempt to maximize progress in every step.\n\n### 1.2. Interpreting simplex steps in our problem space\n\nNote that groups of transactions can exist that are reachable from one another through free $d_j$ variables, and thus they must all be included in the solution set, or all excluded from it. Let's call these groups \"chunks\", for reasons that will become clear later.\n\nBecause of further rules on the choices taken by the simplex algorithm, further properties hold:\n* No \"cycle\" of free $d_j$ variables can exist within a chunk (because at least one $d_j = 0$ would be redundant).\n* Within every chunk, there can at most be 1 free $t_i$ variable (a second one would be redundant).\n* By a counting argument (exactly $n-1$ free variables) it follows that all chunks, except for one, will contain a free $t_i$ variable, and thus be excluded.\n\nOverall this means that every state of the simplex algorithm corresponds to a **partitioning of the transaction graph into chunks**, each of which is connected internally by a **spanning tree** (because no cycles) of free $d_j$ variables, and among them **one is the solution set**. In every iteration of the algorithm:\n* One free variable is made basic, which is either:\n  * A $t_i$ variable: an excluded chunk whose feerate is higher than the included chunk's feerate is made not-excluded. It may become included, or end up being merged with another (included or excluded) chunk, see below.\n  * A $d_j$ variable: a chunk breaks apart into two chunks, as the dependency between them is made basic (in a spanning tree, removing any link breaks the tree in two), where the \"top\" chunk (the one containing the parent of the $d_j$ variable) has higher feerate than the included chunk, or the \"bottom\" chunk has lower feerate than the included chunk.\n* One basic variable is made free, which is either:\n  * A $t_i$ variable: the previously-included chunk is made excluded, if the chunk made non-excluded or split off above has no (basic) dependencies on another chunk (i.e., it is topological).\n  * A $d_j$ variable otherwise: the non-excluded or newly-split off chunk merges with another chunk it depends on.\n\n### 1.3. From high-feerate subsets to full linearization\n\nThere appear to be several superfluous details to the algorithm above. It keeps track of (up to) one specific free $t_i$ variable within each chunk, but the choice of *which* transaction $i$ that is for each excluded chunk is almost entirely irrelevant to the algorithm. Furthermore, one can wonder what the point is of having to keep track of which chunk is included explicitly, as it will always just be the highest-feerate one among the chunks. We choose to make a few simplifications to the algorithm:\n* Get rid of the $t_i$ variables. The included chunk is implicitly the one with the highest-feerate; the rest is excluded.\n* Avoid comparing with the chunk feerate of the included set:\n  * To determine which $d_j$ to make basic, do not compare the top or bottom would-be chunk with the included chunk, but with each other. The rule becomes that one can split a chunk in two along a dependency, if doing so results in a top chunk with higher feerate than the bottom one.\n  * A $d_j$ variable can be made free if the chunk the child is in has a higher feerate than the top chunk (i.e., merging a chunk with a parent if it is not topological).\n  * By doing this, we're effectively not just optimizing the included chunk, but **all chunks**. And this is why they're called chunks: they become the chunks of the full linearization when the algorithm ends, with credit to @murch for noting the similarity between the algorithm and the [chunking algorithm](https://delvingbitcoin.org/t/introduction-to-cluster-linearization/1032#h-22-feerate-diagrams-and-chunking-5).\n\nThus, overall, we obtain an algorithm whose state is a [spanning forest](https://en.wikipedia.org/wiki/Spanning_tree#Spanning_forests) (a collection of spanning trees covering the whole graph) for the overall cluster, and which aims to find the overall linearization of the whole thing, not just the first chunk.\n\n## 2. The algorithm\n\nRestated from scratch, without the whole LP/Simplex background that led to it, the spanning forest linearization algorithm is:\n* Input:\n  * A transaction graph for a cluster, with fees, sizes, and dependencies between the transactions.\n* Output:\n  * An optimal linearization for the graph.\n* State:\n  * For every dependency in the graph, maintain a boolean, \"**active**\". Initially all dependencies are inactive (active dependency corresponds to \"free $d_j$\" in the previous section, but \"active\" seems like a more appropriate name when dropping the simplex context).\n  * The set of active dependencies implicitly partition the graph into **chunks**. Two transactions are in the same chunk if there is a path of active dependencies (each in either the parent-of or child-of direction) from one to another. If there is no such path, the transactions are in different chunks. This means that the initial state is every transaction in its own singleton chunk.\n  * An invariant of the algorithm is that no cycles can exist within the active dependencies (again, ignoring direction). In other words, the active dependencies within each chunk form a spanning tree, and the active dependencies for the entire cluster form a **spanning forest** for it.\n* Algorithm:\n  * Perform one of these permitted steps as long as any apply:\n    1. **Merging chunks**. If there is an inactive dependency, where the parent and child are in distinct chunks, and the child chunk has higher feerate than the parent chunk, make the dependency active, merging the two chunks.\n    2. **Splitting chunks**. If there is an active dependency, consider the two chunks that would be created by making it inactive (due to the spanning-forest property, this is the case for every active dependency). If the would-be chunk with the parent of the dependency in has a higher feerate than the child chunk, make the dependency inactive, splitting the chunk in two.\n  * Output the final chunks, sorted from high feerate to low feerate, tie-breaking by topology, each chunk internally sorted by an arbitrary for valid topological order (e.g., by increasing number of ancestors).\n\n### 2.1. Correctness\n\nSince several changes were made to the simplex algorithm along the way to obtain the spanning forest algorithm, it is not necessarily the case anymore that it is correct.\n\nThe algorithm above, **if it terminates**, outputs an optimal linearization, or put otherwise: the successively output chunks are each a highest-feerate topological subset of what remains of the cluster. The [theory thread](https://delvingbitcoin.org/t/cluster-mempool-definitions-theory/202) proves that this suffices for an optimal linearization. To see the output chunks are optimal:\n* There cannot be any higher-feerate chunk that depends on a lower-feerate chunk, because if there was, the chunk merging rule above would still apply, and the algorithm has not terminated. Thus, the chunks that come out, from high feerate to low feeate, form a **valid linearization**.\n* As for optimality, it holds that the highest-feerate chunk is actually the highest-feerate topologically-valid subset of the **active subgraph** of the cluster. Since fewer dependencies means fewer constraints, the highest-feerate set for just the active subgraph is certainly no worse than the highest-feerate set for the actual, more constrained, problem. To see why the highest-feerate chunk is the highest-feerate topologically-valid subset of the active subgraph:\n  * It is certainly topologically-valid, because it is topologically-valid for the actual problem (see above).\n  * Imagine a higher-feerate topologically-valid subset of the active subgraph existed. Consider its intersections with the final chunks in the algorithm. At least one of those must have a feerate above the highest-feerate chunk itself (if not, their combined feerate, which is a weighted average, would not be higher either). That intersection must be obtainable by cutting off certain branches of the spanning tree, and some of those branches must have a feerate lower than the chunk feerate (otherwise cutting them off would not be an improvement). However, because the splitting rule of the algorithm is not applicable (or the algorithm wouldn't have terminated), no such branch can exist (if it did, the chunk would have been split in two right there).\n* All the properties above remain valid after the highest-feerate chunk is removed from the graph, which is exactly the state obtained before the next chunk is output.\n\nNote that all of these properties are conditional on the assumption that the algorithm terminates. It is less clear under what conditions that is the case (see below).\n\n### 2.2. Refining the merge and split choices\n\nSo far, we have left unspecified *which* dependencies are to be made active/inactive. From casual fuzzing, it appears that just making random choices is sufficient to obtain the optimal result, but random choices **can result in repeated states**, which means it can include pointless work which we'd like to avoid.\n\nThanks to @ajtowns and @ClaraShk for the discussions that led to the conclusions in this section.\n\n#### 2.2.1. Prioritize merge over split\n\nOne easy refinement, which will simplify further analysis, is to prioritize merging over splitting. The idea is that merging is making the state *topological*, while splitting is about making the state *better*. If we, after every improving split step, perform merges as long as possible, we make the state topological as soon as possible again. This has the practical advantage that if we want to stop the algorithm (due to running out of time), we end up with something that's at least valid.\n\n#### 2.2.2. Merge by highest feerate difference\n\nWhen performing merges, there may be multiple inactive dependencies where the child chunk has a higher feerate than the parent, in which case it is unclear which one to pick. From casual fuzzing, it appears that prioritizing the one where the feerate *difference* between the child and parent is maximal has a number of advantages:\n\n* The same state is never repeated, which - if true - must imply that some (possibly minute) strict improvement happens every iteration.\n* It appears to be a good choice for minimizing the number of iterations needed in the worst case.\n\nFurthermore, it simplifies reasoning about termination. Call an \"**improvement step**\" to be one application of the splitting rule (using whatever criterion to select which one), followed by as many applications of the merging rule (using maximum feerate difference first as selection strategy) as possible. If the state was topological (= no merge steps applicable) before an improvement step, we can compare the feerate diagrams of the linearizations that would be output before and after the improvement step.\n\nThink of the state of the chunks implicitly as an ordered list, from higher-feerate to lower-feerate, plus the fact that chunks can only depend on chunks that come before it. When a split happens, there are two possibilities:\n* The higher-feerate split-off chunk (the parent chunk) depends (through another inactive dependency) on the lower-feerate one (the child chunk). Since the produced child has lower feerate than any other chunk it may depend on, the maximum-feerate-difference rule in this case says that it must **merge with itself again**, even if other merges are possible. In this case, the feerate diagram remains unchanged, because we end up with the same chunks as before the improvement step, though with a different spanning tree in the re-merged chunk. Whether this can end up in an infinite loop, and if not, how many iterations improving the same chunk this way can happen is an open question.\n* If such a self-merge does not happen (because the split-off parent chunk does not depend on the split-off child), imagine that the two new chunks initially take on adjacent positions in the sorted chunk list (which is now no longer properly sorted), where the split chunk used to, with the parent one first. We can look at what happens to the parent and child chunk separately:\n  * The higher-feerate split-off chunk can in this case only depends on chunks that used to precede it, but it may have a higher feerate than those. The maximum-feerate-difference rule now effectively boils down to this new chunk \"**bubbling up**\" (think: bubble sort) until it either ends up in a position where its feerate is no longer higher than what precedes it, or it finds a lower-feerate chunk it depends on, merging with it, and then continuing the process bubbling this merged chunk further up possibly. This is very similar to the behavior inside the [post-linearization algorithm](https://delvingbitcoin.org/t/linearization-post-processing-o-n-2-fancy-chunking/201) described earlier.\n  * The same process, but reversed, happens to the split-off child. Its feerate dropped compared to the earlier chunk, so now chunks that used to succeed it may now have higher feerate than it. So the split-off child chunk starts **bubbling down**, until it finds a position in the sort chunk list where it has higher feerate than what follows, or when it encounters a higher-feerate chunk that depends on it, where it merges, and possibly continuing further.\n\nWhenever a self-merge does not happen, every improvement step strictly improves the feerate diagram, by moving a higher-feerate part up, and a lower-feerate part down, each potentially merging with other chunks. So the question of termination is just about improvement steps that result in a self-merge.\n\n#### 2.2.3. Split by maximizing *(fee<sub>parent</sub> size<sub>child</sub> - fee<sub>child</sub> size<sub>parent</sub>)*\n\nIt remains to be discussed how to decide what split to perform within an improvement step when there are multiple possible options. If it is the case that every improvement step is guaranteed to make some kind of progress, it may be acceptable to just **make improvement steps randomly**, but perhaps we can do better.\n\nGoing back to the LP formulation of the problem, it is a requirement that the derivative of the goal variable $g$ w.r.t. the variable being made free (in the case of a split, the $d_j$ variable) is positive (this translated to needing a would-be parent chunk to having higher feerate than the child). A natural choice is picking the one with the **highest derivative** in the simplex phrasing. Working that out corresponds to maximizing the function\n\n$$\n\\begin{equation}\n\\begin{split}\nq(A, B) & = \\,\\, & \\left(\\operatorname{feerate}(A) - \\operatorname{feerate}(B)\\right) \\operatorname{size}(A) \\operatorname{size}(B) \\\\ \n& = & \\left(\\frac{\\operatorname{fee}(A)}{\\operatorname{size}(A)} - \\frac{\\operatorname{fee}(B)}{\\operatorname{size}(B)}\\right)\\operatorname{size}(A)\\operatorname{size}(B) \\\\\n& = & \\operatorname{fee}(A)\\operatorname{size}(B) - \\operatorname{fee}(B)\\operatorname{size}(A)\n\\end{split}\n\\end{equation}\n$$\n\nfor A the would-be parent chunk and B the would-be child chunk. And from casual fuzzing, this indeed appears to be a good choice, with apparently relatively low worst-case numbers of iterations. Furthermore, it appears to result in **never repeating the same *split*** (in the sense that the same chunk is never split in the same parent and child chunk, ignoring the exact spanning tree it has. Other rules (e.g. maximizing feerate difference directly when splitting) don't seem to have this property: while they don't repeat the exact same state, they do appear to result in sometimes repeating the same splits a finite number of times.\n\nAnother insight is that *if* a split step does not result in a self-merge (see above), $q(A,B)/2$ is a lower bound on **increase in surface area** (integral) under the feerate diagram curve, which is directly related to our overall goal (because the optimal linearization necessarily has the highest-possible surface area under the curve).\n\nThe $q()$ function has other interesting properties too, which may or may not matter:\n* $q(A,B) = \\sum_{i \\in A} \\sum_{j \\in B} q(i,j)$ (i.e., it is a [bilinear map](https://en.wikipedia.org/wiki/Bilinear_map)).\n* $q(A,B) = q(A, A \\cup B) = q(A \\cup B, B)$\n\n### 2.3. Initial state\n\nWe started with stating that the initial state is all dependencies inactive (i.e., all transactions in their own chunk). But that state is typically not topologically valid, which is a requirement for the analysis of the improvement steps above.\n\nIt is possible to just start with a general merging step to avoid this: even just merging everything randomly would suffice. But we can do better. In practice, we always **start with a known linearization** for a cluster, and the goal is improving it, rather than finding a new linearization from scratch.\n\nWith that, we can use the following approach:\n* Iterate over all transactions in the input linearization, from front to back. And for each:\n  * Perform a bubbling-up merge on the chunk that transaction is in (initially a singleton).\n\nThe result is actually **exactly the [post-linearization algorithm](https://delvingbitcoin.org/t/linearization-post-processing-o-n-2-fancy-chunking/201)**, but in the spanning-forest setting. This means that the chunks that come out, even without any further improvement steps, will form a linearization at least as good as the input linearization. Any work done on top just makes it even better.\n\nIn other words, we can see this as natural linearization-improvement algorithm, avoiding the need for [LIMO](https://delvingbitcoin.org/t/limo-combining-the-best-parts-of-linearization-search-and-merging/825) or explicit [linearization merging](https://delvingbitcoin.org/t/merging-incomparable-linearizations/209). A downside is that it is unclear how to incorporate a \"make sure every next chunk is at least as good as the highest-feerate ancestor set among what remains\", without explicitly computing such a linearization and merging with it. It is unclear how big of a problem that is.\n\n### 2.4. Resulting combined algorithm\n\nPutting all the pieces above together, we get:\n\n* SpanningTreeLinearize(C, L)\n  * Helper function: MergeUpwards(T)\n    * Loop:\n      * Find the chunk H which transaction T is in.\n      * Among all other chunks H', which H has an (inactive) dependency on, find the lowest-feerate one.\n      * If no H' is found, stop loop.\n      * Activate a dependency between H and H'.\n  * Helper function: MergeDownwards(T):\n    * Loop:\n      * Find the chunk H which transaction T is in.\n      * Among all other chunks H' which have an (inactive) dependency on H, find the highest-feerate one.\n      * If no H' is found, stop loop.\n      * Activate a dependency between H and H'.\n  * Helper function: Improve(D)\n    * Deactivate D, resulting in chunks Hp and Hc.\n    * If Hc has a dependency on Hp, activate it and stop.\n    * Otherwise:\n      * MergeUpwards(Hp)\n      * MergeDownwards(Hc)\n  * Initialize state with all dependencies of C as inactive.\n  * For each transaction T in L:\n    * MergeUpwards(T)\n  * Loop:\n    * Among all active dependencies, find D, the one with the highest *q = fee(Hp)size(Hc) - fee(Hc)size(Hp)*, where *Hp* is the would-be parent chunk if it were deactivated, and *Hc* the would-be child chunk.\n    * If *q <= 0*, or no active dependency exists, stop.\n    * Improve(D)\n  * For all chunks H, in decreasing chunk feerate order, tie-breaking by topology:\n    * Output H in arbitrary topologically-valid order.\n## 3. Speculation about complexity\n\nIn my prototype implementation, I have used a number of techniques to make this efficient.\n\nThe main one is to maintain at all times the precomputed **would-be parent chunk fees and sizes** for each active dependency. This is fairly expensive, but for each update (merge or split) can be computed at once for the entire chunk at roughly the same cost as would to compute it for just one dependency individually. To update after a merge:\n* Let T and B be the top and bottom chunks being merged.\n* Travel from the activated dependency outward, along active dependencies, and:\n  * For each dependency D traversed downwards (reached the parent transaction before the child):\n    * If D is inside T, add B's fee/size to D's would-be-parent fee/size.\n    * If D is inside B, add T's fee/size to D's would-be-parent fee/size.\n\nAnd similar for splits, except subtracting instead of adding.\n\nThis has cost $O(d)$ per update, where $d$ is the number of active and inactive dependencies that are inside or on the border of the chunk. As this may include all dependencies in the graph, $d$ may be up to $O(m)$, but by keeping track within each transaction which dependencies are active and inactive, it may be possible to bring it to $O(n)$ (as at most $n-1$ dependencies can be active at a given point in time).\n\nEvery split step requires looking over all active dependencies, comparing their would-be parent chunk feerate with the would-be child chunk feerate, to determine which to split, which is $O(n)$, as there are at most $n-1$ active dependencies. Let $s$ be the number of improvement steps performed. The number of splits is of course $s$: one per improvement. The number of merges may be up to $s+n-1$, as we can start with up to $n$ chunks, end with at least $1$, and each merge reduces the number of chunks by one, while each split increases the number by one. Lastly, each merge is $O(m)$ to figure out what to merge with, but it may be possible to bring this down to $O(n)$ by observing that no two dependencies from the currently-being-bubbled chunk to the same chunk are worth considering. So overall, I believe this leads to a cost of $O(m(s+n))$, and it may be possible to reduce it to $O(n(s+n)) = O(ns + n^2)$.\n \nThe big question is of course what $s$ is. Basic on fuzzing results, it seems to mostly depend on $m$, and be somewhat super-linear in it, perhaps $m^{3/2}$ or $m^2$. Given the fact that $m$ may be up to $n^2/4$ itself, this leads to an overall complexity of maybe (and again, this is just based on very basic fuzzing and extrapolating the worst case numbers seen) of somewhere between $O(n^4)$ and $O(n^6)$. It is also entirely possible that the worst case is exponential and I just haven't been able to discover the cases that exhibit this behavior, or it may even be the case that non-terminating examples exist.\n\n### 3.1. Goals\n\nNote however that while the best possible complexity is certainly an interesting research question, what matters practically is rather:\n* What is the largest cluster size for which we can guarantee a \"good enough\" linearization in a very short timeframe (think ~50 \u00b5s). So far, we have defined \"good enough\" as \"each chunk is at least as good as the best remaining ancestor set\", as that is both efficient to compute ($O(n^2)$), and roughly matches the existing CPFP-aware mining algorithm in Bitcoin Core. Since this is a whole-linearization algorithm, and not an individual find-good-subset algorithm, it may be hard to incorporate actual ancestor set finding in it, but maybe an argument can be made that $k$ iterations of spanning-forest improvement steps is qualitatively as good as ancestor set, for some function $k$.\n* Beyond the minimal \"good enough\" above, I think the real question is which algorithm has the best worst-case \"improvement per unit of time\", for cluster sizes within the bound established above.",
  "actions_summary": [
    {
      "id": 2,
      "count": 2
    }
  ],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 96,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "reactions": [
    {
      "id": "+1",
      "type": "emoji",
      "count": 2
    }
  ],
  "current_user_reaction": null,
  "reaction_users_count": 2,
  "current_user_used_main_reaction": false
}