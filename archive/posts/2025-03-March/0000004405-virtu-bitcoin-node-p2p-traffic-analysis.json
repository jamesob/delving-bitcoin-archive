{
  "id": 4405,
  "name": "",
  "username": "virtu",
  "avatar_template": "/user_avatar/delvingbitcoin.org/virtu/{size}/540_2.png",
  "created_at": "2025-03-03T14:01:18.346Z",
  "cooked": "<p>I\u2019d like to share some of my recent node traffic analyses in case someone else might find the method for making estimates estimates or some of the insights enabled by the estimates useful.</p>\n<p>I initially did this work to figure out what kind of bandwidth savings to expect for Erlay, but I think the approach and data might be useful for other P2P optimizations as well.</p>\n<p>While measuring overall node traffic is trivial (e.g., via <code>iptables</code>,\n<code>systemd</code>\u2019s IP accounting), there\u2019s hardly anything useful to learn from it. In need of a useful traffic breakdowns through various lenses, I decided to\ntry to estimate TCP/IP traffic using data collected via the the\n<a href=\"https://github.com/bitcoin/bitcoin/blob/3c1f72a36700271c7c1293383549c3be29f28edb/doc/tracing.md#tracepoint-netinbound_message\"><code>net:inbound_message</code></a>\nand <a><code>net:outbound_message</code></a>\ntracepoints.</p>\n<p>Here\u2019s how it works. Based on payload sizes reported by the tracepoints, the P2P\nmessage size is computed by adding the P2P message overhead (24 bytes, made up\nup a 4-byte magic, 12-byte command, and 4-byte each for payload length and\nchecksum). Next, the number of TCP segments required to transmit the message is\nestimated by dividing the the P2P message size by the TCP segment size, where\nthe TCP segment size is estimated to be the difference between the maximum\ntransfer unit (MTU), which was assumed to be 1500 bytes, and the sum of the IP\nand TCP header sizes, the former of which was assumed to be 20 bytes in case of\nIPv4 and 40 bytes in case of IPv6 and the latter to be 32 bytes (made up of the\ndefault TCP header size of 20 bytes plus a ten-byte timestamp used by default by\nthe Linux kernel to make real-time round-trip measurements plus two padding\nbytes to make the TCP header options to align to 32-bit boundaries). Next, the\nTCP/IP header overhead is computed by multiplying the number of TCP segments by\nthe header overhead (which is the sum of IP and TCP headers). Also, since since\nACKs are generally sent for every two TCP packets, the ACK overhead is estimated\nto be half the number of TCP segments times the sum of IP plus TCP header sizes.\nThe final TCP/IP traffic estimate then corresponds to the sum of the P2P message\nsize, the TCP/IP header overhead and the ACK overhead.</p>\n<p>While all of this might sound rather convoluted to someone unfamiliar with the TCP/IP stack, it\u2019s actually a first-principles-based analytic estimate.</p>\n<p>Let\u2019s have a look at some data to get an idea of the accuracy of traffic estimates made that way.\nThe graph below shows tracepoint-based hourly traffic estimates for different P2P message types (types with negligible amount of traffic have been grouped into the other category) over time. Individual message types are stacked on top of each other so they should add up to total node traffic. For validation, the black line shows total node traffic as reported by <code>systemd</code>\u2019s IP accounting. Note that during the time data was recorded, the node underwent four different modes of operations, which are accompanied by significant changes in hourly bandwidth, so the graph uses a log-scale on the <em>y</em>-axis to make the data readable:</p>\n<ul>\n<li>During the first couple of hours, you can see tens of GB of block messages, which of course correspond to IBD.</li>\n<li>Afterwards, when hourly traffic drops to tens of MB per hour, the node was running for a couple of weeks without being reachable until around Feb 12.</li>\n<li>From Feb 12 to Feb 18, the node was made reachable, but was run in pruned mode, and traffic increases slightly, because the number of peer connection increases by an order of magnitude due to inbound connections.</li>\n<li>Finally, I attached a disk from a non-pruned node and disabled pruning on the node under observation as well. The upshot being that peers could now involve my node during their IBD, cause total node traffic to surge to ~10GB per hour.</li>\n</ul>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/2X/e/e403a3d58114afa82016ac6768cdbc7412af9e5d.jpeg\" data-download-href=\"https://delvingbitcoin.org/uploads/default/e403a3d58114afa82016ac6768cdbc7412af9e5d\" title=\"validation-hourly+all+logscale\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/2X/e/e403a3d58114afa82016ac6768cdbc7412af9e5d_2_690x368.jpeg\" alt=\"validation-hourly+all+logscale\" data-base62-sha1=\"wx6F2fDgn5mtu4O3jWl4MDPds8l\" width=\"690\" height=\"368\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/2X/e/e403a3d58114afa82016ac6768cdbc7412af9e5d_2_690x368.jpeg, https://delvingbitcoin.org/uploads/default/optimized/2X/e/e403a3d58114afa82016ac6768cdbc7412af9e5d_2_1035x552.jpeg 1.5x, https://delvingbitcoin.org/uploads/default/optimized/2X/e/e403a3d58114afa82016ac6768cdbc7412af9e5d_2_1380x736.jpeg 2x\" data-dominant-color=\"E4C4B8\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">validation-hourly+all+logscale</span><span class=\"informations\">1920\u00d71026 183 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>Looking at the data, one might get the feeling that the stacked estimates and\nthe measurement (black line) are in very good agreement all of the time. But a\nlog-scale graph is a bad way to judge the agreement visually, so the next two\ngraphs use a linear <em>y</em>-axis scale, and focus on the time the node was not\nreachable, making it much easier to judge the discrepancy between estimate\nand measurement. The graph on the left shows daily traffic, while the one on the\nright shows hourly data. In both cases, the average model error hovers at around\n1% with a maximum error of around 2%. Good enough in my book.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/2X/e/e1a22320b94d411ac31dc8995ac300c27c738c4d.jpeg\" data-download-href=\"https://delvingbitcoin.org/uploads/default/e1a22320b94d411ac31dc8995ac300c27c738c4d\" title=\"validation-non-listening\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/2X/e/e1a22320b94d411ac31dc8995ac300c27c738c4d_2_690x256.jpeg\" alt=\"validation-non-listening\" data-base62-sha1=\"wc2O6RUgErieioDrPLwZFmCy0VL\" width=\"690\" height=\"256\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/2X/e/e1a22320b94d411ac31dc8995ac300c27c738c4d_2_690x256.jpeg, https://delvingbitcoin.org/uploads/default/optimized/2X/e/e1a22320b94d411ac31dc8995ac300c27c738c4d_2_1035x384.jpeg 1.5x, https://delvingbitcoin.org/uploads/default/optimized/2X/e/e1a22320b94d411ac31dc8995ac300c27c738c4d_2_1380x512.jpeg 2x\" data-dominant-color=\"D0DDCF\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">validation-non-listening</span><span class=\"informations\">1920\u00d7714 121 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>So, after demonstrating the method works, we can safely use its estimates for interpreting node traffic. For example, breaking down traffic by message type can help us assess potential optimizations. In the case of Erlay, I think it\u2019s safe to say that there\u2019s significant bandwidth-saving potential for non-listening (where daily INV traffic is more than 100MB out of a total of around 500-600MB) and pruned nodes; for listening nodes, where INV traffic rises by an order of magnitude but total node traffic is in the hundreds of GB per day: hardly gonna make a difference.</p>\n<p>So, what else can you do with this approach other than breaking traffic down by message type? For one, you can break down traffic by payload (what\u2019s in the P2P message) and overhead (e.g. the P2P message header overhead and TCP/IP overhead), which can be a useful tool to measure the impact of message buffer timer optimizations.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/2X/f/f13d63908cfdb4f7f4840d8e9d573f68770763ab.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/f13d63908cfdb4f7f4840d8e9d573f68770763ab\" title=\"breakdown-inv-absolute\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/2X/f/f13d63908cfdb4f7f4840d8e9d573f68770763ab_2_690x443.png\" alt=\"breakdown-inv-absolute\" data-base62-sha1=\"yq6AVgnGGfTK9aOckAE0bnovKv9\" width=\"690\" height=\"443\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/2X/f/f13d63908cfdb4f7f4840d8e9d573f68770763ab_2_690x443.png, https://delvingbitcoin.org/uploads/default/optimized/2X/f/f13d63908cfdb4f7f4840d8e9d573f68770763ab_2_1035x664.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/2X/f/f13d63908cfdb4f7f4840d8e9d573f68770763ab_2_1380x886.png 2x\" data-dominant-color=\"E8E3EE\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">breakdown-inv-absolute</span><span class=\"informations\">2851\u00d71831 238 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>You can also break down traffic by connection types as shown in the graph below. Again, the graph shows the different stages the node was in during the time data was collected. During the first days, block data for IBD is received via both regular outbound and block-relay-only connections. After IBD, the traffic for all connection types stabilizes. After making the node reachable, but pruned, a small amount of traffic from incoming connections becomes visible (NB that it\u2019s actually more traffic than the outbound connections because there\u2019s many more inbound than outbound connections, but this is visually skewed by the log scale on the <em>y</em> axis). After disabling pruning, traffic by incoming connections grows significantly as peers start using my node for IBD. A breakdown such as this could be useful in verifying the impact of optimizations such as raising the number of block-only connections, which while significantly improving against eclipse attacks should not scale up traffic a lot.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/2X/a/acdc3551289a6cbf08936d7ce8177030f8727009.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/acdc3551289a6cbf08936d7ce8177030f8727009\" title=\"total_traffic_by_connection_type\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/2X/a/acdc3551289a6cbf08936d7ce8177030f8727009_2_690x368.png\" alt=\"total_traffic_by_connection_type\" data-base62-sha1=\"oFbUYtFropLiJEOrRTnXeQmHeyd\" width=\"690\" height=\"368\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/2X/a/acdc3551289a6cbf08936d7ce8177030f8727009_2_690x368.png, https://delvingbitcoin.org/uploads/default/optimized/2X/a/acdc3551289a6cbf08936d7ce8177030f8727009_2_1035x552.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/2X/a/acdc3551289a6cbf08936d7ce8177030f8727009_2_1380x736.png 2x\" data-dominant-color=\"C2BBAE\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">total_traffic_by_connection_type</span><span class=\"informations\">3166\u00d71690 244 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>Also, you can also break down per-peer traffic, which I found instructive to test and fill in some missing numbers of my mental models about overall P2P networks dynamics. In the plot below, each dot, color-coded by connection type, corresponds to the total TCP/IP traffic exchanged with a peer on a given day. Note that, since we might get disconnected by a peer at any moment and will then connect to a new one, there might be more dots in a connection type category than the number of slots for that type.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/2X/9/9ae89dd041eebb51ab6dfc2f21bb9a74cc9ee2ba.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/9ae89dd041eebb51ab6dfc2f21bb9a74cc9ee2ba\" title=\"daily-peer-traffic\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/2X/9/9ae89dd041eebb51ab6dfc2f21bb9a74cc9ee2ba_2_690x367.png\" alt=\"daily-peer-traffic\" data-base62-sha1=\"m6nUlRFRYGqxw3LrIGLnkG0fH5g\" width=\"690\" height=\"367\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/2X/9/9ae89dd041eebb51ab6dfc2f21bb9a74cc9ee2ba_2_690x367.png, https://delvingbitcoin.org/uploads/default/optimized/2X/9/9ae89dd041eebb51ab6dfc2f21bb9a74cc9ee2ba_2_1035x550.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/2X/9/9ae89dd041eebb51ab6dfc2f21bb9a74cc9ee2ba_2_1380x734.png 2x\" data-dominant-color=\"F5F6F7\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">daily-peer-traffic</span><span class=\"informations\">3169\u00d71690 397 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>\n<p>Unsurprisingly, we find both block-only and outbound connections with high traffic during IBD. When the node was bootstrapped but not reachable, we see traffic for both connection types drop and exhibit a level of stability that feels reasonable.</p>\n<p>The first inbound connections appear on Feb 18. When pruning was disabled shortly before Feb 18, high-traffic incoming connections with more than 100MB of traffic being to appear.</p>\n<p>I believe incoming nodes can be grouped into three categories: IBD nodes, regular nodes, and spy nodes.</p>\n<ul>\n<li>Regular nodes: My node\u2019s outbound full relay connections (purple) are used for regular node maintenance (receiving INVs, TXs, BLOCKs, etc) and exhibit daily traffic between 10MB-100MB. Since my node\u2019s outbound connections correspond to incoming connections on another node, incoming connections on my end that serve regular node maintenance should thus exhibit similar traffic patterns, which some of my node\u2019s inbound connections do. Especially from Feb 12 to Feb 18, when my node was originally made reachable but running in pruned mode.</li>\n<li>IBD nodes: I think it\u2019s safe to conclude that those incoming peer connections with more than 100MB of daily traffic correspond to nodes using my node for IBD, with the most obvious piece of evidence being that none of these high-traffic peers existed when my node was pruned.</li>\n<li>Spy nodes: Now interestingly, the bulk of inbound peers exchange only around 1MB of traffic with my node, which is too low (using traffic via my outbound connections as baseline) for them to be regular connections. All those nodes do is complete the P2P handshake, and politely reply to ping messages. Other than that, they just suck up our INV messages. Now one might argue these might be SPV nodes but they\u2019re definitely not (the most obvious give away is that they never send any SPV-related messages; in fact, all they ever send is a single <code>VERSION</code> and a single <code>ADDR</code> message, as well as <code>PONG</code>s; the fact that they\u2019re coming from a limited bunch of IPs or IP ranges doesn\u2019t help)</li>\n</ul>\n<p>If someone has any more ideas about useful data to generate, let me know and I\u2019ll try to visualize it. If you want to play around yourself, I\u2019ve put all of the <a href=\"https://github.com/virtu/bitcoind-p2p-traffic\">Jupyter notebooks on GitHub</a>. Unfortunately, raw tracepoint data was too big for GitHub but I\u2019ve left some data that\u2019s been grouped to hourly and daily resolution in the repo; if you want access to some raw tracepoint data, just let me know where to scp or ftp it.</p>",
  "post_number": 1,
  "post_type": 1,
  "updated_at": "2025-03-03T14:06:21.413Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 0,
  "reads": 9,
  "readers_count": 8,
  "score": 46.8,
  "yours": false,
  "topic_id": 1490,
  "topic_slug": "bitcoin-node-p2p-traffic-analysis",
  "topic_title": "Bitcoin node P2P traffic analysis",
  "topic_html_title": "Bitcoin node P2P traffic analysis",
  "category_id": 8,
  "display_username": "",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 2,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "I'd like to share some of my recent node traffic analyses in case someone else might find the method for making estimates estimates or some of the insights enabled by the estimates useful.\n\nI initially did this work to figure out what kind of bandwidth savings to expect for Erlay, but I think the approach and data might be useful for other P2P optimizations as well.\n\nWhile measuring overall node traffic is trivial (e.g., via `iptables`,\n`systemd`'s IP accounting), there's hardly anything useful to learn from it. In need of a useful traffic breakdowns through various lenses, I decided to\ntry to estimate TCP/IP traffic using data collected via the the\n[`net:inbound_message`](https://github.com/bitcoin/bitcoin/blob/3c1f72a36700271c7c1293383549c3be29f28edb/doc/tracing.md#tracepoint-netinbound_message)\nand [`net:outbound_message`](`https://github.com/bitcoin/bitcoin/blob/3c1f72a36700271c7c1293383549c3be29f28edb/doc/tracing.md#tracepoint-netinbound_message)\ntracepoints.\n\nHere's how it works. Based on payload sizes reported by the tracepoints, the P2P\nmessage size is computed by adding the P2P message overhead (24 bytes, made up\nup a 4-byte magic, 12-byte command, and 4-byte each for payload length and\nchecksum). Next, the number of TCP segments required to transmit the message is\nestimated by dividing the the P2P message size by the TCP segment size, where\nthe TCP segment size is estimated to be the difference between the maximum\ntransfer unit (MTU), which was assumed to be 1500 bytes, and the sum of the IP\nand TCP header sizes, the former of which was assumed to be 20 bytes in case of\nIPv4 and 40 bytes in case of IPv6 and the latter to be 32 bytes (made up of the\ndefault TCP header size of 20 bytes plus a ten-byte timestamp used by default by\nthe Linux kernel to make real-time round-trip measurements plus two padding\nbytes to make the TCP header options to align to 32-bit boundaries). Next, the\nTCP/IP header overhead is computed by multiplying the number of TCP segments by\nthe header overhead (which is the sum of IP and TCP headers). Also, since since\nACKs are generally sent for every two TCP packets, the ACK overhead is estimated\nto be half the number of TCP segments times the sum of IP plus TCP header sizes.\nThe final TCP/IP traffic estimate then corresponds to the sum of the P2P message\nsize, the TCP/IP header overhead and the ACK overhead.\n\nWhile all of this might sound rather convoluted to someone unfamiliar with the TCP/IP stack, it's actually a first-principles-based analytic estimate.\n\nLet's have a look at some data to get an idea of the accuracy of traffic estimates made that way.\nThe graph below shows tracepoint-based hourly traffic estimates for different P2P message types (types with negligible amount of traffic have been grouped into the other category) over time. Individual message types are stacked on top of each other so they should add up to total node traffic. For validation, the black line shows total node traffic as reported by `systemd`'s IP accounting. Note that during the time data was recorded, the node underwent four different modes of operations, which are accompanied by significant changes in hourly bandwidth, so the graph uses a log-scale on the _y_-axis to make the data readable: \n- During the first couple of hours, you can see tens of GB of block messages, which of course correspond to IBD.\n- Afterwards, when hourly traffic drops to tens of MB per hour, the node was running for a couple of weeks without being reachable until around Feb 12.\n- From Feb 12 to Feb 18, the node was made reachable, but was run in pruned mode, and traffic increases slightly, because the number of peer connection increases by an order of magnitude due to inbound connections.\n- Finally, I attached a disk from a non-pruned node and disabled pruning on the node under observation as well. The upshot being that peers could now involve my node during their IBD, cause total node traffic to surge to ~10GB per hour.\n\n![validation-hourly+all+logscale|690x368](upload://wx6F2fDgn5mtu4O3jWl4MDPds8l.jpeg)\n\nLooking at the data, one might get the feeling that the stacked estimates and\nthe measurement (black line) are in very good agreement all of the time. But a\nlog-scale graph is a bad way to judge the agreement visually, so the next two\ngraphs use a linear _y_-axis scale, and focus on the time the node was not\nreachable, making it much easier to judge the discrepancy between estimate\nand measurement. The graph on the left shows daily traffic, while the one on the\nright shows hourly data. In both cases, the average model error hovers at around\n1% with a maximum error of around 2%. Good enough in my book.\n\n![validation-non-listening|690x256](upload://wc2O6RUgErieioDrPLwZFmCy0VL.jpeg)\n\nSo, after demonstrating the method works, we can safely use its estimates for interpreting node traffic. For example, breaking down traffic by message type can help us assess potential optimizations. In the case of Erlay, I think it's safe to say that there's significant bandwidth-saving potential for non-listening (where daily INV traffic is more than 100MB out of a total of around 500-600MB) and pruned nodes; for listening nodes, where INV traffic rises by an order of magnitude but total node traffic is in the hundreds of GB per day: hardly gonna make a difference.\n\nSo, what else can you do with this approach other than breaking traffic down by message type? For one, you can break down traffic by payload (what's in the P2P message) and overhead (e.g. the P2P message header overhead and TCP/IP overhead), which can be a useful tool to measure the impact of message buffer timer optimizations.\n\n![breakdown-inv-absolute|690x443](upload://yq6AVgnGGfTK9aOckAE0bnovKv9.png)\n\nYou can also break down traffic by connection types as shown in the graph below. Again, the graph shows the different stages the node was in during the time data was collected. During the first days, block data for IBD is received via both regular outbound and block-relay-only connections. After IBD, the traffic for all connection types stabilizes. After making the node reachable, but pruned, a small amount of traffic from incoming connections becomes visible (NB that it's actually more traffic than the outbound connections because there's many more inbound than outbound connections, but this is visually skewed by the log scale on the _y_ axis). After disabling pruning, traffic by incoming connections grows significantly as peers start using my node for IBD. A breakdown such as this could be useful in verifying the impact of optimizations such as raising the number of block-only connections, which while significantly improving against eclipse attacks should not scale up traffic a lot.\n\n![total_traffic_by_connection_type|690x368](upload://oFbUYtFropLiJEOrRTnXeQmHeyd.png)\n\nAlso, you can also break down per-peer traffic, which I found instructive to test and fill in some missing numbers of my mental models about overall P2P networks dynamics. In the plot below, each dot, color-coded by connection type, corresponds to the total TCP/IP traffic exchanged with a peer on a given day. Note that, since we might get disconnected by a peer at any moment and will then connect to a new one, there might be more dots in a connection type category than the number of slots for that type.\n\n![daily-peer-traffic|690x367](upload://m6nUlRFRYGqxw3LrIGLnkG0fH5g.png)\n\nUnsurprisingly, we find both block-only and outbound connections with high traffic during IBD. When the node was bootstrapped but not reachable, we see traffic for both connection types drop and exhibit a level of stability that feels reasonable.\n\nThe first inbound connections appear on Feb 18. When pruning was disabled shortly before Feb 18, high-traffic incoming connections with more than 100MB of traffic being to appear.\n\nI believe incoming nodes can be grouped into three categories: IBD nodes, regular nodes, and spy nodes.\n- Regular nodes: My node's outbound full relay connections (purple) are used for regular node maintenance (receiving INVs, TXs, BLOCKs, etc) and exhibit daily traffic between 10MB-100MB. Since my node's outbound connections correspond to incoming connections on another node, incoming connections on my end that serve regular node maintenance should thus exhibit similar traffic patterns, which some of my node's inbound connections do. Especially from Feb 12 to Feb 18, when my node was originally made reachable but running in pruned mode.\n- IBD nodes: I think it's safe to conclude that those incoming peer connections with more than 100MB of daily traffic correspond to nodes using my node for IBD, with the most obvious piece of evidence being that none of these high-traffic peers existed when my node was pruned.\n- Spy nodes: Now interestingly, the bulk of inbound peers exchange only around 1MB of traffic with my node, which is too low (using traffic via my outbound connections as baseline) for them to be regular connections. All those nodes do is complete the P2P handshake, and politely reply to ping messages. Other than that, they just suck up our INV messages. Now one might argue these might be SPV nodes but they're definitely not (the most obvious give away is that they never send any SPV-related messages; in fact, all they ever send is a single `VERSION` and a single `ADDR` message, as well as `PONG`s; the fact that they're coming from a limited bunch of IPs or IP ranges doesn't help)\n\nIf someone has any more ideas about useful data to generate, let me know and I'll try to visualize it. If you want to play around yourself, I've put all of the [Jupyter notebooks on GitHub](https://github.com/virtu/bitcoind-p2p-traffic). Unfortunately, raw tracepoint data was too big for GitHub but I've left some data that's been grouped to hourly and daily resolution in the repo; if you want access to some raw tracepoint data, just let me know where to scp or ftp it.",
  "actions_summary": [
    {
      "id": 2,
      "count": 3
    }
  ],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 91,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": "Github link was replaced with a permanent link",
  "can_view_edit_history": true,
  "wiki": false,
  "reactions": [
    {
      "id": "+1",
      "type": "emoji",
      "count": 3
    }
  ],
  "current_user_reaction": null,
  "reaction_users_count": 3,
  "current_user_used_main_reaction": false
}