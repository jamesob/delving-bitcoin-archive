{
  "id": 5227,
  "name": "Matt Corallo",
  "username": "MattCorallo",
  "avatar_template": "/letter_avatar_proxy/v4/letter/m/e47c2d/{size}.png",
  "created_at": "2025-06-05T13:52:50.052Z",
  "cooked": "<aside class=\"quote no-group\" data-username=\"roasbeef\" data-post=\"24\" data-topic=\"1723\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/roasbeef/48/160_2.png\" class=\"avatar\"> roasbeef:</div>\n<blockquote>\n<p>They can also further bisect the reported values, either by iteratively probing nodes in the path, or connecting out to them to measure ping latency. This brings forth a related third question: <strong>what do we gain by encoding less precise values</strong>?</p>\n</blockquote>\n</aside>\n<p>With randomized delays (plus randomization in I/O latency), doing iterative probing may require a very nontrivial number of tries to map the whole network. With future upfront fees, this may be impractical. In practice, we see people who try this largely fail to provide reasonably accurate data today (at least in the sense that we care about here - they can certainly provide \u201cthis node is terrible/on tor\u201d vs \u201cthis node seems reasonable\u201d-type data, which is obviously a thing we want to provide to all nodes here).</p>\n<aside class=\"quote no-group\" data-username=\"roasbeef\" data-post=\"24\" data-topic=\"1723\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/roasbeef/48/160_2.png\" class=\"avatar\"> roasbeef:</div>\n<blockquote>\n<p>One way to partially address this concern would be to: prefix the latency encoding with the type and parameter. So the final value on the wire would be <code>encoding_type || encode_param || encoding_value</code>. This would:</p>\n</blockquote>\n</aside>\n<p>I don\u2019t buy that its worth us all implementing a configuration knob and multiple encoding options for this. Sometimes more flexibility isn\u2019t the right answer <img src=\"https://delvingbitcoin.org/images/emoji/twitter/slight_smile.png?v=14\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<aside class=\"quote no-group\" data-username=\"roasbeef\" data-post=\"24\" data-topic=\"1723\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/roasbeef/48/160_2.png\" class=\"avatar\"> roasbeef:</div>\n<blockquote>\n<p>Let nodes choose if they wanted to give granular information or not (hey! I\u2019m fast, pick me!).</p>\n</blockquote>\n</aside>\n<p>This would defeat the whole purpose of attempting to reduce the information provided -  privacy loves company - the whole point of the discussion here was to ensure that nodes <em>cannot</em> (in a standardized way) communicate granular latency information as it incentivizes them to do so as senders would (presumably) use that information to (strongly) prefer nodes with even marginal decreases in latency.</p>\n<aside class=\"quote no-group\" data-username=\"GeorgeTsagk\" data-post=\"26\" data-topic=\"1723\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/georgetsagk/48/162_2.png\" class=\"avatar\"> GeorgeTsagk:</div>\n<blockquote>\n<p>It\u2019s also important to note that there are more crucial things to address that directly improve privacy over lightning, like forwarding and sender/receiver delays to mitigate the on/off path adversaries as described previously in this thread. Without addressing these, applying a privacy-dressing on the sender\u2019s feedback is only a partial illusion of privacy.</p>\n</blockquote>\n</aside>\n<p>Indeed, but providing fine-grained latency information strongly incentivizes nodes to remove any forwarding/batching delays, effectively limiting our options later to improve privacy (at least by ensuring everyone has a batching delay, at least by default). I don\u2019t think this conversation was ever about whether this, itself, provides privacy, but rather whether it closes off privacy features we want.</p>\n<aside class=\"quote no-group\" data-username=\"GeorgeTsagk\" data-post=\"26\" data-topic=\"1723\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/georgetsagk/48/162_2.png\" class=\"avatar\"> GeorgeTsagk:</div>\n<blockquote>\n<p>b) Focusing on the performance of LN is a fundamental use-case. Every day someone pays for their food or ticket with LN and the payment resolves fast is a small win for the whole network. I\u2019m not saying we need 100ms instead of 1s, but we definitely need to treat performance as a fundamental requirement of the network.</p>\n</blockquote>\n</aside>\n<p>Its important that we be clear about what this means - lightning has some fundamental limits, and will (in its current protocol) certainly never achieve reliable payment latencies below a second or so (and is already achieving such latencies in practice!). Indeed, changes in payment latency by an order of magnitude absolutely changes what lightning can be used for and opens up new use-cases which we should want. However, in Carla\u2019s analysis in the OP it seems like forwarding/batching delays round an RTT or less can result in reasonable privacy improvements (assuming enough traffic on nodes and some other network-level improvements we want to make).</p>\n<p>Thus, declining to offer fine-grained latency information and encouraging nodes to always batch forwards/failures on the order of 100ms will not change the set of use-cases LN is usable for, nor change the user experience of a lightning payment, nor materially change the \u201cperformance of LN\u201d. Given this, and the fact that privacy is also a critical feature of LN (whether we have it today or not), I\u2019m not entirely clear on why its worth providing fine-grained latency information here. We don\u2019t currently envision a serious use-case for that kind of data, and there\u2019s some (even if marginal) risk in making it available.</p>",
  "post_number": 30,
  "post_type": 1,
  "posts_count": 40,
  "updated_at": "2025-06-05T13:52:50.052Z",
  "reply_count": 1,
  "reply_to_post_number": 24,
  "quote_count": 2,
  "incoming_link_count": 0,
  "reads": 23,
  "readers_count": 22,
  "score": 9.6,
  "yours": false,
  "topic_id": 1723,
  "topic_slug": "latency-and-privacy-in-lightning",
  "topic_title": "Latency and Privacy in Lightning",
  "topic_html_title": "Latency and Privacy in Lightning",
  "category_id": 7,
  "display_username": "Matt Corallo",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "[quote=\"roasbeef, post:24, topic:1723\"]\nThey can also further bisect the reported values, either by iteratively probing nodes in the path, or connecting out to them to measure ping latency. This brings forth a related third question: **what do we gain by encoding less precise values**?\n[/quote]\n\nWith randomized delays (plus randomization in I/O latency), doing iterative probing may require a very nontrivial number of tries to map the whole network. With future upfront fees, this may be impractical. In practice, we see people who try this largely fail to provide reasonably accurate data today (at least in the sense that we care about here - they can certainly provide \"this node is terrible/on tor\" vs \"this node seems reasonable\"-type data, which is obviously a thing we want to provide to all nodes here).\n\n[quote=\"roasbeef, post:24, topic:1723\"]\nOne way to partially address this concern would be to: prefix the latency encoding with the type and parameter. So the final value on the wire would be `encoding_type || encode_param || encoding_value`. This would:\n[/quote]\n\nI don't buy that its worth us all implementing a configuration knob and multiple encoding options for this. Sometimes more flexibility isn't the right answer :)\n\n[quote=\"roasbeef, post:24, topic:1723\"]\nLet nodes choose if they wanted to give granular information or not (hey! I\u2019m fast, pick me!).\n[/quote]\n\nThis would defeat the whole purpose of attempting to reduce the information provided -  privacy loves company - the whole point of the discussion here was to ensure that nodes *cannot* (in a standardized way) communicate granular latency information as it incentivizes them to do so as senders would (presumably) use that information to (strongly) prefer nodes with even marginal decreases in latency.\n\n[quote=\"GeorgeTsagk, post:26, topic:1723\"]\nIt\u2019s also important to note that there are more crucial things to address that directly improve privacy over lightning, like forwarding and sender/receiver delays to mitigate the on/off path adversaries as described previously in this thread. Without addressing these, applying a privacy-dressing on the sender\u2019s feedback is only a partial illusion of privacy.\n[/quote]\n\nIndeed, but providing fine-grained latency information strongly incentivizes nodes to remove any forwarding/batching delays, effectively limiting our options later to improve privacy (at least by ensuring everyone has a batching delay, at least by default). I don't think this conversation was ever about whether this, itself, provides privacy, but rather whether it closes off privacy features we want.\n\n[quote=\"GeorgeTsagk, post:26, topic:1723\"]\nb) Focusing on the performance of LN is a fundamental use-case. Every day someone pays for their food or ticket with LN and the payment resolves fast is a small win for the whole network. I\u2019m not saying we need 100ms instead of 1s, but we definitely need to treat performance as a fundamental requirement of the network.\n[/quote]\n\nIts important that we be clear about what this means - lightning has some fundamental limits, and will (in its current protocol) certainly never achieve reliable payment latencies below a second or so (and is already achieving such latencies in practice!). Indeed, changes in payment latency by an order of magnitude absolutely changes what lightning can be used for and opens up new use-cases which we should want. However, in Carla's analysis in the OP it seems like forwarding/batching delays round an RTT or less can result in reasonable privacy improvements (assuming enough traffic on nodes and some other network-level improvements we want to make).\n\nThus, declining to offer fine-grained latency information and encouraging nodes to always batch forwards/failures on the order of 100ms will not change the set of use-cases LN is usable for, nor change the user experience of a lightning payment, nor materially change the \"performance of LN\". Given this, and the fact that privacy is also a critical feature of LN (whether we have it today or not), I'm not entirely clear on why its worth providing fine-grained latency information here. We don't currently envision a serious use-case for that kind of data, and there's some (even if marginal) risk in making it available.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 50,
  "hidden": false,
  "trust_level": 2,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "With randomized delays (plus randomization in I/O latency), doing iterative probing may require a very nontrivial number of tries to map the whole network. With future upfront fees, this may be impractical. In practice, we see people who try this largely fail to provide reasonably accurate data tod&hellip;",
  "truncated": true,
  "post_url": "/t/latency-and-privacy-in-lightning/1723/30",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}