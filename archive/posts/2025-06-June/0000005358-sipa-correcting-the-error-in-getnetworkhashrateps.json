{
  "id": 5358,
  "name": "Pieter Wuille",
  "username": "sipa",
  "avatar_template": "/user_avatar/delvingbitcoin.org/sipa/{size}/1100_2.png",
  "created_at": "2025-06-27T18:52:13.568Z",
  "cooked": "<p>You are correct, <a class=\"mention\" href=\"/u/zawy\">@zawy</a>.</p>\n<p>Working this out a bit more formally:</p>\n<ul>\n<li>Let <span class=\"math\">r</span> be the unknown but fixed hashrate (in hashes/second). We assume it does not change during the time of our observation.</li>\n<li>Let <span class=\"math\">W_i</span> be the amount of work in block <span class=\"math\">i</span>, a known constant (equal to its expected number of hashes), so:\n<ul>\n<li><span class=\"math\">W_i = 2^{256} / (\\mathrm{target}_i + 1)</span></li>\n<li><span class=\"math\">W_i = 2^{48} / (2^{16}-1) \\cdot \\mathrm{difficulty}_i</span></li>\n</ul>\n</li>\n<li>Let <span class=\"math\">t_i</span> be the durations of each block in seconds, which we assume are exactly observable (not true in practice, but the best we can do).</li>\n</ul>\n<p>The hashes per block is a random variable that follows a geometric distribution, but can be approximated well as an exponential one due to the enormous number of trials involved:</p>\n<div class=\"math\">\nh_i \\sim \\mathrm{Exp}(\\lambda=1/W_i)\n</div>\n<p>and the time per block <span class=\"math\">t_i = h_i / r</span>, and thus by the fact that <span class=\"math\">\\lambda</span> is an inverse scale parameter,</p>\n<div class=\"math\">\nt_i \\sim \\mathrm{Exp}(\\lambda=r/W_i)\n</div>\n<p>To simplify what follows, introduce <span class=\"math\">\\alpha_i = t_i / W_i</span>, which measures how long blocks took relative to their difficulty (its unit is seconds per hash). For these, we have:</p>\n<div class=\"math\">\n\\alpha_i \\sim \\mathrm{Exp}(\\lambda=r)\n</div>\n<p>So all <span class=\"math\">\\alpha_i</span> are identically distributed. They can also be shown to be independent, even when there are difficulty adjustments in between the measured blocks. Their PDF is</p>\n<div class=\"math\">\nf(\\alpha) = r \\exp(-r\\alpha)\n</div>\n<p>Our goal is estimating <span class=\"math\">r</span>, based on a series of <span class=\"math\">n</span> observations (blocks) <span class=\"math\">\\bar{\\alpha}</span>. To start, we can build a <a href=\"https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\">maximum-likelihood estimator</a> for <span class=\"math\">r</span>, which is the value <span class=\"math\">\\hat{r}_\\mathrm{MLE}</span> for which the function</p>\n<div class=\"math\">\n\\begin{split}\n\\hat{l}(r;\\bar{\\alpha}) \\, &amp; = \\, &amp; \\sum_{i=1}^n \\log f(\\alpha_i) \\\\\n&amp; = \\, &amp; \\sum_{i=1}^n \\log \\left( r \\exp(-r\\alpha_i) \\right) \\\\\n&amp; = \\, &amp; n \\log(r) + \\sum_{i=1}^n \\log ( \\exp(-r\\alpha_i)) \\\\\n&amp; = \\, &amp; n \\log(r) - r \\sum_{i=1}^n \\alpha_i \\\\\n\\end{split}\n</div>\n<p>is maximal. The derivative in <span class=\"math\">r</span> is</p>\n<div class=\"math\">\n\\hat{l}'(r;\\bar{\\alpha}) = \\frac{n}{r} - \\sum_{i=1}^n \\alpha_i\n</div>\n<p>which is <span class=\"math\">0</span>, and maximizes <span class=\"math\">\\hat{l}</span> in</p>\n<div class=\"math\">\n\\hat{r}_\\mathrm{MLE} = \\dfrac{n}{\\sum_{i=1}^n \\alpha_i}\n</div>\n<p>If the difficulty is constant within the window, then this is equal to the current formula in <code>getnetworkhashps</code>:</p>\n<div class=\"math\">\n\\hat{r}_\\mathrm{RPC} = \\dfrac{\\sum_{i=1}^n W_i}{\\sum_{i=1}^n t_i}\n</div>\n<hr>\n<p>So far so good. The formula being used is the maximum-likelihood estimator, at least when the difficulty does not change within the measured interval. And if the difficulty does change within it, then the starting assumption that the true but unknown hashrate is a constant throughout the interval probably doesn\u2019t hold anyway, and it may be reasonable to deviate from it.</p>\n<p>However, the real question is whether this estimator is unbiased. To determine that, we compute the expected value of the estimation <span class=\"math\">\\hat{r}_\\mathrm{MLE}</span> when repeating the experiment many times (each experiment consisting of <span class=\"math\">n</span> block measurements), with a <em>known</em> true hashrate <span class=\"math\">r</span>.</p>\n<div class=\"math\">\n\\mathrm{E}[\\hat{r}_\\mathrm{MLE}] = \\mathrm{E}\\left[\\dfrac{n}{\\sum_{i=1}^n \\alpha_i}\\right]\n</div>\n<p>Let</p>\n<div class=\"math\">\n\\beta = \\sum_{i=1}^n \\alpha_i\n</div>\n<p>which is distributed as <span class=\"math\">\\beta \\sim \\mathrm{\\Gamma}(n, r)</span>. Then</p>\n<div class=\"math\">\n\\begin{split}\n\\mathrm{E}[\\hat{r}_\\mathrm{MLE}] \\, &amp; = \\, &amp; n \\cdot \\mathrm{E}[\\beta^{-1}] \\\\\n&amp; = \\, &amp; n \\cdot \\frac{r}{n-1} \\\\\n&amp; = \\, &amp; \\frac{n}{n-1} r\n\\end{split}\n</div>\n<p>which is indeed a factor <span class=\"math\">\\frac{n}{n-1}</span> higher than what it should be. An unbiased estimator can be created by correcting for this factor, and we get</p>\n<div class=\"math\">\n\\begin{split}\n\\hat{r} &amp; \\, = &amp; \\, \\frac{n-1}{\\sum_{i=1}^n \\alpha_i} \\\\\n&amp; \\, = &amp; \\, \\frac{n-1}{\\sum_{i=1}^n \\frac{t_i}{W_i}}\n\\end{split}\n</div>\n<p>I believe it can be shown that this unbiased estimator is <a href=\"https://en.wikipedia.org/wiki/Sufficient_statistic\">sufficient</a> and <a href=\"https://en.wikipedia.org/wiki/Completeness_(statistics)\">complete</a>, which would <a href=\"https://en.wikipedia.org/wiki/Lehmann%E2%80%93Scheff%C3%A9_theorem\">imply</a> it is the <a href=\"https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator\">minimum-variance unbiased estimator</a>(MVUE).</p>",
  "post_number": 4,
  "post_type": 1,
  "posts_count": 8,
  "updated_at": "2025-06-27T22:25:19.851Z",
  "reply_count": 1,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 4,
  "reads": 17,
  "readers_count": 16,
  "score": 43.4,
  "yours": false,
  "topic_id": 1745,
  "topic_slug": "correcting-the-error-in-getnetworkhashrateps",
  "topic_title": "Correcting the error in getnetworkhashrateps",
  "topic_html_title": "Correcting the error in getnetworkhashrateps",
  "category_id": 7,
  "display_username": "Pieter Wuille",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 5,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "You are correct, @zawy.\n\nWorking this out a bit more formally:\n* Let $r$ be the unknown but fixed hashrate (in hashes/second). We assume it does not change during the time of our observation.\n* Let $W_i$ be the amount of work in block $i$, a known constant (equal to its expected number of hashes), so:\n  * $W_i = 2^{256} / (\\mathrm{target}_i + 1)$\n  * $W_i = 2^{48} / (2^{16}-1) \\cdot \\mathrm{difficulty}_i$\n* Let $t_i$ be the durations of each block in seconds, which we assume are exactly observable (not true in practice, but the best we can do).\n\nThe hashes per block is a random variable that follows a geometric distribution, but can be approximated well as an exponential one due to the enormous number of trials involved:\n$$\nh_i \\sim \\mathrm{Exp}(\\lambda=1/W_i)\n$$\nand the time per block $t_i = h_i / r$, and thus by the fact that $\\lambda$ is an inverse scale parameter, \n$$\nt_i \\sim \\mathrm{Exp}(\\lambda=r/W_i)\n$$\nTo simplify what follows, introduce $\\alpha_i = t_i / W_i$, which measures how long blocks took relative to their difficulty (its unit is seconds per hash). For these, we have:\n$$\n\\alpha_i \\sim \\mathrm{Exp}(\\lambda=r)\n$$\nSo all $\\alpha_i$ are identically distributed. They can also be shown to be independent, even when there are difficulty adjustments in between the measured blocks. Their PDF is\n$$\nf(\\alpha) = r \\exp(-r\\alpha)\n$$\n\nOur goal is estimating $r$, based on a series of $n$ observations (blocks) $\\bar{\\alpha}$. To start, we can build a [maximum-likelihood estimator](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) for $r$, which is the value $\\hat{r}_\\mathrm{MLE}$ for which the function\n$$\n\\begin{split}\n\\hat{l}(r;\\bar{\\alpha}) \\, & = \\, & \\sum_{i=1}^n \\log f(\\alpha_i) \\\\\n& = \\, & \\sum_{i=1}^n \\log \\left( r \\exp(-r\\alpha_i) \\right) \\\\\n& = \\, & n \\log(r) + \\sum_{i=1}^n \\log ( \\exp(-r\\alpha_i)) \\\\\n& = \\, & n \\log(r) - r \\sum_{i=1}^n \\alpha_i \\\\\n\\end{split}\n$$\nis maximal. The derivative in $r$ is\n$$\n\\hat{l}'(r;\\bar{\\alpha}) = \\frac{n}{r} - \\sum_{i=1}^n \\alpha_i\n$$\nwhich is $0$, and maximizes $\\hat{l}$ in\n$$\n\\hat{r}_\\mathrm{MLE} = \\dfrac{n}{\\sum_{i=1}^n \\alpha_i}\n$$\n\nIf the difficulty is constant within the window, then this is equal to the current formula in `getnetworkhashps`:\n$$\n\\hat{r}_\\mathrm{RPC} = \\dfrac{\\sum_{i=1}^n W_i}{\\sum_{i=1}^n t_i}\n$$\n\n---\n\nSo far so good. The formula being used is the maximum-likelihood estimator, at least when the difficulty does not change within the measured interval. And if the difficulty does change within it, then the starting assumption that the true but unknown hashrate is a constant throughout the interval probably doesn't hold anyway, and it may be reasonable to deviate from it.\n\nHowever, the real question is whether this estimator is unbiased. To determine that, we compute the expected value of the estimation $\\hat{r}_\\mathrm{MLE}$ when repeating the experiment many times (each experiment consisting of $n$ block measurements), with a *known* true hashrate $r$.\n\n$$\n\\mathrm{E}[\\hat{r}_\\mathrm{MLE}] = \\mathrm{E}\\left[\\dfrac{n}{\\sum_{i=1}^n \\alpha_i}\\right]\n$$\nLet\n$$\n\\beta = \\sum_{i=1}^n \\alpha_i\n$$\nwhich is distributed as $\\beta \\sim \\mathrm{\\Gamma}(n, r)$. Then\n$$\n\\begin{split}\n\\mathrm{E}[\\hat{r}_\\mathrm{MLE}] \\, & = \\, & n \\cdot \\mathrm{E}[\\beta^{-1}] \\\\\n& = \\, & n \\cdot \\frac{r}{n-1} \\\\\n& = \\, & \\frac{n}{n-1} r\n\\end{split}\n$$\nwhich is indeed a factor $\\frac{n}{n-1}$ higher than what it should be. An unbiased estimator can be created by correcting for this factor, and we get\n$$\n\\begin{split}\n\\hat{r} & \\, = & \\, \\frac{n-1}{\\sum_{i=1}^n \\alpha_i} \\\\\n& \\, = & \\, \\frac{n-1}{\\sum_{i=1}^n \\frac{t_i}{W_i}}\n\\end{split}\n$$\n\nI believe it can be shown that this unbiased estimator is [sufficient](https://en.wikipedia.org/wiki/Sufficient_statistic) and [complete](https://en.wikipedia.org/wiki/Completeness_(statistics)), which would [imply](https://en.wikipedia.org/wiki/Lehmann%E2%80%93Scheff%C3%A9_theorem) it is the [minimum-variance unbiased estimator](https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator)(MVUE).",
  "actions_summary": [
    {
      "id": 2,
      "count": 1
    }
  ],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 96,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "You are correct, <a class=\"mention\" href=\"/u/zawy\">@zawy</a>. \nWorking this out a bit more formally: \n\nLet r be the unknown but fixed hashrate (in hashes/second). We assume it does not change during the time of our observation.\nLet W_i be the amount of work in block i, a known constant (equal to its expected number of hashes), so:\n\nW_i &hellip;",
  "truncated": true,
  "post_url": "/t/correcting-the-error-in-getnetworkhashrateps/1745/4",
  "reactions": [
    {
      "id": "+1",
      "type": "emoji",
      "count": 1
    }
  ],
  "current_user_reaction": null,
  "reaction_users_count": 1,
  "current_user_used_main_reaction": false
}