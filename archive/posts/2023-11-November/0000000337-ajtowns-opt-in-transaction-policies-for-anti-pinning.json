{
  "id": 337,
  "name": "Anthony Towns",
  "username": "ajtowns",
  "avatar_template": "/user_avatar/delvingbitcoin.org/ajtowns/{size}/417_2.png",
  "created_at": "2023-11-08T02:11:55.629Z",
  "cooked": "<aside class=\"quote no-group\" data-username=\"instagibbs\" data-post=\"1\" data-topic=\"159\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/instagibbs/48/28_2.png\" class=\"avatar\"> instagibbs:</div>\n<blockquote>\n<h1>Opt-in policies</h1>\n<p>Instead of working around rule#3 directly, we can instead allow wallets to add \u201cprior restraint\u201d to features of the transaction that may mitigate these issues.</p>\n</blockquote>\n</aside>\n<p>I guess the concern I have with this is some rules we have that create pinning vectors align with \u201cminer incentives\u201d, and if miners are incentivised to enforce those rules, then they\u2019ll just work around any rules we create to reintroduce those pinning vectors?</p>\n<p>How about imagining an alternative relay network structured like this:</p>\n<ol>\n<li>nodes maintain a mempool consisting of up to 1MvB of txs that\u2019s the predicted next block</li>\n<li>new txs/package are accepted when they improve the mempool\u2019s total fee</li>\n<li>txs/packages are relayed on if they\u2019re accepted</li>\n</ol>\n<p>Such an approach would be obnoxious in some ways: when a new block comes in, your entire mempool becomes invalid, and you need to validate a new block\u2019s worth of transactions ASAP in order to be able to produce new block templates, eg. Handwave that problem away for now.</p>\n<p>Would such a relay network work? I think so:</p>\n<ul>\n<li>node operators: the risk of free relay here becomes minimal, because blocks will contain every tx in the mempool, and blocks are found probabilistically, so your bandwidth is unlikely to be used for spam</li>\n<li>miners: this directly optimises for the next block fee</li>\n<li>users: pinning vectors are minimised; though there\u2019s no provision for adding your tx to the backlog and getting it mined eventually</li>\n</ul>\n<p>This approach would perhaps be incompatible with non-consensus \u201cprior restraint\u201d features though: miners and users could easily just remove those features and maintain a network like this.</p>\n<p>Because such a network would be optimal for miners/users, I think that implies that it will be implemented (eventually), so node operators will (eventually) need to design around the problems/limitations such a network would create.</p>\n<p>Could you add some sort of a backlog back into this approach?</p>\n<p>The most significant problem with doing so is that any given block could invalidate up to about 1GvB of txs from the backlog, by replacing many maximum size txs (100kvB) in the backlog by minimum size txs in the block (100vB):</p>\n<pre data-code-height=\"147\" data-code-wrap=\"mermaid\"><code class=\"lang-mermaid\">graph LR;\n    A[UTXO: A]-.-&gt;B[Backlog Tx: 100kVB];\n    A--&gt;C[Block Tx: 100vB];\n</code></pre>\n<p>This is already possible, though it requires actually mining a block. With a relay network of the type described here it would become easier in two ways: you don\u2019t need to manually find a cooperating miner, and when there is a long delay between blocks you could cycle multiple GvB of replacement transactions through the backlog, while only having to actually pay once a block is found.</p>\n<p>One approach to allowing a backlog while reducing the impact of free relay might be to make backlog relay unreliable. For example: for a tx that\u2019s in the top x% of the backlog, relay it with (100-x)% probability. If an unrelayed tx moves up from position x to position y, perhaps relay it with probability (1-y/x)%. In that case, when replacing 1GB of mempool you\u2019ve likely only actually relayed 50% of those txs. Choosing a different relay distribution (ie, biassing towards relaying at the top of the backlog, and against relaying at the bottom of the backlog) could reduce that amount further.</p>\n<p>Presuming nodes aggressively rebroadcast txs once they move from the backlog into the next-block-mempool, that likely provides good protection against pinning: it makes it easy to find a node that doesn\u2019t have a conflicting tx already in its backlog (since the conflicting tx has low fee, and is thus less likely to be relayed); once the tx is accepted into some node\u2019s backlog, it\u2019ll remain there; and once the tx becomes acceptable for the next block, it will be aggressively relayed and removed the attempted pin from other nodes\u2019 backlogs.</p>\n<p>Perhaps a way of thinking about this would be:</p>\n<ul>\n<li>nodes will aggressively try to sync the top MvB to maximise fee revenue</li>\n<li>txs that don\u2019t pay a high enough fee get considered for the backlog; in that case they must be \u201cpolite\u201d \u2013 smoothly incresing the fees of the total backlog, etc</li>\n<li>but the backlog is deliberately non-uniform, so that it\u2019s easy to send a double-spend to some peer, provided the tx it\u2019s double-spending is relatively low in the backlog</li>\n<li>conflicts amongst different peers\u2019 backlogs are resolved when one of the double-spends makes it to the top of the backlog, and a block gets mined</li>\n</ul>\n<p>Max tx size, ancestor count/size, min incremental fee, etc are still enforced both for next-block txs as well as backlog txs.</p>\n<p>Would something along those lines be crazy? I really feel like \u201cpinning\u201d is a fundamental design bug\u2026</p>",
  "post_number": 2,
  "post_type": 1,
  "posts_count": 3,
  "updated_at": "2023-11-08T02:11:55.629Z",
  "reply_count": 1,
  "reply_to_post_number": null,
  "quote_count": 1,
  "incoming_link_count": 0,
  "reads": 20,
  "readers_count": 19,
  "score": 9.0,
  "yours": false,
  "topic_id": 159,
  "topic_slug": "opt-in-transaction-policies-for-anti-pinning",
  "topic_title": "Opt-in transaction policies for anti-pinning",
  "topic_html_title": "Opt-in transaction policies for anti-pinning",
  "category_id": 8,
  "display_username": "Anthony Towns",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "[quote=\"instagibbs, post:1, topic:159\"]\n# Opt-in policies\n\nInstead of working around rule#3 directly, we can instead allow wallets to add \u201cprior restraint\u201d to features of the transaction that may mitigate these issues.\n[/quote]\n\nI guess the concern I have with this is some rules we have that create pinning vectors align with \"miner incentives\", and if miners are incentivised to enforce those rules, then they'll just work around any rules we create to reintroduce those pinning vectors?\n\nHow about imagining an alternative relay network structured like this:\n\n 1. nodes maintain a mempool consisting of up to 1MvB of txs that's the predicted next block\n 2. new txs/package are accepted when they improve the mempool's total fee\n 3. txs/packages are relayed on if they're accepted\n\nSuch an approach would be obnoxious in some ways: when a new block comes in, your entire mempool becomes invalid, and you need to validate a new block's worth of transactions ASAP in order to be able to produce new block templates, eg. Handwave that problem away for now.\n\nWould such a relay network work? I think so:\n\n * node operators: the risk of free relay here becomes minimal, because blocks will contain every tx in the mempool, and blocks are found probabilistically, so your bandwidth is unlikely to be used for spam\n * miners: this directly optimises for the next block fee\n * users: pinning vectors are minimised; though there's no provision for adding your tx to the backlog and getting it mined eventually\n\nThis approach would perhaps be incompatible with non-consensus \"prior restraint\" features though: miners and users could easily just remove those features and maintain a network like this.\n\nBecause such a network would be optimal for miners/users, I think that implies that it will be implemented (eventually), so node operators will (eventually) need to design around the problems/limitations such a network would create.\n\nCould you add some sort of a backlog back into this approach?\n\nThe most significant problem with doing so is that any given block could invalidate up to about 1GvB of txs from the backlog, by replacing many maximum size txs (100kvB) in the backlog by minimum size txs in the block (100vB):\n\n```mermaid height=147,auto\ngraph LR;\n    A[UTXO: A]-.->B[Backlog Tx: 100kVB];\n    A-->C[Block Tx: 100vB];\n```\n\nThis is already possible, though it requires actually mining a block. With a relay network of the type described here it would become easier in two ways: you don't need to manually find a cooperating miner, and when there is a long delay between blocks you could cycle multiple GvB of replacement transactions through the backlog, while only having to actually pay once a block is found.\n\nOne approach to allowing a backlog while reducing the impact of free relay might be to make backlog relay unreliable. For example: for a tx that's in the top x% of the backlog, relay it with (100-x)% probability. If an unrelayed tx moves up from position x to position y, perhaps relay it with probability (1-y/x)%. In that case, when replacing 1GB of mempool you've likely only actually relayed 50% of those txs. Choosing a different relay distribution (ie, biassing towards relaying at the top of the backlog, and against relaying at the bottom of the backlog) could reduce that amount further.\n\nPresuming nodes aggressively rebroadcast txs once they move from the backlog into the next-block-mempool, that likely provides good protection against pinning: it makes it easy to find a node that doesn't have a conflicting tx already in its backlog (since the conflicting tx has low fee, and is thus less likely to be relayed); once the tx is accepted into some node's backlog, it'll remain there; and once the tx becomes acceptable for the next block, it will be aggressively relayed and removed the attempted pin from other nodes' backlogs.\n\nPerhaps a way of thinking about this would be:\n\n * nodes will aggressively try to sync the top MvB to maximise fee revenue\n * txs that don't pay a high enough fee get considered for the backlog; in that case they must be \"polite\" -- smoothly incresing the fees of the total backlog, etc\n * but the backlog is deliberately non-uniform, so that it's easy to send a double-spend to some peer, provided the tx it's double-spending is relatively low in the backlog\n * conflicts amongst different peers' backlogs are resolved when one of the double-spends makes it to the top of the backlog, and a block gets mined\n\nMax tx size, ancestor count/size, min incremental fee, etc are still enforced both for next-block txs as well as backlog txs. \n\nWould something along those lines be crazy? I really feel like \"pinning\" is a fundamental design bug...",
  "actions_summary": [],
  "moderator": true,
  "admin": true,
  "staff": true,
  "user_id": 3,
  "hidden": false,
  "trust_level": 4,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "I guess the concern I have with this is some rules we have that create pinning vectors align with \u201cminer incentives\u201d, and if miners are incentivised to enforce those rules, then they\u2019ll just work around any rules we create to reintroduce those pinning vectors? \nHow about imagining an alternative re&hellip;",
  "truncated": true,
  "post_url": "/t/opt-in-transaction-policies-for-anti-pinning/159/2",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}