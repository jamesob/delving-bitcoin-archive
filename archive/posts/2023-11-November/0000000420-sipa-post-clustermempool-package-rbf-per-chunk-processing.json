{
  "id": 420,
  "name": "Pieter Wuille",
  "username": "sipa",
  "avatar_template": "/user_avatar/delvingbitcoin.org/sipa/{size}/1100_2.png",
  "created_at": "2023-11-15T19:08:21.910Z",
  "cooked": "<aside class=\"quote no-group\" data-username=\"sdaftuar\" data-post=\"5\" data-topic=\"190\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/letter_avatar_proxy/v4/letter/s/0ea827/48.png\" class=\"avatar\"> sdaftuar:</div>\n<blockquote>\n<p>Also, I think that if we were to allow lower feerate chunks to satisfy the anti-DoS rules for a higher feerate chunk, that (a) this would result in a great deal of additional complexity in our implementation (we would want to check that any chunks that get bundled together for validation are actually connected, to avoid unrelated transactions from interacting in unexpected ways), and (b) this could also permit higher feerate chunks to satisfy the incentive compatibility rule (ie feerate diagram test) for lower-feerate chunks. This strikes me as potentially undesirable.</p>\n</blockquote>\n</aside>\n<p>Right, this feels wrong.</p>\n<aside class=\"quote no-group\" data-username=\"instagibbs\" data-post=\"6\" data-topic=\"190\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/instagibbs/48/28_2.png\" class=\"avatar\"> instagibbs:</div>\n<blockquote>\n<p>I think this also goes back to what I think you phrased something like \u201cwe pick our most incentive compatible thing, then do DoS checks to accept/reject\u201d. The alternative strategy strikes me as the other direction, where we search for something DoS-compatible, then pick the most incentive compatible of that.</p>\n</blockquote>\n</aside>\n<p>Indeed, trying to aggregate chunks is definitely in the direction of \u201csearch within DoS-compatible solutions\u201d, which I believe is the wrong way to go. If we\u2019d have spare cycles, and somehow considered doing something more computationally expensive, it should be searching aggregates of chunks (or subsets of chunks) for better incentive-compatibility - not DoS-compatibility.</p>\n<aside class=\"quote no-group\" data-username=\"ajtowns\" data-post=\"2\" data-topic=\"190\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/ajtowns/48/3_2.png\" class=\"avatar\"> ajtowns:</div>\n<blockquote>\n<p>I don\u2019t think it\u2019s terrible to say \u201cif you want child fees to pay for the parent to be accepted, the child\u2019s fee rate needs to be greater than the parent\u2019s fee rate\u201d \u2013 at that point the linearisation should put the child and parent in the same chunk and you\u2019re good? That seems like a pretty easy rule for wallets to follow as well.</p>\n</blockquote>\n</aside>\n<p>Continuing my thought from yesterday, the example with a high-feerate small parent chunk that evicts more than it pays for and low-feerate child that would pay for it. If this is actually a goal, instead of raising the feerate of the child chunk, one can also just attach a small high-fee additional transaction to the parent chunk. So perhaps that makes it not that much of a concern (even if a use case exists, which I\u2019m not convinced about).</p>\n<p>So overall, let\u2019s stick with chunk per chunk.</p>\n<aside class=\"quote no-group\" data-username=\"instagibbs\" data-post=\"6\" data-topic=\"190\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/instagibbs/48/28_2.png\" class=\"avatar\"> instagibbs:</div>\n<blockquote>\n<p>If we get some \u201cmalicious\u201d failure like signature failure I think it makes sense to drop the rest of the package potentially. ala 26711.</p>\n</blockquote>\n</aside>\n<p>What if it\u2019s a failure due to recent softfork or so?</p>",
  "post_number": 7,
  "post_type": 1,
  "posts_count": 16,
  "updated_at": "2023-11-15T19:13:35.289Z",
  "reply_count": 1,
  "reply_to_post_number": null,
  "quote_count": 3,
  "incoming_link_count": 0,
  "reads": 29,
  "readers_count": 28,
  "score": 40.6,
  "yours": false,
  "topic_id": 190,
  "topic_slug": "post-clustermempool-package-rbf-per-chunk-processing",
  "topic_title": "Post-clustermempool package RBF: per-chunk processing",
  "topic_html_title": "Post-clustermempool package RBF: per-chunk processing",
  "category_id": 8,
  "display_username": "Pieter Wuille",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 2,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "[quote=\"sdaftuar, post:5, topic:190\"]\nAlso, I think that if we were to allow lower feerate chunks to satisfy the anti-DoS rules for a higher feerate chunk, that (a) this would result in a great deal of additional complexity in our implementation (we would want to check that any chunks that get bundled together for validation are actually connected, to avoid unrelated transactions from interacting in unexpected ways), and (b) this could also permit higher feerate chunks to satisfy the incentive compatibility rule (ie feerate diagram test) for lower-feerate chunks. This strikes me as potentially undesirable.\n[/quote]\n\nRight, this feels wrong.\n\n[quote=\"instagibbs, post:6, topic:190\"]\nI think this also goes back to what I think you phrased something like \u201cwe pick our most incentive compatible thing, then do DoS checks to accept/reject\u201d. The alternative strategy strikes me as the other direction, where we search for something DoS-compatible, then pick the most incentive compatible of that.\n[/quote]\n\nIndeed, trying to aggregate chunks is definitely in the direction of \"search within DoS-compatible solutions\", which I believe is the wrong way to go. If we'd have spare cycles, and somehow considered doing something more computationally expensive, it should be searching aggregates of chunks (or subsets of chunks) for better incentive-compatibility - not DoS-compatibility.\n\n[quote=\"ajtowns, post:2, topic:190\"]\nI don\u2019t think it\u2019s terrible to say \u201cif you want child fees to pay for the parent to be accepted, the child\u2019s fee rate needs to be greater than the parent\u2019s fee rate\u201d \u2013 at that point the linearisation should put the child and parent in the same chunk and you\u2019re good? That seems like a pretty easy rule for wallets to follow as well.\n[/quote]\n\nContinuing my thought from yesterday, the example with a high-feerate small parent chunk that evicts more than it pays for and low-feerate child that would pay for it. If this is actually a goal, instead of raising the feerate of the child chunk, one can also just attach a small high-fee additional transaction to the parent chunk. So perhaps that makes it not that much of a concern (even if a use case exists, which I'm not convinced about).\n\nSo overall, let's stick with chunk per chunk.\n\n[quote=\"instagibbs, post:6, topic:190\"]\nIf we get some \u201cmalicious\u201d failure like signature failure I think it makes sense to drop the rest of the package potentially. ala 26711.\n[/quote]\n\nWhat if it's a failure due to recent softfork or so?",
  "actions_summary": [
    {
      "id": 2,
      "count": 2
    }
  ],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 96,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "Right, this feels wrong. \n\nIndeed, trying to aggregate chunks is definitely in the direction of \u201csearch within DoS-compatible solutions\u201d, which I believe is the wrong way to go. If we\u2019d have spare cycles, and somehow considered doing something more computationally expensive, it should be searching &hellip;",
  "truncated": true,
  "post_url": "/t/post-clustermempool-package-rbf-per-chunk-processing/190/7",
  "reactions": [
    {
      "id": "+1",
      "type": "emoji",
      "count": 2
    }
  ],
  "current_user_reaction": null,
  "reaction_users_count": 2,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}