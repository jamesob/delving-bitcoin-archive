{
  "id": 3370,
  "name": "",
  "username": "roasbeef",
  "avatar_template": "/user_avatar/delvingbitcoin.org/roasbeef/{size}/160_2.png",
  "created_at": "2024-10-16T00:11:59.111Z",
  "cooked": "<p>Hi y\u2019all,</p>\n<div data-theme-toc=\"true\"> </div>\n<p>~3 weeks ago, 30+ Lightning developers and researchers gathered in Tokyo,\nJapan for three days to discuss a number of matters related to the current\nstate and future evolution of the Lightning protocol (and where relevant,\nthe Bitcoin p2p and consensus protocol).</p>\n<p>The last such gathering took place in June found here:\n<a href=\"https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-June/003600.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[Lightning-dev] LN Summit 2022 Notes &amp; Summary/Commentary</a>.\n2022 in Oakland, California. The prior raw meetings notes and summary can be\nfound here:\n<a href=\"https://docs.google.com/document/d/1KHocBjlvg-XOFH5oG_HwWdvNBIvQgxwAok3ZQ6bnCW0/edit?usp=sharing\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">LN Summit Oakland 2022 - Google Docs</a>.</p>\n<p>My raw meetings notes for this instance of the Lightning Developer Summit\ncan be found here:\n<a href=\"https://docs.google.com/document/d/1erQfnZjjfRBSSwo_QWiKiCZP5UQ-MR53ZWs4zIAVcqs/edit?usp=sharing\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Lightning Summit Tokyo - 2024 - Google Docs</a>.\nThe rough daily schedule of discussion topics we agreed upon can be found\nhere: <a href=\"https://gist.github.com/Roasbeef/5ac91c57cb9826c628b1445670219728\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">lightning summit.md \u00b7 GitHub</a>.</p>\n<p>It\u2019s worth noting that there were many other break out groups and side\ndiscussions that may not have been captured in my best-effort notes above,\nnor reflected in the schedule I put together above.</p>\n<p>With all that said, what follows is my attempt to summarize the major\ndiscussion points, and conclusions (with some side commentary ofc ;)). If an\nattendee finds anything in my summary inaccurate or incomplete, then\ndiscussion and decision points that took place across the 3 days of the\nsummit, please feel free to reply and fill in the gaps.</p>\n<h1><a name=\"p-3370-whats-up-with-package-relay-and-v3-commitments-anyway-1\" class=\"anchor\" href=\"#p-3370-whats-up-with-package-relay-and-v3-commitments-anyway-1\"></a>What\u2019s up with Package Relay and V3 Commitments Anyway?</h1>\n<p>This was the first major topic discussed, right on the heels of the latest\nrelease candidate of Bitcoin Core 28.0 (at the time of writing, has now been\nreleased).</p>\n<h2><a name=\"p-3370-fee-estimation-base-commitments-2\" class=\"anchor\" href=\"#p-3370-fee-estimation-base-commitments-2\"></a>Fee Estimation &amp; Base Commitments</h2>\n<p>Before jumping into the latest and greatest as far the newly proposed\ncommitment design, I\u2019ll first give a brief overview on how the commitment\ndesign works today, and the shortcomings thereof.</p>\n<p>(if you already know how commitment transactions work today in LN, then you\ncan skip this section)</p>\n<p>A key aspect of the design of the Lightning Network is the concept of\nunilateral exit. At any time, either party needs to be able to forcibly exit\nfrom the channel to reclaim their funds after a time delay. A time delay is\nneeded as it\u2019s possible that one party attempts to cheat by publishing an\nold revoked state to the chain. The time delay gives the honest party a\nchance to refute the claim by an adversary, proving that they know a secret\nwhich could only have been utilized if a revoked state was published.</p>\n<p>The ability to unilaterally exit from a channel is also a key component\nrequired to be able to enforce the HTLC contract, which implements the\n\u201cclaim of refund\u201d functionality that makes multi-hop Lightning payments\npossible. These contracts have another time delay component: an absolute\ntime delay. Along a route, each hop has T blocks (the CLTV time lock delta)\nto confirm their commitment transaction, and also time out any lingering\noutgoing HTLCs. If they can\u2019t confirm in time, then they lose risking funds\nwith a \u201cone sided redemption\u201d, as then the incoming HTLC will timeout,\ncreating a timeout/redemption race condition.</p>\n<p>A node\u2019s ability to achieve timely confirmation of their commitment\ntransaction depends on their ability to do effective fee estimation. If the\ncommitment transaction (which can\u2019t unilaterally be modified by a single\nparty) has insufficient fees, then it may not be confirmed in time (or even\nbe accepted to the mempool!). Today, the initiator of a channel can send the\n<code>update_fee</code> message to increase (or decrease) the current commitment fee\nrate. This is a critical tool, however it forces the initiator to either be\nprepared to significantly overpay for the commitment fee (to ensure they can\nland in the next block) or expertly guess what the fee will be ahead of\ntime. As the initiator pays all fees on the commitment transaction, the\nresponder is unable to directly influence what this fee may be, instead\ntheir only recourse today is to attempt to force close if they disagree on\nwhat the fee should be.</p>\n<h2><a name=\"p-3370-anchor-outputs-to-the-rescue-3\" class=\"anchor\" href=\"#p-3370-anchor-outputs-to-the-rescue-3\"></a>Anchor Outputs to the Rescue!</h2>\n<p>In order to address some of the shortcomings of the existing commitment\ntransaction format, anchor outputs were proposed <a href=\"https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-January/000943.html\" rel=\"noopener nofollow ugc\">1</a>. The general idea is\nthat both peers gain a near-dust output that can be spent by only them,\nwhich allows bumping the fee of the commitment transaction after the fact.\nThis lessens, but doesn\u2019t eliminate the requirement to estimate what future\nfees will be. Rather than ned to get into the very next block, the goal is\nnow to have a fee high enough to get into the <em>mempool</em>. Once in the\nmempool, then either side can bump the fee to eventually confirm the\ncommitment transaction. Along the way, we also allowed second-level HTLCs to\nbe bundled together, allowing for a greater degree of batching when sweeping\nresolved contracts. The fee needed to get into the mempol, similar to the\nfee needed to get into the next block, is also a moving target. As nodes\nhave a default constrained mempool of ~300 MB, as the transaction arrival\nrate increases, nodes begin to drop transactions from their mempool,\nincreasing their minimum transaction fee in the process. Eventually, the\nexisting max anchor transaction fee may not be high enough to actually get\ninto the mempool, meaning the commitment transaction is dropped out. When\nthis happens, nodes no longer have a way (using the existing p2p network) to\npropagate their commitment transaction, meaning it may not confirm in time\nto avoid the redemption race condition, or at all.</p>\n<p>Along the way developers and researcher discovered a series of subtle\ninteractions with transaction broadcast, and mempool relay policies that may\nactually allow an adversary to \u201cpin\u201d <a href=\"https://github.com/t-bast/lightning-docs/blob/398a1b78250f564f7c86a414810f7e87e5af23ba/pinning-attacks.md\" rel=\"noopener nofollow ugc\">2</a> a transaction to the mempool,\nthereby purposefully preventing its confirmation. The various known pinning\nvectors take advantage of degenerate cases related to BIP 125, and the\nvarious ancestor limits widely utilized in mempool policy today.</p>\n<h2><a name=\"p-3370-v3-truc-and-1p1c-to-the-rescue-actually-this-time-4\" class=\"anchor\" href=\"#p-3370-v3-truc-and-1p1c-to-the-rescue-actually-this-time-4\"></a>V3, TRUC, and 1P1C to the Rescue (actually this time)!</h2>\n<p>Enter transaction v3 and TRUC. The ultimate dream of LN developers has been\nto just get rid of <code>update_fee</code>, and instead just have the commitment be\n<em>zero fee</em>. This takes away all the guesswork of figuring out what the true\nmempool fee should be. In isolation however, if the commitment transaction\nis zero fee, then it won\u2019t be accepted to the mempool, so it won\u2019t propagate\nacross the p2p network.</p>\n<p>A combination of TRUC (a.k.a BIP 434), a new style of anchors, and\noptimistic 1-Parent-1-Child relay is the current best known solution that\npractical addresses LN\u2019s current transaction relay and confirmation\nproblems.</p>\n<p>TRUC introduces a new set of transaction replacement rules meant to address\nthe degenerate cases of BIP 125 for a subset of use cases. Along the way, it\nintroduces a new set of transaction topology size restrictions to further\nconstrain the problem. TRUC transactions use a transaction version of 3\n(instead of a sequence number like BIP 125) to signal that a transaction\nwishes to opt into the new rule set.</p>\n<p>Also available with Bitcoin Core 28.0, is a new standard public key script\ntype: PayToAnchor (P2A) <a href=\"https://github.com/t-bast/lightning-docs/blob/398a1b78250f564f7c86a414810f7e87e5af23ba/pinning-attacks.md\" rel=\"noopener nofollow ugc\">2</a>. P2A is a new special segwit v1 output (<code>OP_1 &lt;0x4e73&gt;</code>) meant to be used for the purpose of fee bumping via CPFP. The\ninput spending this output <em>must</em> have a pure witness input, and can be used\nwithout a signature. Future versions of this new output type may eventually\nallow outputs to be dust, as long as they\u2019re immediately spent in the same\nblock they\u2019re created within (via CPFP).</p>\n<p>The final component of the design for the newly revamped LN commitment\ntransaction is: 1-Parent-1-Child (1P1C) <a href=\"https://github.com/bitcoin/bitcoin/pull/28970\" rel=\"noopener nofollow ugc\">4</a>. 1P1C is basically opportunistic\npackage relay. Rather than rely on a new p2p message which may take time to\nbe deployed across the entire network, 1P1C has nodes change their behavior\nwhen it comes to receiving orphan transactions (txns where the node doesn\u2019t\nknow the inputs). Instead of just storing the child in the orphan pool, the\nnode will now opportunistically attempt to fetch the parent from the\nreporting node, even if the parent has a fee which is below the current\nmempol min fee.</p>\n<p>In concert, these three new transaction relay primitives can be used to\nre-design the Lightning commitment transaction format (namely anchors) to\nresolve many of the long standing issues described above. t-bast has already\nstarted to prototype what the new commitment format would look like:\n<a href=\"https://x.com/realtbast/status/1834213774674247987\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">x.com</a>.</p>\n<p>With all that said, there\u2019re a few open design questions, including:</p>\n<ul>\n<li>How should dust be handled?\n<ul>\n<li>With the P2A approach, we can put all the dust into the anchor output\nitself, instead of going to miner fees as they do today. This would\nresolve some known issues related to an excessive amount of dust on a\ncommitment transaction, but also add some new interactions if the P2A\nanchor output requires no signature.</li>\n</ul>\n</li>\n<li>Should the P2A output actually be keyless?\n<ul>\n<li>If the output is keyless, then any 3rd party can <em>immediately</em> help to\npeers can sweep, until 16 blocks after confirmation). sweep the anchor\noutput (compared to today when only the two channel</li>\n<li>Related to the above, if we put all the dust outputs in the P2A, then\nwhoever sweeps the P2A can also claim the entire dust output.\nNaturally miners would be the parties that are most reliably able to\nclaim these funds, assuming there\u2019s no signature required.</li>\n<li>Some argued that we should still have a signature here, as then it\nrestricts the parties that can interfere with the channel peers\nattempting to confirm the commitment transaction. This is meant to\nhedge against a future discovered defect in the TRUC+P2A interaction,\nthat enables a yet to be discovered type of pinning.</li>\n</ul>\n</li>\n<li>Does the viral nature of V3 transactions impede certain use cases\nrelated to advanced splicing?\n<ul>\n<li>All unconfirmed descendents from a v3 transaction must also be v3\nthemselves. There was some concern that this would impact uses of\nsplicing where a node attempts to satisfy several transaction flows\nwith a single batched transaction. The viral nature of the v3\ntransaction type would then force any parties that are spending\nunconfirmed change to themselves use txn v3, which may be beyond the\nextents of their current capabilities.</li>\n</ul>\n</li>\n</ul>\n<p>The new TRUC rules also allow for a type of package RBF, where if there\u2019s a\nnew conflicting package, then it\u2019ll attempt to do RBF against the existing\nconflicting package set. This is also known as sibling eviction in the\ncontext of 1P1C <a href=\"https://github.com/bitcoin/bitcoin/pull/29306\" rel=\"noopener nofollow ugc\">5</a>.</p>\n<p>All the information above can be found in more detail in this handy dandy\nguide for wallet developers <a href=\"https://docs.google.com/document/d/1Topu844KUUnrBED4VaJE0lVnk9_mV6UZSz456slMO8k/edit\" rel=\"noopener nofollow ugc\">6</a>.</p>\n<p>When the dust settled, this was probably one of the more concrete new\ninitiatives following the spec summit. From here, we\u2019ll begin to spec out\nexactly what the new v3 commitment type looks like, while we wait for a\nsufficient amount of the p2p network to update to the new version, so we can\nrely on the new relay behavior.</p>\n<p>One thing worth calling out is that this shift will have further\nimplications w.r.t how the wallets that back Lightning nodes handle UTXO\nstockpiling. With this approach, given the commitment transaction will have\nzero fees, in order to confirm the transaction, a wallet <em>must</em> use an\nexisting UTXO to anchor down the commitment transaction for it to be even\nbroadcastable. In practice, this means that wallets will need to reserve\nfunds on chain for the purpose of force closing channels in a timely manner.\nTools such as splicing, and submarine swaps can be used to allow wallets to\nshift around funds, or batch several on-chain transactions in one.</p>\n<h1><a name=\"p-3370-ptlcs-simplified-commitments-5\" class=\"anchor\" href=\"#p-3370-ptlcs-simplified-commitments-5\"></a>PTLCs &amp; Simplified Commitments</h1>\n<p>Next up was a session focussed on discussing the combination of PTLCs, and\nthe Simplified Channel State Machine. At first, these two topics might\nappear to be completely unrelated, but as we\u2019ll see shortly, some of the\nedge cases that arise with design considerations of PTLCs can be mitigated\nby modifying the current channel state machine protocol (dubbed the\nLightning Commitment Protocol - LCP) to a simplified variant.</p>\n<p>First, a brief background on PTLCs. Today in the LN protocol, we use payment\nhashes to implement the multi-hop claim-or-refund gadget that allows for\ntrust-minimize multi-hop payments. While simple, the current protocol has a\nbig privacy drawback: the payment hash is the same over the entire route, so\nif an adversary appears in two locations in a route, then they can trivially\ncorrelate a payment (and also disrupt MPP circuits).</p>\n<p>To fix this privacy hole, many years ago developers proposed we switch over\nto using EC points and private keys instead of payment hashes and preimages.\nIn\n2018 a formalized scheme was published <a href=\"https://eprint.iacr.org/2018/472\" rel=\"noopener nofollow ugc\">7</a>, that would actually allow the\nconstruction to be instantiated using adapter signatures across both the\nECDSA and Schnorr signature schemes. This was interesting, as it meant that\nwe didn\u2019t necessarily have to wait for taproot (which enabled schnorr) to be\nactivated. Instead, a single multi-hop lock could be transported across\nECDSA and schnorr hops alike. Ultimately, for various reasons, this hybrid\nversion was never deployed. The upside is that we can now deploy a simpler,\nunified, schnorr-only version of multi-hop locks.</p>\n<p>Fast forwarding to the present, instagibbs, alongside of his research into a\nconcrete design of LN-Symmetry has been exploring the design landscape, from\nmessaging changes to state machine extensions <a href=\"https://gist.github.com/instagibbs/1d02d0251640c250ceea1c66665ec163\" rel=\"noopener nofollow ugc\">8</a>.</p>\n<p>After discussing some of his findings, we started to zoom in on some key\ndesign questions:</p>\n<ol>\n<li>\n<p>Should we use a single signature, or multi-sig adapter signature? With\neither approach, the adapter <code>T</code> used to create the signature will allow\nthe proposing party to complete the private key needed to sign for the\nincoming HTLC.</p>\n<p>The musig2 based adapter signature is smaller (just a single sig\ninstead of two), but adds extra coordination requirements as a nonce\nneeds to be sent by both parties to properly create a new commitment\ntransaction.</p>\n<p>The single sig adapter is larger (two signatures, just like current\nsecond-level HTLCs today), but simplifies the protocol, as HTLC\nsignatures can be sent alongside the <code>commit_sig</code> message as usual.</p>\n</li>\n<li>\n<p>If we decide to go with the musig2 adapter signature design, then\nshould we attempt to retain the current full-duplex async LCP flow, or\nshould we instead simplify further, instead going for a simple sync\ncommitment state machine protocol?</p>\n<p>The introduction of musig2 nonces for second-level HTLCs complicates\nthe existing LCP protocol as we can no longer send the second-level\nHTLC signatures along side the <code>commit_sig</code> message, as the proposer\nnow needs a partial signature from the accepting party before they can\nsafely proceed.</p>\n<p>However, if we modify the channel state machine protocol to instead be\n<em>round based</em>, then though we sacrifice some x-put, we don\u2019t need to\nworry about handling the various interleaved executions possible\n(simultaneously <code>update_add_sig</code>+<code>commit_sig</code> by both parties). This\nbrings us to the topic of Simplified Commit.</p>\n</li>\n</ol>\n<h2><a name=\"p-3370-round-based-channel-state-machine-protocols-6\" class=\"anchor\" href=\"#p-3370-round-based-channel-state-machine-protocols-6\"></a>Round-Based Channel State Machine Protocols</h2>\n<p>Today in the LN protocol, we have a full-duplex async state machine. What\nthis means is that both sides can propose updates at any time, without\nrequiring a priori agreement from the other party. At any time, we may have\n4 possible commitment transactions at play: one finalized commitment for\neach party, and another pending commitment (sig received, but revoke not\nsent). Assuming both sides continue to send sigs and revoke their old\ncommitment, then eventually the \u201ccommitment chain head\u201d for both parties\nwill converge on the same set of active HTLCs.</p>\n<p>This scheme is great for throughput, as if both sides sent several\nrevocation points at the start of a connection, then they can continue to\nsend new states without every waiting for the other party, as they consume\nrevocations in a sliding window, gaining a new one from the remote party as\nthe revoke old states. It resembles the sliding window aspect of TCP.</p>\n<p>One downside of this protocol is that we can never recover from a\ndisagreement or error encountered during execution. As both sides just send\nupdates at will (continuing to do so even after retransmission), there\u2019s no\nway to pause or go backwards to recover and resume the protocol.</p>\n<p>One example is the channel reserve. With the current commitment transaction,\neach time either party adds an HTLC, the initiator needs to pay for the fees\nfrom their funding input. However, it\u2019s possible that in order to actually\npay propose new HTLCs at any time, it\u2019s possible that for this fee, they\nneed to dip into the channel reserve. As both parties can propose HTLCs at\nanytime, in order to prevent this edge case, they need to leave enough fee\nbuffer for future HTLCs. However it\u2019s difficult to accurately model what the\nfuture HTLCf low from the remote party will be.</p>\n<p>If we had a round based protocol, then we\u2019d be able to catch all these edge\ncases up front, and ensure that we can always resume channel execution,\navoiding expensive force closes. Such a round based protocol would resemble\nRTS (Request To Send) and CTS (Clear to Send) based flow control protocols\n<a href=\"https://gist.github.com/instagibbs/1d02d0251640c250ceea1c66665ec163\" rel=\"noopener nofollow ugc\">8</a>.</p>\n<p>During normal execution, both sides take rounds proposing changes to the set\nof commitment transactions (add or remove HTLCs). If one side doesn\u2019t have\nany changes, then they can yield to the other party. Importantly, with this\nsimplified protocol, either side can explicitly NACK or ACK a set of\nproposed changes. With the ability to NACK proposed updates, then we can\nrecover from incorrect flows, making the protocol more robust in the face of\nspurious force closes.</p>\n<p>If we send up going with the musig2 based adapter PTLCs, then during the\nround based execution, both parties can send nonces up front, eliminating\nthe difficult to reason about async interleaved nonce exchange. One other\nbonus is that this protocol would likely be a lot easier to reason about, as\nthe current state machine protocol is notoriously underspecified.</p>\n<h1><a name=\"p-3370-getting-fancy-off-chain-super-scalar-channel-factories-friends-7\" class=\"anchor\" href=\"#p-3370-getting-fancy-off-chain-super-scalar-channel-factories-friends-7\"></a>Getting Fancy Off-Chain: Super Scalar, Channel Factories &amp; Friends</h1>\n<p>To close out the first day, we had a session focused on off-chain channel\nconstructions that utilize shared UTXO ownership to enable: off-chain\nchannel creation, cheaper self-custodial mobile on boarding, and batched\nmulti-party transaction execution. Related proposed protocols include:\nchannel factories, timeout trees, ark, clark, etc, etc.</p>\n<p>A recently published proposal, SuperScalar <a href=\"https://delvingbitcoin.org/t/superscalar-laddered-timeout-tree-structured-decker-wattenhofer-factories/1143?\">9</a>, seeks to combine many of\nthese primitives, into a solution to the Last-Mile Problem as pertains to\nself-custodial mobile Lightning. SuperScalar seeks to improve the state of\nthe art while: ensuring the LSP can\u2019t steal funds, not relying on any\nproposed Bitcoin consensus changes, and finally retaining the ability to\nmake forward progress with the system while allowing some/all users to be\noffline.</p>\n<p>SuperScalar can best be understood as combination of 3 techniques: Duplex\nMicropayment Channels <a href=\"https://tik-old.ee.ethz.ch/file/716b955c130e6c703fac336ea17b1670/duplex-micropayment-channels.pdfkj\" rel=\"noopener nofollow ugc\">10</a>, John Law\u2019s Timeout Trees <a href=\"https://github.com/JohnLaw2/ln-scaling-covenants\" rel=\"noopener nofollow ugc\">11</a>, and a laddering\ntechnique that allows the coordinator of the SuperScalar instance to spread\nout their funds and minimize opportunity cost.</p>\n<p>I won\u2019t attempt to describe the scheme in full detail here, instead I\u2019d\nrefer interested parties to the Delving Bitcoin post referenced above. Since\nthe summit, Z has created a few new iterations of his scheme, addressing\nsome of the shortcomings and also branching off in distinct directions.</p>\n<p>At a high level, once you combine all of the above, you have a big tree of\ntransactions, with the leaves of each transaction being a normal 2-party\nchannel, with the LSP and a user. One level up from the channel leaf, is\nanother combined multisig of the participants in that sub-tree. Each leaf\nalso has an additional output dedicated to it, with L additional coins that\ncan be used to allocate additional liquidity to a channel, requiring only\nthe LSP and that user to be online. If more parties are online, then a\nhigher branch in the tree can be re-signed, allowing broader redistribution\nof capacity in the channel.</p>\n<p>The laddering technique comes into play to allow the LSP to distribute their\nfund over several instance of this off-chain tree structure. Timeout trees\nare utilized to give all users a delay based exit path. Rather than needing\nto always force close to reveal the entire tree off-chain, after a period of\ntime, all funds in the construction go to the LSP. This means that users\nneed to jump to the next instance/ladder of the structure, similar to the\nway shared VTXOs work in the Ark construction (which also uses a form of\ntimeout trees). As a result, all channels in the construction no longer have\nan indefinite lifetime: users either need to send all funds out of the\nSuperScalar instance, or work with the LSP to gain a new channel in the next\ninstance. Otherwise, losers forfeit their funds to the LSP.</p>\n<p>The lifetime of a SuperScalar instance can be divided into two periods: an\nactive period, and a dying period. During the active period, users use their\nchannels as normal. They might choose to exit the instance early, but can\nremain offline mostly. During the dying period, the users MUST come online\nto get their funds out to sweep themselves, or two move to another instance\nof the tree. There\u2019s a sort of safety period built in, once the dying period\nstarts, the LSP will stop coordinating with users to make updates to the\ntree, and also likely only sign off on outgoing payments (the LSP is a part\nin all channels, but sub-channels are also possible, with some additional\ntrust).</p>\n<p>Returning back to the extra output L, as described above, the output L is\nfreely spendable by the LSP. If a users need additional capacity, then they\ncan spend L, creating a new sub-channel with a given user, A. However, they\ncan also sign L with another user, B, thereby double spending the output L\n<em>off chain</em>. Only one version of that spend can ever hit the chain, so in\nessence the LSP has overdrawn, potentially stealing funds or causing a user\nto forfeit funds they thought were theirs. One solution to this would be\nusing a signature scheme wherein signing twice causes the singer to lose\ntheir private key. There\u2019re a few ways to construct such a scheme: OP_CAT,\ndecomposing the signature into 7+ instances, or using the two-shot adapter\nsignature scheme described in this paper <a href=\"https://github.com/JohnLaw2/ln-scaling-covenants\" rel=\"noopener nofollow ugc\">11</a>.</p>\n<p>The usage of Duplex Micropayment channels in higher internal branches means\nthe number of internal updates grows, so the does the amount of transactions\na user need to publish in order to forcibly exit from the construct. As\nalways, we eventually bump into an inescapable tradeoff related to the unit\neconomics of blockchains: if the transaction cost required to make a payment\nexceeds the value of the payment itself, it either won\u2019t happen, or will\nhappen on a system that makes a tradeoff of security vs cost. In other\nwords, it may not make much sense for users to have small channels in such a\nconstruct, due to the additional fees required on forced exit. For small\nchannels to be economical then the coordinator needs to subsidize them, or\nusers bank on never needing to manifest them on-chain, as they\u2019re always\nhopping to the next SuperScalar ladder.</p>\n<p>One other interesting topic that came up was a sort of conjecture re the\nimpossibility of securely joining a channel off-chain, without any on-chain\ntransactions at all. To see why this can be difficult, consider a scenario\nwhere Alice and Bob already have a channel, and want to add Carol to the\nchannel. A+B make a new state update, and add a third commitment output to\nthe channel, using Carol\u2019s key. Carol asks A+B for some information to\nconvince her that this is the latest state, but in practice, A+B can always\njust fabricate some imaginary state update history. As the only two signers\nin the root multi-sig, A+B can always just double spend the commitment they\ngave to Carol, removing her from the channel, possibly stealing her balance.\nIf you squint a bit, this ends up resembling the \u201cnothing at stake\u201d issue\nwith PoS chains: there\u2019s no cost for A+B to make a fake history to fool, and\neventually cheat Carol out of her funds\".</p>\n<p>The main conclusion of this impossibility conjecture is that purely\noff-chain dynamic membership (anyone can join and leave at anytime)\nconstructs either require: (1) trust in the root signers, (2) some sort of\nattribution+punishment mechanism, or (3) on-chain transaction(s). Solutions\nin the first category include: Liquid, Statechains, and Ark with\nout-of-round payments. In the second category, over the past year we\u2019ve seen\nthe emergence of systems like BitVM, which rely on a 1-of-n honesty\nassumption, leveraging an interactive on-chain fraud proof to attribute and\npunish faults. In the final category, I\u2019d place constructs such as: Ark,\nSuperScalar, and generally John Law\u2019s timeout trees. In this final category,\nusers use the new output(s) created by the on-chain transaction to verify a\nset of valid+immutable transactions from leaf to root that when broadcast,\nallow them to unilaterally claim their new channel.</p>\n<p>With all that said, I think some relevant takeaways from this section were\nthat:</p>\n<ul>\n<li>\n<p>Devs+services are seeking ways to on board users with a low chain\nfootprint, that is also capital efficient.</p>\n</li>\n<li>\n<p>Prospective solutions seem to incorporate some combination of: channel\nfactories, time out trees, multi-party channels, and ephemeral off-chain\ncoin swap protocols (the Ark family).</p>\n</li>\n<li>\n<p>To avoid taking on too much complexity unnecessarily, any new solution\nshould likely follow an incremental deployment plan, shipping components\nin series, with each of them building on top of each other.</p>\n</li>\n</ul>\n<h1><a name=\"p-3370-bonus-session-lightning-talks-8\" class=\"anchor\" href=\"#p-3370-bonus-session-lightning-talks-8\"></a>Bonus Session: Lightning Talks</h1>\n<p>In between sessions there was a call for lightning talks re anything cool\nthat people were working on.</p>\n<p>One cool idea presented was basically an ability for users to recover from\nspurious force closes. These happen from time to time, due to\ncross-implementation issues, most commonly some sort of fee agreement. The\ngeneral idea here is to just give away an extra key to allow the remote\nparty to spend their output asap if they force close. This would be purely\nan altruistic action on the behalf of the party that didn\u2019t go to chain,\nit\u2019s a a friendly thing to do that helps out the other party.</p>\n<p>Mechanically, one way to accomplish this would be to give the remote party\neverything they need to sweep their output via the revocation path (which is\nusually used by the opposite party). Some also discussed modifying the\noutput derivation construct slightly, and packaging new information in the\nchannel reestablishment message. The non-broadcasting party would only\nreveal this information when they know for sure that the latest state has\nbeen published and confirmed.</p>\n<h1><a name=\"p-3370-make-gossip-suck-less-9\" class=\"anchor\" href=\"#p-3370-make-gossip-suck-less-9\"></a>Make Gossip Suck Less</h1>\n<p>To open up the second day, the first session was focused on identifying\nconcrete improvements that can be made to the gossip protocol.</p>\n<h2><a name=\"p-3370-gossip-syncing-heuristics-10\" class=\"anchor\" href=\"#p-3370-gossip-syncing-heuristics-10\"></a>Gossip Syncing Heuristics</h2>\n<p>The gossip protocol has a well defined structure, but leaves many behavioral\naspects up to the individual implementations. Examples of such behaviors\ninclude: How many sync peers do you maintain? Do you rate limiting incoming\ngossip at all? How to validate new incoming channels (if at all)? Do you\nperiodically spot check the graph for missing channels? Do you just download\neverything from scratch each time?</p>\n<p>Through the course of the conversation, for the most part, each\nimplementation learned of some new thing that other implementations do that\nthey don\u2019t. Anecdotally every few months/weeks, we discover some subtle bug\nthat has been hampering the propagation of new channel updates or channel\nannouncements. lnd found that the biggest improvement to discoverability and\npropagation they made recently was to actually start using the channel\nupdate timestamp information in gossip queries. Without this, node\u2019s aren\u2019t\nable to recognize that though they have the same set of channels as the\nremote party (based in scid\u2019s), they other party may have some <em>newer</em>\nchannels than they did. If an implementation prunes \u201czombie\u201d channels after\nsome period of time, but isn\u2019t actively syncing gossip, then if they don\u2019t\nspot check by looking at the channel update timestamp in gossip queries,\nthen they\u2019re bound to fail to resurrect old zombie channels, thereby missing\nlarge sections of the graph.</p>\n<h2><a name=\"p-3370-gossip-20-11\" class=\"anchor\" href=\"#p-3370-gossip-20-11\"></a>Gossip 2.0</h2>\n<p>Next, we turned to the new upcoming revamp of the gossip protocol, code name\nGossip 2.5 (or 2.0, depending on who you talk to). Since the last spec\nmeeting, lnd has continued to progress on both the spec [14] and\nimplementation <a href=\"https://github.com/lightningnetwork/lnd/pull/8256\" rel=\"noopener nofollow ugc\">15</a>. At this point the spec is waiting on additional\nreview/feedback, with lnd having had the protocol working e2e (new channels\nonly) since the start of the year.</p>\n<p>One new addition discussed was adding SPV proofs to the channel\nannouncements. Some implementation either conditionally (eg: lnd with the\n<code>--routing.assumechanvalid</code> flag) or unconditionally never validate the\non-chain provenance of announced channels. For light clients that use purely\nthe p2p network (eg: Neutrino), fetching tens of thousands of blocks can be\na big sink into power/bandwidth/cpu. If channel announcements optional carry\n(but always commit to!) an SPV proof, then the existence of a channel can be\nverified using only the latest header chain. If only the hash digest/root of\nthe final payload is signed, then nodes that don\u2019t need the extra proofs can\nask the sending party to omit them. In the past lnd has worked on a proof\nformat that supports aggregation at the batch level, which can likely be\nreused <a href=\"https://github.com/lightningnetwork/lnd/pull/5987\" rel=\"noopener nofollow ugc\">16</a>.</p>\n<p>As far as interop testing, other implementations either currently have other\npriorities or may be waiting on upstream libraries to integrate musig2 (post\nsummit, the libsecp PR for musig2 was merged!). Today none of the major\nimplementations have added support for testnet4, so since it presumably has\nno LN channels, attendees agreed to have the testnet4 be the first uniform\ntestbed for gossip 2.0!</p>\n<p>Gossip 2.0 does away with the old timestamp field on channel updates and\nreplaces them with block heights. This simplified rate limiting, as you can\nprescribe that a peer only gets one update per block. As block heights are\nglobally uniform (no local aspects such as timezones), they\u2019re better suited\nfor various set reconciliation protocols. Several attendees had done some\nresearch into repurposing the existing minisketch implementation, though as\nwe have distinct constraints, we may just end up using something different\nall together.</p>\n<p>(NOTE: I spilled coffee all over my laptop mid way through this session, so\nI missed a good chunk of it while troubleshooting).</p>\n<h1><a name=\"p-3370-fundamental-limits-on-payment-delivery-12\" class=\"anchor\" href=\"#p-3370-fundamental-limits-on-payment-delivery-12\"></a>Fundamental Limits on Payment Delivery</h1>\n<p>Next up, we had a session to discuss some of the latest\nresearch/formalizations related to pathfinding/routing in the LN. The main\ntopic was a presentation/discussion related to some new research that\nattempts to uncover the fundamental limitations of payment deliverability in\npayment channel networks <a href=\"https://github.com/renepickhardt/Lightning-Network-Limitations/blob/305db330c96dc751f0615d9abb096b12b8a6191f/Limits%20of%20two%20party%20channels/paper/a%20mathematical%20theory%20of%20payment%20channel%20networks.pdf\" rel=\"noopener nofollow ugc\">13</a>.</p>\n<p>At a high level, the research models the graph as a series of edge+vertices,\nwith each edge carrying 3 attributes: the balance on the local side, the\nbalance on the remote side, and the total capacity. Given a sample graph,\none can determine if a payment is reachable if there exist a series of pair\nwise balance modifications that give the \u201creceiving\u201d party the desired\nbalance end state. Rather than running normal greedy based path finding\nalgorithms, this looks at the global feasibility of a payment as a whole.\nNote that this approach naturally captures the ability to force a rebalance\nduring a payment to make an otherwise unsatisfiable flow satisfiable.</p>\n<p>Inevitably, there\u2019ll be certain payment flows that just won\u2019t be possible at\nall. Reasons for this include insufficient channel capacity, the sender not\nhaving enough balance, or the receiver, etc. When this happens, within the\nmodel an on-chain transaction must occur to either add or remove funds from\nthe network\u2019s active balance set. Examples of on-chain transactions include:\nopening a channel, closing a channel, splicing, or using submarine swaps.</p>\n<p>Based on the above, given some starting assumptions (graph topology, balance\ndistribution, distribution to sample for the likelihood of a payment between\nany two nodes), one can arrive at a sort of upper limit of effective\nthroughput for a payment channel network. To arrive at this value (T), you\ndivide the bandwidth of chain TPS (Q) by the expected rate of infeasible\npayments (R) \u2013 T = Q/R. If we set T to be something like 47k tps, then if\nwe plug in the current TPS of the main chain (~14), then we arrive at\n0.029%, meaning that only 0.29% payments can be infeasible to approach 47k\nTPS.</p>\n<p>Ultimately, these figures boil down to some back of the envelope math based\non simplifying assumptions. One aspect not factored in is the possibly of\nbatching chain interactions, s.t several channels/users can configure their\noff-chain capacity/bandwidth with a single on-chain transaction. The simple\nderivation above also doesn\u2019t factor in balanced payments (eg: I send\nbetween my 2 nodes w/ no fee), which will never need to trigger on-chain\ntransactions, yet aren\u2019t counted towards the TPS derivation. Nevertheless,\nmodels like this are useful to get a feel for the limits of the system in\nthe abstract.</p>\n<h2><a name=\"p-3370-multiparty-channels-credit-channels-13\" class=\"anchor\" href=\"#p-3370-multiparty-channels-credit-channels-13\"></a>Multiparty Channels &amp; Credit Channels</h2>\n<p>The research then identifies two primitives that can help to reduce the\namount of infeasible payments: multiparty channels, and credit within the\nnetwork.</p>\n<p>Multiparty channels aggregate several users in the channel graph,\neffectively forming new fully connected subgraphs. The intuition here is\nthat: if you hold the amt of coins each party adds to a channel as constant,\nthen by increasing the number of parties you also include the max amount\nthat any given user can own. By increasing the max amount that any user can\nown, you reduce the amount of feasible payments due to balance/capacity\nconstraints.</p>\n<p>Turning now to credit, the idea is also simple: if a payment is infeasible,\nthen along some hop, credit can be introduced to permanently or temporarily\nexpand the capacity within a channel, crediting one party with increased\nbalance. To minimize systemic risk, such credit likely shouldn\u2019t be\nintroduced in the core of the network, instead only existing at the edges.\nProtocols such as Taproot Assets can be used in theory to increase payment\nfeasibility while also reducing on boarding costs as they enable users to\nnatively express the concept of addressable/verifiable credit in channels.</p>\n<h1><a name=\"p-3370-last-mile-mobile-on-boarding-14\" class=\"anchor\" href=\"#p-3370-last-mile-mobile-on-boarding-14\"></a>Last Mile Mobile On Boarding</h1>\n<p>To close things out, we had two distinct, but related sessions, focused on\nself-custodial mobile onboarding and UX. First, the Last Mile Problem as it\npertains to mobile UX onboarding <a href=\"https://bitcoinmagazine.com/technical/assessing-the-lightning-networks-last-mile-solutions\" rel=\"noopener nofollow ugc\">17</a>.</p>\n<p>Today in LN, most of the UX challenges arise when a user attempts to pay\nanother user, but the receiver is using a self-custodial mobile wallet. This\nis similar to the last mile transportation problem when it comes to Internet\ninfrastructure and bandwidth: the internal of the network contains \u201cfat\npipes\u201d with high bandwidth to quickly shuffle information around the\ninternal network. However, getting from the internal network to the final\ndestination is more costly, less reliable, and slower.</p>\n<h2><a name=\"p-3370-onboarding-costs-channel-liquidity-15\" class=\"anchor\" href=\"#p-3370-onboarding-costs-channel-liquidity-15\"></a>Onboarding Costs &amp; Channel Liquidity</h2>\n<p>In the LN domain, rather than dealing with aging infrastructure or high\nconstruction costs, the challenges are instead related to aspects of the\nmobile platform itself. Compared to always-online routing nodes, mobile\nnodes need to wake up to sign for new incoming updates. Additionally, if a\nmobile node wants to be set up to be a primarily net receiver (no coins yet,\non boarding directly onto LN) then an existing routing node must commit some\nliquidity towards them. The establishment of this first channel is a capital\nsync on the part of the routing node, as it\u2019s possible the mobile node\nchurns out of the network, leaving an idle channel with funds suspended. To\nregain the funds in the face of a persistently offline user, the routing\nnode then needs to force close, costing chain fees as well as time while the\nrelative time locks expire (up 2 weeks of delay).</p>\n<p>As we dive into the last-mile liquidity costs, we begin to run into some\nfundamental limits of unit economics. If a user is receiving just 10 sats\nover the channel, and it would cost 1000 sats in chain fees to open the\nchannel, then creating connectivity for that user would be a net loss for\nthe routing node (not to mention min channel limits on the network today).\nAny inbound that a routing node allocates to a low activity user could\ninstead be allocated to a higher velocity of corridor of the networking,\nwherein the channel can earn fees to offset the chain allocation costs.\nAssuming the costs are aligned, or subsidies are in place, then\ninfrastructure tools such as: Phoenix Wallet\u2019s JIT channel system, Liquidity\nAds <a href=\"https://github.com/lightning/bolts/pull/1153\" rel=\"noopener nofollow ugc\">18</a>, Sidecar Channels w/ Lightning Pool, Amboss\u2019 Magma, etc, etc.</p>\n<h2><a name=\"p-3370-protocol-induced-ux-concerns-16\" class=\"anchor\" href=\"#p-3370-protocol-induced-ux-concerns-16\"></a>Protocol Induced UX Concerns</h2>\n<p>Interactivity and chain fee on boarding costs aside, the current protocol\ndesign has some abstraction leaks that end up bubbling up into end user\nmobile wallets. One example is the reserve: to ensure that both parties have\nsome skin-in-the-game at all times (deterrence against breach attempts) they\nmust maintain a minimum balance in the channel at all times (commonly ~1%).\nThis confuses users, as they commonly want to send all funds away from their\nwallet to migrate (or otherwise), but instead find they always need to keep\na small amount of funds at all times. Tangentially, as fees rise, then the\nsize of the economically viable channel also rises along with it.</p>\n<h2><a name=\"p-3370-liquidity-fee-rebates-17\" class=\"anchor\" href=\"#p-3370-liquidity-fee-rebates-17\"></a>Liquidity Fee Rebates</h2>\n<p>One solution for the dust/small amount problem that has popped up recently\nis the concept of \u201cfee rebates\u201d used by phoenixd <a href=\"https://phoenix.acinq.co/server\" rel=\"noopener nofollow ugc\">21</a>. A fee rebate is a\nnon-refundable payment towards future inbound channel liquidity. Each time a\nuser receives funds via a special routing node (one that supports this\nprotocol extension), while the user doesn\u2019t yet have a channel, received\nfunds go into this fee rebate bucket. Once the user has enough funds in this\nbucket, then the routing node will open a channel towards it, paying the\nservice and chain fees out of the fee rebate bucket. The min amount needed\nto contribute towards channel opening will vary based on the current service\nand chain fees.</p>\n<p>From a practical perspective, fee rebates work pretty well. Assuming a user\nis eventually able to receive enough, then they can instantly receive funds\nwithout needing to pause for channel opening. Once they have enough funds to\nwarrant an L1 UTXO, then they pay for the creation of that UTXO from their\nfee rebate bucket. This technique can be combined with systems like ecash\n(pending amount represented in a mint), or even credit channels using\nTaproot Assets as mentioned above (asset UTXO represented in a Pocket\nUniverse to defer L1 costs).</p>\n<p>From here, the conversation turned back to various off-chain channel factory\nlike constructions, and their limits when it comes to a certain distribution\nof chain fees, number of users, and the balance distribution of those users.\nBasically if you imagine some sort of construct, either based on timeout\ntrees, then if there\u2019re 100 million users and each user has 1 satoshi it,\nthen it isn\u2019t economically feasible for them to unroll the entire thing on\nchain (fees &gt;\n1 sat). If we assume there\u2019s a built in mechanism for users to move funds\nelsewhere so the coordinator can reclaim the funds (similar pattern for Ark,\netc), then if the users don\u2019t exit in time, the 1 BTC is forfeited to the\ncoordinator. All users trying to go on chain is paramount to just burning\nthe entire amount, so some attendees theorized a \u201cbig red button\u201d that can\nbe used to burn all the existing balances. Ideally burning would require\nsome sort of Script (or client side) verifiable proof that the coordinator\nwas about to cheat somehow.</p>\n<p>While the above scenario is more or less just a thought experiment, I think\nit teases at some of the fundamental limitations when it comes to chain\nfees, and small L1 UTXOs. Nothing terribly new though, this interaction is\nwhy most full nodes by default will recognize the concept of dust: if it\ncosts more than 1/3 of the UTXO balance to pay for fees to move the UTXO,\nthen it\u2019s uneconomical. The same applies for off-chain systems, with only\nsome sort of subsidy or exogenous value system as an escape hatch. Any\ntransfers that are uneconomical on the base chain, or the next higher layer,\nwill inevitably migrate to some other system that retains the BTC unit of\naccount, but trades off cheap fees for security.</p>\n<h1><a name=\"p-3370-bolt-12-whats-next-18\" class=\"anchor\" href=\"#p-3370-bolt-12-whats-next-18\"></a>BOLT 12: What\u2019s Next</h1>\n<p>Amidst the haze, savory flavors, and cold beverages of all you can eat+drink\nSukiyaki the BOLT 12 PR was merged into the spec repo! Earlier in the day,\nas one of the last sessions of the summit, we had a session focused on\nwhat\u2019s next after BOLT 12, namely what extensions that were cut from the\noriginal version are people interested in pursuing.</p>\n<h2><a name=\"p-3370-potential-bolt-12-extensions-19\" class=\"anchor\" href=\"#p-3370-potential-bolt-12-extensions-19\"></a>Potential BOLT 12 Extensions</h2>\n<p>The first extension discussed was: invoice replacement. Consider a case\nwhere a user fetches an invoice using an Offer, but waits too long before\npaying, so all the blinded paths and/or invoice itself are expired. In this\nscenario, it\u2019d be useful for the user to be able to ask for a replacement\ninvoice. Exactly how this differs from just fetching another fresh invoice\nusing the Offer is perhaps a contextual question.</p>\n<p>One area that some of the implementers seemed most poised to dust off again\nis: recurrent payments. Portions of recurrence were part of the original\nspec, but were eventually ripped out to slim things down some. Relevant\nrecurrence params include: the time interval, the payment window, limit, and\nthen start+end period. A neat trick that the receiver can use is utilizing a\nhash chain to minimize the amount of preimage storage they need. If they can\nsend the sender a special salt/seed (during initial negotiation), then only\nthe sender+receiver would be aware that the preimages nicely arrange into a\nhash chain.</p>\n<p>On the topic of authentication, a notion of the sort of reverse version of\nBIP 353 was brought up. The general idea is to give users the ability to bind a\nnode\u2019s public key to a domain name. This would serve to authenticate that\nnode Y is actually associated with some service/domain/company.</p>\n<h2><a name=\"p-3370-onion-message-rate-limiting-back-pressure-propagation-20\" class=\"anchor\" href=\"#p-3370-onion-message-rate-limiting-back-pressure-propagation-20\"></a>Onion Message Rate Limiting &amp; Back Pressure Propagation</h2>\n<p>At the tail end of the session, the focus shifted to onion messaging, and\nthe current state of implementation/behavior across the major\nimplementations. One topic raised was how wallets are handling the fallback\nand related UX implications if/when a wallet <em>fails</em> to fetch an Offer.\nOnion messaging is an unreliable, best-effort forwarding network w/o any\nbuilt-in acknowledgements, so it\u2019s possible that a message is just never\ndelivered. As a result, wallets need to be ready to either try another\nroute, retransmit the message, or fallback to some other mechanism if/when a\nwallet fails to fetch an invoice using an Offer.</p>\n<p>Generally the current state of things is that either a single hop onion\nmessaging route it used, or a direct connection. A direct connection refers\nto connection over the p2p network to either the receiver, the introduction\npoint, or nodes leading up to the introduction point in an attempt to send a\nmessage that travels over a shorter path). If <em>that</em> attempt fails (no nodes\nlistening, receiver not offline, etc) then wallets either need some other\nfallback, or may attempt to send some sort of spontaneous payment.</p>\n<p>Returning back to message delivery, it\u2019s clear that some sort of rate\nlimiting is needed. Nodes may start with some sort of free budget, but will\nneed to throttle messaging as otherwise tens a node could mindlessly forward\n10s of GBs of free onion messaging traffic (IMO, it\u2019s inevitable that after\na free tier, most nodes will end up switching to a bandwidth metered payment\nsystem for onion messaging <a href=\"https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/03498.html\" rel=\"noopener nofollow ugc\">22</a>). Therefore nodes will need to adopt some\nsort of bandwidth and rate limiting. If the network is significantly over\nprovisioned relative to the typical messaging usage, then service will\nremain relatively high, as nothing approaches the configured bandwidth\nlimits. However if the network is under provisioned relative to messaging\nactivity (people are trying to livestream their gaming sessions or w/e),\nthen service is hampered, as most message attempts fail due to a tragedy of\nthe commons). As is, the difference between a message being dropped, not\ndelivered, or an offline receiver are all indistinguishable from each other,\ncreating further UX challenges.</p>\n<p>Eventually discussion turned to the old back pressure rate limiting\nalgorithm previously proposed on the mailing list <a href=\"https://gist.github.com/t-bast/e37ee9249d9825e51d260335c94f0fcf\" rel=\"noopener nofollow ugc\">23</a>.  While this\nalgorithm, nodes can maintain a relatively compact description of the set of\npeers that had sent them message last. Once a peer exceeds a limit, then an\n<code>onion_message_drop</code> message is sent to the sender. The sender then attempts\nto trace back who sent the message to itself, further propagating the\n<code>onion_message_drop</code> message backwards, halving the rate limit in the\nprocess. If the sender doesn\u2019t overflow the rate limit within a 30 second\ninterval, then the receiver should double their rate limit until it reaches\nthe normal rate limit.</p>\n<p>There\u2019re some open questions lingering here such as: How can nodes make sure\nthey are attributing the spam to the proper peer? Is it possible for nodes\nto frame other nodes to cut off their messaging activity? Is there any\nadditional meta data required to properly attribute the source of the spam?\nIs this resilient to a spammer that knows the rate limit and can stay right\nunder it, while maximizing utilized bandwidth? When this scheme was\noriginally brought up, some basic simulations were run <a href=\"https://gist.github.com/joostjager/bca727bdd4fc806e4c0050e12838ffa3\" rel=\"noopener nofollow ugc\">24</a> to gauge the\nefficacy and resilience of the scheme. Initial results were promising, with\nsome additional research questions posed <a href=\"https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-July/003663.html\" rel=\"noopener nofollow ugc\">25</a>.</p>\n<p>Ultimately, some attendees agreed to start to revive the work/research on\nthe back pressure algorithm, applying conservative rate limiting parameters\nin the short term.</p>\n<p>\u2013 Laolu</p>",
  "post_number": 1,
  "post_type": 1,
  "posts_count": 4,
  "updated_at": "2024-10-16T05:53:32.808Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 1636,
  "reads": 89,
  "readers_count": 88,
  "score": 8342.8,
  "yours": false,
  "topic_id": 1198,
  "topic_slug": "ln-summit-2024-notes-summary-commentary",
  "topic_title": "LN Summit 2024 Notes & Summary/Commentary",
  "topic_html_title": "LN Summit 2024 Notes &amp; Summary/Commentary",
  "category_id": 7,
  "display_username": "",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 3,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "Hi y'all,\n\n<div data-theme-toc=\"true\"> </div>\n\n~3 weeks ago, 30+ Lightning developers and researchers gathered in Tokyo,\nJapan for three days to discuss a number of matters related to the current\nstate and future evolution of the Lightning protocol (and where relevant,\nthe Bitcoin p2p and consensus protocol). \n\nThe last such gathering took place in June found here:\nhttps://lists.linuxfoundation.org/pipermail/lightning-dev/2022-June/003600.html.\n2022 in Oakland, California. The prior raw meetings notes and summary can be\nfound here:\nhttps://docs.google.com/document/d/1KHocBjlvg-XOFH5oG_HwWdvNBIvQgxwAok3ZQ6bnCW0/edit?usp=sharing. \n\nMy raw meetings notes for this instance of the Lightning Developer Summit\ncan be found here:\nhttps://docs.google.com/document/d/1erQfnZjjfRBSSwo_QWiKiCZP5UQ-MR53ZWs4zIAVcqs/edit?usp=sharing.\nThe rough daily schedule of discussion topics we agreed upon can be found\nhere: https://gist.github.com/Roasbeef/5ac91c57cb9826c628b1445670219728.\n\nIt's worth noting that there were many other break out groups and side\ndiscussions that may not have been captured in my best-effort notes above,\nnor reflected in the schedule I put together above.\n\nWith all that said, what follows is my attempt to summarize the major\ndiscussion points, and conclusions (with some side commentary ofc ;)). If an\nattendee finds anything in my summary inaccurate or incomplete, then\ndiscussion and decision points that took place across the 3 days of the\nsummit, please feel free to reply and fill in the gaps.\n\n# What's up with Package Relay and V3 Commitments Anyway?\n\nThis was the first major topic discussed, right on the heels of the latest\nrelease candidate of Bitcoin Core 28.0 (at the time of writing, has now been\nreleased).\n\n## Fee Estimation & Base Commitments\n\nBefore jumping into the latest and greatest as far the newly proposed\ncommitment design, I'll first give a brief overview on how the commitment\ndesign works today, and the shortcomings thereof.\n\n(if you already know how commitment transactions work today in LN, then you\ncan skip this section)\n\nA key aspect of the design of the Lightning Network is the concept of\nunilateral exit. At any time, either party needs to be able to forcibly exit\nfrom the channel to reclaim their funds after a time delay. A time delay is\nneeded as it's possible that one party attempts to cheat by publishing an\nold revoked state to the chain. The time delay gives the honest party a\nchance to refute the claim by an adversary, proving that they know a secret\nwhich could only have been utilized if a revoked state was published.\n\nThe ability to unilaterally exit from a channel is also a key component\nrequired to be able to enforce the HTLC contract, which implements the\n\"claim of refund\" functionality that makes multi-hop Lightning payments\npossible. These contracts have another time delay component: an absolute\ntime delay. Along a route, each hop has T blocks (the CLTV time lock delta)\nto confirm their commitment transaction, and also time out any lingering\noutgoing HTLCs. If they can't confirm in time, then they lose risking funds\nwith a \"one sided redemption\", as then the incoming HTLC will timeout,\ncreating a timeout/redemption race condition.\n\nA node's ability to achieve timely confirmation of their commitment\ntransaction depends on their ability to do effective fee estimation. If the\ncommitment transaction (which can't unilaterally be modified by a single\nparty) has insufficient fees, then it may not be confirmed in time (or even\nbe accepted to the mempool!). Today, the initiator of a channel can send the\n`update_fee` message to increase (or decrease) the current commitment fee\nrate. This is a critical tool, however it forces the initiator to either be\nprepared to significantly overpay for the commitment fee (to ensure they can\nland in the next block) or expertly guess what the fee will be ahead of\ntime. As the initiator pays all fees on the commitment transaction, the\nresponder is unable to directly influence what this fee may be, instead\ntheir only recourse today is to attempt to force close if they disagree on\nwhat the fee should be. \n\n## Anchor Outputs to the Rescue!\n\nIn order to address some of the shortcomings of the existing commitment\ntransaction format, anchor outputs were proposed [1]. The general idea is\nthat both peers gain a near-dust output that can be spent by only them,\nwhich allows bumping the fee of the commitment transaction after the fact.\nThis lessens, but doesn't eliminate the requirement to estimate what future\nfees will be. Rather than ned to get into the very next block, the goal is\nnow to have a fee high enough to get into the *mempool*. Once in the\nmempool, then either side can bump the fee to eventually confirm the\ncommitment transaction. Along the way, we also allowed second-level HTLCs to\nbe bundled together, allowing for a greater degree of batching when sweeping\nresolved contracts. The fee needed to get into the mempol, similar to the\nfee needed to get into the next block, is also a moving target. As nodes\nhave a default constrained mempool of ~300 MB, as the transaction arrival\nrate increases, nodes begin to drop transactions from their mempool,\nincreasing their minimum transaction fee in the process. Eventually, the\nexisting max anchor transaction fee may not be high enough to actually get\ninto the mempool, meaning the commitment transaction is dropped out. When\nthis happens, nodes no longer have a way (using the existing p2p network) to\npropagate their commitment transaction, meaning it may not confirm in time\nto avoid the redemption race condition, or at all.\n\nAlong the way developers and researcher discovered a series of subtle\ninteractions with transaction broadcast, and mempool relay policies that may\nactually allow an adversary to \"pin\" [2] a transaction to the mempool,\nthereby purposefully preventing its confirmation. The various known pinning\nvectors take advantage of degenerate cases related to BIP 125, and the\nvarious ancestor limits widely utilized in mempool policy today.\n\n## V3, TRUC, and 1P1C to the Rescue (actually this time)!\n\nEnter transaction v3 and TRUC. The ultimate dream of LN developers has been\nto just get rid of `update_fee`, and instead just have the commitment be\n*zero fee*. This takes away all the guesswork of figuring out what the true\nmempool fee should be. In isolation however, if the commitment transaction\nis zero fee, then it won't be accepted to the mempool, so it won't propagate\nacross the p2p network.\n\nA combination of TRUC (a.k.a BIP 434), a new style of anchors, and\noptimistic 1-Parent-1-Child relay is the current best known solution that\npractical addresses LN's current transaction relay and confirmation\nproblems.\n\nTRUC introduces a new set of transaction replacement rules meant to address\nthe degenerate cases of BIP 125 for a subset of use cases. Along the way, it\nintroduces a new set of transaction topology size restrictions to further\nconstrain the problem. TRUC transactions use a transaction version of 3\n(instead of a sequence number like BIP 125) to signal that a transaction\nwishes to opt into the new rule set. \n\nAlso available with Bitcoin Core 28.0, is a new standard public key script\ntype: PayToAnchor (P2A) [2]. P2A is a new special segwit v1 output (`OP_1\n<0x4e73>`) meant to be used for the purpose of fee bumping via CPFP. The\ninput spending this output _must_ have a pure witness input, and can be used\nwithout a signature. Future versions of this new output type may eventually\nallow outputs to be dust, as long as they're immediately spent in the same\nblock they're created within (via CPFP).\n\nThe final component of the design for the newly revamped LN commitment\ntransaction is: 1-Parent-1-Child (1P1C) [4]. 1P1C is basically opportunistic\npackage relay. Rather than rely on a new p2p message which may take time to\nbe deployed across the entire network, 1P1C has nodes change their behavior\nwhen it comes to receiving orphan transactions (txns where the node doesn't\nknow the inputs). Instead of just storing the child in the orphan pool, the\nnode will now opportunistically attempt to fetch the parent from the\nreporting node, even if the parent has a fee which is below the current\nmempol min fee.\n\nIn concert, these three new transaction relay primitives can be used to\nre-design the Lightning commitment transaction format (namely anchors) to\nresolve many of the long standing issues described above. t-bast has already\nstarted to prototype what the new commitment format would look like:\nhttps://x.com/realtbast/status/1834213774674247987.\n\nWith all that said, there're a few open design questions, including:\n  * How should dust be handled?\n    * With the P2A approach, we can put all the dust into the anchor output\n      itself, instead of going to miner fees as they do today. This would\n      resolve some known issues related to an excessive amount of dust on a\n      commitment transaction, but also add some new interactions if the P2A\n      anchor output requires no signature.\n  * Should the P2A output actually be keyless?\n    * If the output is keyless, then any 3rd party can *immediately* help to\n      peers can sweep, until 16 blocks after confirmation). sweep the anchor\n      output (compared to today when only the two channel\n    * Related to the above, if we put all the dust outputs in the P2A, then\n      whoever sweeps the P2A can also claim the entire dust output.\n      Naturally miners would be the parties that are most reliably able to\n      claim these funds, assuming there's no signature required.\n    * Some argued that we should still have a signature here, as then it\n      restricts the parties that can interfere with the channel peers\n      attempting to confirm the commitment transaction. This is meant to\n      hedge against a future discovered defect in the TRUC+P2A interaction,\n      that enables a yet to be discovered type of pinning.\n  * Does the viral nature of V3 transactions impede certain use cases\n    related to advanced splicing?\n    * All unconfirmed descendents from a v3 transaction must also be v3\n      themselves. There was some concern that this would impact uses of\n      splicing where a node attempts to satisfy several transaction flows\n      with a single batched transaction. The viral nature of the v3\n      transaction type would then force any parties that are spending\n      unconfirmed change to themselves use txn v3, which may be beyond the\n      extents of their current capabilities.\n\n\nThe new TRUC rules also allow for a type of package RBF, where if there's a\nnew conflicting package, then it'll attempt to do RBF against the existing\nconflicting package set. This is also known as sibling eviction in the\ncontext of 1P1C [5].\n\nAll the information above can be found in more detail in this handy dandy\nguide for wallet developers [6].\n\nWhen the dust settled, this was probably one of the more concrete new\ninitiatives following the spec summit. From here, we'll begin to spec out\nexactly what the new v3 commitment type looks like, while we wait for a\nsufficient amount of the p2p network to update to the new version, so we can\nrely on the new relay behavior.\n\nOne thing worth calling out is that this shift will have further\nimplications w.r.t how the wallets that back Lightning nodes handle UTXO\nstockpiling. With this approach, given the commitment transaction will have\nzero fees, in order to confirm the transaction, a wallet _must_ use an\nexisting UTXO to anchor down the commitment transaction for it to be even\nbroadcastable. In practice, this means that wallets will need to reserve\nfunds on chain for the purpose of force closing channels in a timely manner.\nTools such as splicing, and submarine swaps can be used to allow wallets to\nshift around funds, or batch several on-chain transactions in one.\n\n# PTLCs & Simplified Commitments\n\nNext up was a session focussed on discussing the combination of PTLCs, and\nthe Simplified Channel State Machine. At first, these two topics might\nappear to be completely unrelated, but as we'll see shortly, some of the\nedge cases that arise with design considerations of PTLCs can be mitigated\nby modifying the current channel state machine protocol (dubbed the\nLightning Commitment Protocol - LCP) to a simplified variant.\n\nFirst, a brief background on PTLCs. Today in the LN protocol, we use payment\nhashes to implement the multi-hop claim-or-refund gadget that allows for\ntrust-minimize multi-hop payments. While simple, the current protocol has a\nbig privacy drawback: the payment hash is the same over the entire route, so\nif an adversary appears in two locations in a route, then they can trivially\n  correlate a payment (and also disrupt MPP circuits). \n\nTo fix this privacy hole, many years ago developers proposed we switch over\nto using EC points and private keys instead of payment hashes and preimages.\nIn\n2018 a formalized scheme was published [7], that would actually allow the\nconstruction to be instantiated using adapter signatures across both the\nECDSA and Schnorr signature schemes. This was interesting, as it meant that\nwe didn't necessarily have to wait for taproot (which enabled schnorr) to be\nactivated. Instead, a single multi-hop lock could be transported across\nECDSA and schnorr hops alike. Ultimately, for various reasons, this hybrid\nversion was never deployed. The upside is that we can now deploy a simpler,\nunified, schnorr-only version of multi-hop locks.\n\nFast forwarding to the present, instagibbs, alongside of his research into a\nconcrete design of LN-Symmetry has been exploring the design landscape, from\nmessaging changes to state machine extensions [8].\n\nAfter discussing some of his findings, we started to zoom in on some key\ndesign questions:\n  1. Should we use a single signature, or multi-sig adapter signature? With\n  either approach, the adapter `T` used to create the signature will allow\n  the proposing party to complete the private key needed to sign for the\n  incoming HTLC.\n\n     The musig2 based adapter signature is smaller (just a single sig\n     instead of two), but adds extra coordination requirements as a nonce\n     needs to be sent by both parties to properly create a new commitment\n     transaction.\n\n     The single sig adapter is larger (two signatures, just like current\n     second-level HTLCs today), but simplifies the protocol, as HTLC\n     signatures can be sent alongside the `commit_sig` message as usual.\n\n  2. If we decide to go with the musig2 adapter signature design, then\n  should we attempt to retain the current full-duplex async LCP flow, or\n  should we instead simplify further, instead going for a simple sync\n  commitment state machine protocol?\n\n     The introduction of musig2 nonces for second-level HTLCs complicates\n     the existing LCP protocol as we can no longer send the second-level\n     HTLC signatures along side the `commit_sig` message, as the proposer\n     now needs a partial signature from the accepting party before they can\n     safely proceed.\n\n     However, if we modify the channel state machine protocol to instead be\n     _round based_, then though we sacrifice some x-put, we don't need to\n     worry about handling the various interleaved executions possible\n     (simultaneously `update_add_sig`+`commit_sig` by both parties). This\n     brings us to the topic of Simplified Commit.\n\n\n## Round-Based Channel State Machine Protocols\n\nToday in the LN protocol, we have a full-duplex async state machine. What\nthis means is that both sides can propose updates at any time, without\nrequiring a priori agreement from the other party. At any time, we may have\n4 possible commitment transactions at play: one finalized commitment for\neach party, and another pending commitment (sig received, but revoke not\nsent). Assuming both sides continue to send sigs and revoke their old\ncommitment, then eventually the \"commitment chain head\" for both parties\nwill converge on the same set of active HTLCs.\n\nThis scheme is great for throughput, as if both sides sent several\nrevocation points at the start of a connection, then they can continue to\nsend new states without every waiting for the other party, as they consume\nrevocations in a sliding window, gaining a new one from the remote party as\nthe revoke old states. It resembles the sliding window aspect of TCP.\n\nOne downside of this protocol is that we can never recover from a\ndisagreement or error encountered during execution. As both sides just send\nupdates at will (continuing to do so even after retransmission), there's no\nway to pause or go backwards to recover and resume the protocol.\n\nOne example is the channel reserve. With the current commitment transaction,\neach time either party adds an HTLC, the initiator needs to pay for the fees\nfrom their funding input. However, it's possible that in order to actually\npay propose new HTLCs at any time, it's possible that for this fee, they\nneed to dip into the channel reserve. As both parties can propose HTLCs at\nanytime, in order to prevent this edge case, they need to leave enough fee\nbuffer for future HTLCs. However it's difficult to accurately model what the\nfuture HTLCf low from the remote party will be.\n\nIf we had a round based protocol, then we'd be able to catch all these edge\ncases up front, and ensure that we can always resume channel execution,\navoiding expensive force closes. Such a round based protocol would resemble\nRTS (Request To Send) and CTS (Clear to Send) based flow control protocols\n[8]. \n\nDuring normal execution, both sides take rounds proposing changes to the set\nof commitment transactions (add or remove HTLCs). If one side doesn't have\nany changes, then they can yield to the other party. Importantly, with this\nsimplified protocol, either side can explicitly NACK or ACK a set of\nproposed changes. With the ability to NACK proposed updates, then we can\nrecover from incorrect flows, making the protocol more robust in the face of\nspurious force closes.\n\nIf we send up going with the musig2 based adapter PTLCs, then during the\nround based execution, both parties can send nonces up front, eliminating\nthe difficult to reason about async interleaved nonce exchange. One other\nbonus is that this protocol would likely be a lot easier to reason about, as\nthe current state machine protocol is notoriously underspecified.\n\n\n# Getting Fancy Off-Chain: Super Scalar, Channel Factories & Friends\n\nTo close out the first day, we had a session focused on off-chain channel\nconstructions that utilize shared UTXO ownership to enable: off-chain\nchannel creation, cheaper self-custodial mobile on boarding, and batched\nmulti-party transaction execution. Related proposed protocols include:\nchannel factories, timeout trees, ark, clark, etc, etc.\n\nA recently published proposal, SuperScalar [9], seeks to combine many of\nthese primitives, into a solution to the Last-Mile Problem as pertains to\nself-custodial mobile Lightning. SuperScalar seeks to improve the state of\nthe art while: ensuring the LSP can't steal funds, not relying on any\nproposed Bitcoin consensus changes, and finally retaining the ability to\nmake forward progress with the system while allowing some/all users to be\noffline.\n\nSuperScalar can best be understood as combination of 3 techniques: Duplex\nMicropayment Channels [10], John Law's Timeout Trees [11], and a laddering\ntechnique that allows the coordinator of the SuperScalar instance to spread\nout their funds and minimize opportunity cost.\n\nI won't attempt to describe the scheme in full detail here, instead I'd\nrefer interested parties to the Delving Bitcoin post referenced above. Since\nthe summit, Z has created a few new iterations of his scheme, addressing\nsome of the shortcomings and also branching off in distinct directions.\n\nAt a high level, once you combine all of the above, you have a big tree of\ntransactions, with the leaves of each transaction being a normal 2-party\nchannel, with the LSP and a user. One level up from the channel leaf, is\nanother combined multisig of the participants in that sub-tree. Each leaf\nalso has an additional output dedicated to it, with L additional coins that\ncan be used to allocate additional liquidity to a channel, requiring only\nthe LSP and that user to be online. If more parties are online, then a\nhigher branch in the tree can be re-signed, allowing broader redistribution\nof capacity in the channel.\n\nThe laddering technique comes into play to allow the LSP to distribute their\nfund over several instance of this off-chain tree structure. Timeout trees\nare utilized to give all users a delay based exit path. Rather than needing\nto always force close to reveal the entire tree off-chain, after a period of\ntime, all funds in the construction go to the LSP. This means that users\nneed to jump to the next instance/ladder of the structure, similar to the\nway shared VTXOs work in the Ark construction (which also uses a form of\ntimeout trees). As a result, all channels in the construction no longer have\nan indefinite lifetime: users either need to send all funds out of the\nSuperScalar instance, or work with the LSP to gain a new channel in the next\ninstance. Otherwise, losers forfeit their funds to the LSP.\n\nThe lifetime of a SuperScalar instance can be divided into two periods: an\nactive period, and a dying period. During the active period, users use their\nchannels as normal. They might choose to exit the instance early, but can\nremain offline mostly. During the dying period, the users MUST come online\nto get their funds out to sweep themselves, or two move to another instance\nof the tree. There's a sort of safety period built in, once the dying period\nstarts, the LSP will stop coordinating with users to make updates to the\ntree, and also likely only sign off on outgoing payments (the LSP is a part\nin all channels, but sub-channels are also possible, with some additional\ntrust).\n\nReturning back to the extra output L, as described above, the output L is\nfreely spendable by the LSP. If a users need additional capacity, then they\ncan spend L, creating a new sub-channel with a given user, A. However, they\ncan also sign L with another user, B, thereby double spending the output L\n_off chain_. Only one version of that spend can ever hit the chain, so in\nessence the LSP has overdrawn, potentially stealing funds or causing a user\nto forfeit funds they thought were theirs. One solution to this would be\nusing a signature scheme wherein signing twice causes the singer to lose\ntheir private key. There're a few ways to construct such a scheme: OP_CAT,\ndecomposing the signature into 7+ instances, or using the two-shot adapter\nsignature scheme described in this paper [11].\n\nThe usage of Duplex Micropayment channels in higher internal branches means\nthe number of internal updates grows, so the does the amount of transactions\na user need to publish in order to forcibly exit from the construct. As\nalways, we eventually bump into an inescapable tradeoff related to the unit\neconomics of blockchains: if the transaction cost required to make a payment\nexceeds the value of the payment itself, it either won't happen, or will\nhappen on a system that makes a tradeoff of security vs cost. In other\nwords, it may not make much sense for users to have small channels in such a\nconstruct, due to the additional fees required on forced exit. For small\nchannels to be economical then the coordinator needs to subsidize them, or\nusers bank on never needing to manifest them on-chain, as they're always\nhopping to the next SuperScalar ladder.\n\nOne other interesting topic that came up was a sort of conjecture re the\nimpossibility of securely joining a channel off-chain, without any on-chain\ntransactions at all. To see why this can be difficult, consider a scenario\nwhere Alice and Bob already have a channel, and want to add Carol to the\nchannel. A+B make a new state update, and add a third commitment output to\nthe channel, using Carol's key. Carol asks A+B for some information to\nconvince her that this is the latest state, but in practice, A+B can always\njust fabricate some imaginary state update history. As the only two signers\nin the root multi-sig, A+B can always just double spend the commitment they\ngave to Carol, removing her from the channel, possibly stealing her balance.\nIf you squint a bit, this ends up resembling the \"nothing at stake\" issue\nwith PoS chains: there's no cost for A+B to make a fake history to fool, and\neventually cheat Carol out of her funds\".\n\nThe main conclusion of this impossibility conjecture is that purely\noff-chain dynamic membership (anyone can join and leave at anytime)\nconstructs either require: (1) trust in the root signers, (2) some sort of\nattribution+punishment mechanism, or (3) on-chain transaction(s). Solutions\nin the first category include: Liquid, Statechains, and Ark with\nout-of-round payments. In the second category, over the past year we've seen\nthe emergence of systems like BitVM, which rely on a 1-of-n honesty\nassumption, leveraging an interactive on-chain fraud proof to attribute and\npunish faults. In the final category, I'd place constructs such as: Ark,\nSuperScalar, and generally John Law's timeout trees. In this final category,\nusers use the new output(s) created by the on-chain transaction to verify a\nset of valid+immutable transactions from leaf to root that when broadcast,\nallow them to unilaterally claim their new channel.\n\nWith all that said, I think some relevant takeaways from this section were\nthat: \n\n  * Devs+services are seeking ways to on board users with a low chain\n    footprint, that is also capital efficient.\n\n  * Prospective solutions seem to incorporate some combination of: channel\n    factories, time out trees, multi-party channels, and ephemeral off-chain\n    coin swap protocols (the Ark family).\n\n  * To avoid taking on too much complexity unnecessarily, any new solution\n    should likely follow an incremental deployment plan, shipping components\n    in series, with each of them building on top of each other.\n\n# Bonus Session: Lightning Talks\n\nIn between sessions there was a call for lightning talks re anything cool\nthat people were working on.\n\nOne cool idea presented was basically an ability for users to recover from\nspurious force closes. These happen from time to time, due to\ncross-implementation issues, most commonly some sort of fee agreement. The\ngeneral idea here is to just give away an extra key to allow the remote\nparty to spend their output asap if they force close. This would be purely\nan altruistic action on the behalf of the party that didn't go to chain,\nit's a a friendly thing to do that helps out the other party.\n\nMechanically, one way to accomplish this would be to give the remote party\neverything they need to sweep their output via the revocation path (which is\nusually used by the opposite party). Some also discussed modifying the\noutput derivation construct slightly, and packaging new information in the\nchannel reestablishment message. The non-broadcasting party would only\nreveal this information when they know for sure that the latest state has\nbeen published and confirmed.\n\n# Make Gossip Suck Less\n\nTo open up the second day, the first session was focused on identifying\nconcrete improvements that can be made to the gossip protocol.\n\n## Gossip Syncing Heuristics\n\nThe gossip protocol has a well defined structure, but leaves many behavioral\naspects up to the individual implementations. Examples of such behaviors\ninclude: How many sync peers do you maintain? Do you rate limiting incoming\ngossip at all? How to validate new incoming channels (if at all)? Do you\nperiodically spot check the graph for missing channels? Do you just download\neverything from scratch each time?\n\nThrough the course of the conversation, for the most part, each\nimplementation learned of some new thing that other implementations do that\nthey don't. Anecdotally every few months/weeks, we discover some subtle bug\nthat has been hampering the propagation of new channel updates or channel\nannouncements. lnd found that the biggest improvement to discoverability and\npropagation they made recently was to actually start using the channel\nupdate timestamp information in gossip queries. Without this, node's aren't\nable to recognize that though they have the same set of channels as the\nremote party (based in scid's), they other party may have some _newer_\nchannels than they did. If an implementation prunes \"zombie\" channels after\nsome period of time, but isn't actively syncing gossip, then if they don't\nspot check by looking at the channel update timestamp in gossip queries,\nthen they're bound to fail to resurrect old zombie channels, thereby missing\nlarge sections of the graph.\n\n## Gossip 2.0\n\nNext, we turned to the new upcoming revamp of the gossip protocol, code name\nGossip 2.5 (or 2.0, depending on who you talk to). Since the last spec\nmeeting, lnd has continued to progress on both the spec [14] and\nimplementation [15]. At this point the spec is waiting on additional\nreview/feedback, with lnd having had the protocol working e2e (new channels\nonly) since the start of the year.\n\nOne new addition discussed was adding SPV proofs to the channel\nannouncements. Some implementation either conditionally (eg: lnd with the\n`--routing.assumechanvalid` flag) or unconditionally never validate the\non-chain provenance of announced channels. For light clients that use purely\nthe p2p network (eg: Neutrino), fetching tens of thousands of blocks can be\na big sink into power/bandwidth/cpu. If channel announcements optional carry\n(but always commit to!) an SPV proof, then the existence of a channel can be\nverified using only the latest header chain. If only the hash digest/root of\nthe final payload is signed, then nodes that don't need the extra proofs can\nask the sending party to omit them. In the past lnd has worked on a proof\nformat that supports aggregation at the batch level, which can likely be\nreused [16].\n\nAs far as interop testing, other implementations either currently have other\npriorities or may be waiting on upstream libraries to integrate musig2 (post\nsummit, the libsecp PR for musig2 was merged!). Today none of the major\nimplementations have added support for testnet4, so since it presumably has\nno LN channels, attendees agreed to have the testnet4 be the first uniform\ntestbed for gossip 2.0! \n\nGossip 2.0 does away with the old timestamp field on channel updates and\nreplaces them with block heights. This simplified rate limiting, as you can\nprescribe that a peer only gets one update per block. As block heights are\nglobally uniform (no local aspects such as timezones), they're better suited\nfor various set reconciliation protocols. Several attendees had done some\nresearch into repurposing the existing minisketch implementation, though as\nwe have distinct constraints, we may just end up using something different\nall together.\n\n\n(NOTE: I spilled coffee all over my laptop mid way through this session, so\nI missed a good chunk of it while troubleshooting).\n\n# Fundamental Limits on Payment Delivery\n\nNext up, we had a session to discuss some of the latest\nresearch/formalizations related to pathfinding/routing in the LN. The main\ntopic was a presentation/discussion related to some new research that\nattempts to uncover the fundamental limitations of payment deliverability in\npayment channel networks [13].\n\nAt a high level, the research models the graph as a series of edge+vertices,\nwith each edge carrying 3 attributes: the balance on the local side, the\nbalance on the remote side, and the total capacity. Given a sample graph,\none can determine if a payment is reachable if there exist a series of pair\nwise balance modifications that give the \"receiving\" party the desired\nbalance end state. Rather than running normal greedy based path finding\nalgorithms, this looks at the global feasibility of a payment as a whole.\nNote that this approach naturally captures the ability to force a rebalance\nduring a payment to make an otherwise unsatisfiable flow satisfiable.\n\nInevitably, there'll be certain payment flows that just won't be possible at\nall. Reasons for this include insufficient channel capacity, the sender not\nhaving enough balance, or the receiver, etc. When this happens, within the\nmodel an on-chain transaction must occur to either add or remove funds from\nthe network's active balance set. Examples of on-chain transactions include:\nopening a channel, closing a channel, splicing, or using submarine swaps.\n\nBased on the above, given some starting assumptions (graph topology, balance\ndistribution, distribution to sample for the likelihood of a payment between\nany two nodes), one can arrive at a sort of upper limit of effective\nthroughput for a payment channel network. To arrive at this value (T), you\ndivide the bandwidth of chain TPS (Q) by the expected rate of infeasible\npayments (R) -- T = Q/R. If we set T to be something like 47k tps, then if\nwe plug in the current TPS of the main chain (~14), then we arrive at\n0.029%, meaning that only 0.29% payments can be infeasible to approach 47k\nTPS.\n\nUltimately, these figures boil down to some back of the envelope math based\non simplifying assumptions. One aspect not factored in is the possibly of\nbatching chain interactions, s.t several channels/users can configure their\noff-chain capacity/bandwidth with a single on-chain transaction. The simple\nderivation above also doesn't factor in balanced payments (eg: I send\nbetween my 2 nodes w/ no fee), which will never need to trigger on-chain\ntransactions, yet aren't counted towards the TPS derivation. Nevertheless,\nmodels like this are useful to get a feel for the limits of the system in\nthe abstract.\n\n## Multiparty Channels & Credit Channels\n\nThe research then identifies two primitives that can help to reduce the\namount of infeasible payments: multiparty channels, and credit within the\nnetwork.\n\nMultiparty channels aggregate several users in the channel graph,\neffectively forming new fully connected subgraphs. The intuition here is\nthat: if you hold the amt of coins each party adds to a channel as constant,\nthen by increasing the number of parties you also include the max amount\nthat any given user can own. By increasing the max amount that any user can\nown, you reduce the amount of feasible payments due to balance/capacity\nconstraints.\n\nTurning now to credit, the idea is also simple: if a payment is infeasible,\nthen along some hop, credit can be introduced to permanently or temporarily\nexpand the capacity within a channel, crediting one party with increased\nbalance. To minimize systemic risk, such credit likely shouldn't be\nintroduced in the core of the network, instead only existing at the edges.\nProtocols such as Taproot Assets can be used in theory to increase payment\nfeasibility while also reducing on boarding costs as they enable users to\nnatively express the concept of addressable/verifiable credit in channels. \n\n# Last Mile Mobile On Boarding\n\nTo close things out, we had two distinct, but related sessions, focused on\nself-custodial mobile onboarding and UX. First, the Last Mile Problem as it\npertains to mobile UX onboarding [17].\n\nToday in LN, most of the UX challenges arise when a user attempts to pay\nanother user, but the receiver is using a self-custodial mobile wallet. This\nis similar to the last mile transportation problem when it comes to Internet\ninfrastructure and bandwidth: the internal of the network contains \"fat\npipes\" with high bandwidth to quickly shuffle information around the\ninternal network. However, getting from the internal network to the final\ndestination is more costly, less reliable, and slower. \n\n## Onboarding Costs & Channel Liquidity \n\nIn the LN domain, rather than dealing with aging infrastructure or high\nconstruction costs, the challenges are instead related to aspects of the\nmobile platform itself. Compared to always-online routing nodes, mobile\nnodes need to wake up to sign for new incoming updates. Additionally, if a\nmobile node wants to be set up to be a primarily net receiver (no coins yet,\non boarding directly onto LN) then an existing routing node must commit some\nliquidity towards them. The establishment of this first channel is a capital\nsync on the part of the routing node, as it's possible the mobile node\nchurns out of the network, leaving an idle channel with funds suspended. To\nregain the funds in the face of a persistently offline user, the routing\nnode then needs to force close, costing chain fees as well as time while the\nrelative time locks expire (up 2 weeks of delay).\n\nAs we dive into the last-mile liquidity costs, we begin to run into some\nfundamental limits of unit economics. If a user is receiving just 10 sats\nover the channel, and it would cost 1000 sats in chain fees to open the\nchannel, then creating connectivity for that user would be a net loss for\nthe routing node (not to mention min channel limits on the network today).\nAny inbound that a routing node allocates to a low activity user could\ninstead be allocated to a higher velocity of corridor of the networking,\nwherein the channel can earn fees to offset the chain allocation costs.\nAssuming the costs are aligned, or subsidies are in place, then\ninfrastructure tools such as: Phoenix Wallet's JIT channel system, Liquidity\nAds [18], Sidecar Channels w/ Lightning Pool, Amboss' Magma, etc, etc.\n\n## Protocol Induced UX Concerns\n\nInteractivity and chain fee on boarding costs aside, the current protocol\ndesign has some abstraction leaks that end up bubbling up into end user\nmobile wallets. One example is the reserve: to ensure that both parties have\nsome skin-in-the-game at all times (deterrence against breach attempts) they\nmust maintain a minimum balance in the channel at all times (commonly ~1%).\nThis confuses users, as they commonly want to send all funds away from their\nwallet to migrate (or otherwise), but instead find they always need to keep\na small amount of funds at all times. Tangentially, as fees rise, then the\nsize of the economically viable channel also rises along with it.\n\n## Liquidity Fee Rebates\n\nOne solution for the dust/small amount problem that has popped up recently\nis the concept of \"fee rebates\" used by phoenixd [21]. A fee rebate is a\nnon-refundable payment towards future inbound channel liquidity. Each time a\nuser receives funds via a special routing node (one that supports this\nprotocol extension), while the user doesn't yet have a channel, received\nfunds go into this fee rebate bucket. Once the user has enough funds in this\nbucket, then the routing node will open a channel towards it, paying the\nservice and chain fees out of the fee rebate bucket. The min amount needed\nto contribute towards channel opening will vary based on the current service\nand chain fees.\n\nFrom a practical perspective, fee rebates work pretty well. Assuming a user\nis eventually able to receive enough, then they can instantly receive funds\nwithout needing to pause for channel opening. Once they have enough funds to\nwarrant an L1 UTXO, then they pay for the creation of that UTXO from their\nfee rebate bucket. This technique can be combined with systems like ecash\n(pending amount represented in a mint), or even credit channels using\nTaproot Assets as mentioned above (asset UTXO represented in a Pocket\nUniverse to defer L1 costs).\n\nFrom here, the conversation turned back to various off-chain channel factory\nlike constructions, and their limits when it comes to a certain distribution\nof chain fees, number of users, and the balance distribution of those users.\nBasically if you imagine some sort of construct, either based on timeout\ntrees, then if there're 100 million users and each user has 1 satoshi it,\nthen it isn't economically feasible for them to unroll the entire thing on\nchain (fees >\n1 sat). If we assume there's a built in mechanism for users to move funds\nelsewhere so the coordinator can reclaim the funds (similar pattern for Ark,\netc), then if the users don't exit in time, the 1 BTC is forfeited to the\ncoordinator. All users trying to go on chain is paramount to just burning\nthe entire amount, so some attendees theorized a \"big red button\" that can\nbe used to burn all the existing balances. Ideally burning would require\nsome sort of Script (or client side) verifiable proof that the coordinator\nwas about to cheat somehow.\n\nWhile the above scenario is more or less just a thought experiment, I think\nit teases at some of the fundamental limitations when it comes to chain\nfees, and small L1 UTXOs. Nothing terribly new though, this interaction is\nwhy most full nodes by default will recognize the concept of dust: if it\ncosts more than 1/3 of the UTXO balance to pay for fees to move the UTXO,\nthen it's uneconomical. The same applies for off-chain systems, with only\nsome sort of subsidy or exogenous value system as an escape hatch. Any\ntransfers that are uneconomical on the base chain, or the next higher layer,\nwill inevitably migrate to some other system that retains the BTC unit of\naccount, but trades off cheap fees for security.\n\n# BOLT 12: What\u2019s Next\n\nAmidst the haze, savory flavors, and cold beverages of all you can eat+drink\nSukiyaki the BOLT 12 PR was merged into the spec repo! Earlier in the day,\nas one of the last sessions of the summit, we had a session focused on\nwhat's next after BOLT 12, namely what extensions that were cut from the\noriginal version are people interested in pursuing. \n\n## Potential BOLT 12 Extensions\n\nThe first extension discussed was: invoice replacement. Consider a case\nwhere a user fetches an invoice using an Offer, but waits too long before\npaying, so all the blinded paths and/or invoice itself are expired. In this\nscenario, it'd be useful for the user to be able to ask for a replacement\ninvoice. Exactly how this differs from just fetching another fresh invoice\nusing the Offer is perhaps a contextual question.\n\nOne area that some of the implementers seemed most poised to dust off again\nis: recurrent payments. Portions of recurrence were part of the original\nspec, but were eventually ripped out to slim things down some. Relevant\nrecurrence params include: the time interval, the payment window, limit, and\nthen start+end period. A neat trick that the receiver can use is utilizing a\nhash chain to minimize the amount of preimage storage they need. If they can\nsend the sender a special salt/seed (during initial negotiation), then only\nthe sender+receiver would be aware that the preimages nicely arrange into a\nhash chain.\n\nOn the topic of authentication, a notion of the sort of reverse version of\nBIP 353 was brought up. The general idea is to give users the ability to bind a\nnode's public key to a domain name. This would serve to authenticate that\nnode Y is actually associated with some service/domain/company.\n\n## Onion Message Rate Limiting & Back Pressure Propagation\n\nAt the tail end of the session, the focus shifted to onion messaging, and\nthe current state of implementation/behavior across the major\nimplementations. One topic raised was how wallets are handling the fallback\nand related UX implications if/when a wallet _fails_ to fetch an Offer.\nOnion messaging is an unreliable, best-effort forwarding network w/o any\nbuilt-in acknowledgements, so it's possible that a message is just never\ndelivered. As a result, wallets need to be ready to either try another\nroute, retransmit the message, or fallback to some other mechanism if/when a\nwallet fails to fetch an invoice using an Offer.\n\nGenerally the current state of things is that either a single hop onion\nmessaging route it used, or a direct connection. A direct connection refers\nto connection over the p2p network to either the receiver, the introduction\npoint, or nodes leading up to the introduction point in an attempt to send a\nmessage that travels over a shorter path). If _that_ attempt fails (no nodes\nlistening, receiver not offline, etc) then wallets either need some other\nfallback, or may attempt to send some sort of spontaneous payment.\n\nReturning back to message delivery, it's clear that some sort of rate\nlimiting is needed. Nodes may start with some sort of free budget, but will\nneed to throttle messaging as otherwise tens a node could mindlessly forward\n10s of GBs of free onion messaging traffic (IMO, it's inevitable that after\na free tier, most nodes will end up switching to a bandwidth metered payment\nsystem for onion messaging [22]). Therefore nodes will need to adopt some\nsort of bandwidth and rate limiting. If the network is significantly over\nprovisioned relative to the typical messaging usage, then service will\nremain relatively high, as nothing approaches the configured bandwidth\nlimits. However if the network is under provisioned relative to messaging\nactivity (people are trying to livestream their gaming sessions or w/e),\nthen service is hampered, as most message attempts fail due to a tragedy of\nthe commons). As is, the difference between a message being dropped, not\ndelivered, or an offline receiver are all indistinguishable from each other,\ncreating further UX challenges.\n\nEventually discussion turned to the old back pressure rate limiting\nalgorithm previously proposed on the mailing list [23].  While this\nalgorithm, nodes can maintain a relatively compact description of the set of\npeers that had sent them message last. Once a peer exceeds a limit, then an\n`onion_message_drop` message is sent to the sender. The sender then attempts\nto trace back who sent the message to itself, further propagating the\n`onion_message_drop` message backwards, halving the rate limit in the\nprocess. If the sender doesn't overflow the rate limit within a 30 second\ninterval, then the receiver should double their rate limit until it reaches\nthe normal rate limit.\n\nThere're some open questions lingering here such as: How can nodes make sure\nthey are attributing the spam to the proper peer? Is it possible for nodes\nto frame other nodes to cut off their messaging activity? Is there any\nadditional meta data required to properly attribute the source of the spam?\nIs this resilient to a spammer that knows the rate limit and can stay right\nunder it, while maximizing utilized bandwidth? When this scheme was\noriginally brought up, some basic simulations were run [24] to gauge the\nefficacy and resilience of the scheme. Initial results were promising, with\nsome additional research questions posed [25]. \n\nUltimately, some attendees agreed to start to revive the work/research on\nthe back pressure algorithm, applying conservative rate limiting parameters\nin the short term.\n\n-- Laolu\n\n[1]: https://lists.linuxfoundation.org/pipermail/lightning-dev/2018-January/000943.html\n[2]: https://github.com/t-bast/lightning-docs/blob/398a1b78250f564f7c86a414810f7e87e5af23ba/pinning-attacks.md\n[3]: https://github.com/bitcoin/bitcoin/pull/30352\n[4]: https://github.com/bitcoin/bitcoin/pull/28970\n[6]: https://docs.google.com/document/d/1Topu844KUUnrBED4VaJE0lVnk9_mV6UZSz456slMO8k/edit\n[5]: https://github.com/bitcoin/bitcoin/pull/29306\n[8]: https://gist.github.com/instagibbs/1d02d0251640c250ceea1c66665ec163\n[7]: https://eprint.iacr.org/2018/472\n[8]: https://en.wikipedia.org/wiki/IEEE_802.11_RTS/CTS\n[9]: https://delvingbitcoin.org/t/superscalar-laddered-timeout-tree-structured-decker-wattenhofer-factories/1143?\n[10]: https://tik-old.ee.ethz.ch/file/716b955c130e6c703fac336ea17b1670/duplex-micropayment-channels.pdfkj\n[11]: https://github.com/JohnLaw2/ln-scaling-covenants\n[12]: https://eprint.iacr.org/2024/025\n[13]: https://github.com/renepickhardt/Lightning-Network-Limitations/blob/305db330c96dc751f0615d9abb096b12b8a6191f/Limits%20of%20two%20party%20channels/paper/a%20mathematical%20theory%20of%20payment%20channel%20networks.pdf\n[15]: https://github.com/lightningnetwork/lnd/pull/8256\n[16]: https://github.com/lightningnetwork/lnd/pull/5987\n[17]: https://bitcoinmagazine.com/technical/assessing-the-lightning-networks-last-mile-solutions\n[18]: https://github.com/lightning/bolts/pull/1153\n[19]: https://lightning.engineering/posts/2021-05-26-sidecar-channels/\n[20]: https://amboss.tech/docs/magma/intro\n[21]: https://phoenix.acinq.co/server\n[22]: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-February/03498.html\n[23]: https://gist.github.com/t-bast/e37ee9249d9825e51d260335c94f0fcf\n[24]: https://gist.github.com/joostjager/bca727bdd4fc806e4c0050e12838ffa3\n[25]: https://lists.linuxfoundation.org/pipermail/lightning-dev/2022-July/003663.html",
  "actions_summary": [
    {
      "id": 2,
      "count": 7
    }
  ],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 143,
  "hidden": false,
  "trust_level": 2,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "Hi y\u2019all, \n \n~3 weeks ago, 30+ Lightning developers and researchers gathered in Tokyo,\nJapan for three days to discuss a number of matters related to the current\nstate and future evolution of the Lightning protocol (and where relevant,\nthe Bitcoin p2p and consensus protocol). \nThe last such gatherin&hellip;",
  "truncated": true,
  "post_url": "/t/ln-summit-2024-notes-summary-commentary/1198/1",
  "reactions": [
    {
      "id": "heart",
      "type": "emoji",
      "count": 5
    },
    {
      "id": "+1",
      "type": "emoji",
      "count": 3
    }
  ],
  "current_user_reaction": null,
  "reaction_users_count": 7,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null,
  "can_vote": false
}