{
  "id": 172,
  "name": "James O'Beirne",
  "username": "jamesob",
  "avatar_template": "/letter_avatar_proxy/v4/letter/j/958977/{size}.png",
  "created_at": "2023-09-05T17:42:56.174Z",
  "cooked": "<p>It would be great to have a public archive of the posts here, so that</p>\n<ol>\n<li>if this site goes away, we still have all the content, and</li>\n<li>it is searchable by indexers like <a href=\"https://bitcoinsearch.xyz\">https://bitcoinsearch.xyz</a>.</li>\n</ol>\n<p>As far as I can tell, there are three approaches:</p>\n<h2><a name=\"scraping-1\" class=\"anchor\" href=\"#scraping-1\"></a>Scraping</h2>\n<p>Use some kind of crawler (selenium, wget, etc.) to manually create a backup by scraping the site.</p>\n<ul>\n<li>I think this is not great given how JavaScript heavy Discourse is (i.e. it loads posts incrementally based on cursor position). It\u2019s also brittle - scraping code is often complicated.</li>\n<li>\u2026but, it can be done by anyone, with no permissions necessary.</li>\n</ul>\n<h2><a name=\"api-2\" class=\"anchor\" href=\"#api-2\"></a>API</h2>\n<p>Use the <a href=\"https://docs.discourse.org/\">Discourse API</a> to pull content. This could be done either periodically as a full crawl or incrementally on a continuous basis.</p>\n<ul>\n<li>I think this is a fine way to go assuming the API offers everything we need and someone can provision API credentials.</li>\n<li>We may be able to get everything we need out of the <a href=\"https://docs.discourse.org/#tag/Posts/operation/listPosts\">list posts endpoint</a>.</li>\n</ul>\n<h2><a name=\"db-dump-3\" class=\"anchor\" href=\"#db-dump-3\"></a>DB dump</h2>\n<p>Get a SQL dump of the database and run some kind of sanitization/export script on it every so often.</p>\n<ul>\n<li>The upside: easy to be complete and the schema is probably very stable.</li>\n<li>The downside: this can only be performed by administrators.</li>\n</ul>\n<hr>\n<p>I\u2019d say we should either go with API or DB, and then post the results on a git repo somewhere public (preferably a few places!).</p>\n<p>Thoughts from the admins?</p>",
  "post_number": 1,
  "post_type": 1,
  "updated_at": "2023-09-05T17:42:56.174Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 7,
  "reads": 8,
  "readers_count": 7,
  "score": 36.6,
  "yours": false,
  "topic_id": 87,
  "topic_slug": "public-archive-for-delving-bitcoin",
  "topic_title": "Public archive for Delving Bitcoin",
  "topic_html_title": "Public archive for Delving Bitcoin",
  "category_id": 2,
  "display_username": "James O'Beirne",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "It would be great to have a public archive of the posts here, so that\n1. if this site goes away, we still have all the content, and\n1. it is searchable by indexers like https://bitcoinsearch.xyz.\n\nAs far as I can tell, there are three approaches:\n\n## Scraping\n\nUse some kind of crawler (selenium, wget, etc.) to manually create a backup by scraping the site.\n\n- I think this is not great given how JavaScript heavy Discourse is (i.e. it loads posts incrementally based on cursor position). It's also brittle - scraping code is often complicated.\n - ...but, it can be done by anyone, with no permissions necessary.\n\n## API \n\nUse the [Discourse API](https://docs.discourse.org/) to pull content. This could be done either periodically as a full crawl or incrementally on a continuous basis.\n- I think this is a fine way to go assuming the API offers everything we need and someone can provision API credentials.\n - We may be able to get everything we need out of the [list posts endpoint](https://docs.discourse.org/#tag/Posts/operation/listPosts).\n\n## DB dump\n\nGet a SQL dump of the database and run some kind of sanitization/export script on it every so often.\n- The upside: easy to be complete and the schema is probably very stable.\n- The downside: this can only be performed by administrators.\n\n---\n\nI'd say we should either go with API or DB, and then post the results on a git repo somewhere public (preferably a few places!).\n\nThoughts from the admins?",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 9,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false
}