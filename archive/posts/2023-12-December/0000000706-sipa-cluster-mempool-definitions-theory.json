{
  "id": 706,
  "name": "Pieter Wuille",
  "username": "sipa",
  "avatar_template": "/user_avatar/delvingbitcoin.org/sipa/{size}/102_2.png",
  "created_at": "2023-12-10T23:42:27.306Z",
  "cooked": "<aside class=\"quote no-group\" data-username=\"ajtowns\" data-post=\"4\" data-topic=\"202\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/ajtowns/48/3_2.png\" class=\"avatar\"> ajtowns:</div>\n<blockquote>\n<p>Isn\u2019t this just about preferring (a) multiple smaller chunks to a single larger chunk, and (b) smaller chunks to be linearised before larger chunks, given the chunks have equal fee rates?</p>\n</blockquote>\n</aside>\n<p>I believe that is too strong - while we do want to consider breaking up chunks into multiple equal-feerate ones an improvement, I don\u2019t think there is an objective reason to want the smallest ones first. (in my implementation, I do prefer smaller size when comparing equal-feerate transaction sets, but that\u2019s an arbitrary choice that happens to prevent merging of such equal-feerate chunks).</p>\n<p>One possibility, which I think is sufficient, is in the <span class=\"math\">\\gtrsim</span> (&amp; co) definitions is to use the total number of chunks as a tie-breaker when the fee-size diagram coincides exactly (more chunks = better). This has the advantage that \u201coptimal linearization\u201d automatically implies chunks which have no non-trivial subsets of the same feerate, and it doesn\u2019t introduce a new source of incomparability. The disadvantage is that (as stated) it breaks the gathering theorem, and I suspect fixing that would complicate matters somewhat. That may also have implications for the prefix-merging algorithm\u2019s proof (I believe the algorithm itself actually keeps its properties under this stronger ordering definition).</p>\n<p>An alternative is leaving the preorder on linearizations alone, and instead only incorporate into the \u201coptimal linearization\u201d definition that chunks cannot have non-trivial subsets of higher or equal feerate. Alternatively, it could be a notion of \u201cperfect linearization\u201d which is stronger than optimal (and leave optimal to only be about the feerate diagram). This may be nicer in terms of presentation as the properties can be introduced and proven incrementally.</p>",
  "post_number": 5,
  "post_type": 1,
  "updated_at": "2023-12-10T23:42:27.306Z",
  "reply_count": 0,
  "reply_to_post_number": 4,
  "quote_count": 1,
  "incoming_link_count": 0,
  "reads": 3,
  "readers_count": 2,
  "score": 0.6,
  "yours": false,
  "topic_id": 202,
  "topic_slug": "cluster-mempool-definitions-theory",
  "topic_title": "Cluster mempool definitions & theory",
  "topic_html_title": "Cluster mempool definitions &amp; theory",
  "category_id": 9,
  "display_username": "Pieter Wuille",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "[quote=\"ajtowns, post:4, topic:202\"]\nIsn\u2019t this just about preferring (a) multiple smaller chunks to a single larger chunk, and (b) smaller chunks to be linearised before larger chunks, given the chunks have equal fee rates?\n[/quote]\n\nI believe that is too strong - while we do want to consider breaking up chunks into multiple equal-feerate ones an improvement, I don't think there is an objective reason to want the smallest ones first. (in my implementation, I do prefer smaller size when comparing equal-feerate transaction sets, but that's an arbitrary choice that happens to prevent merging of such equal-feerate chunks).\n\nOne possibility, which I think is sufficient, is in the $\\gtrsim$ (& co) definitions is to use the total number of chunks as a tie-breaker when the fee-size diagram coincides exactly (more chunks = better). This has the advantage that \"optimal linearization\" automatically implies chunks which have no non-trivial subsets of the same feerate, and it doesn't introduce a new source of incomparability. The disadvantage is that (as stated) it breaks the gathering theorem, and I suspect fixing that would complicate matters somewhat. That may also have implications for the prefix-merging algorithm's proof (I believe the algorithm itself actually keeps its properties under this stronger ordering definition).\n\nAn alternative is leaving the preorder on linearizations alone, and instead only incorporate into the \"optimal linearization\" definition that chunks cannot have non-trivial subsets of higher or equal feerate. Alternatively, it could be a notion of \"perfect linearization\" which is stronger than optimal (and leave optimal to only be about the feerate diagram). This may be nicer in terms of presentation as the properties can be introduced and proven incrementally.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 96,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false
}