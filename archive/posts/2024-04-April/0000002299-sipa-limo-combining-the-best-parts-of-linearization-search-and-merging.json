{
  "id": 2299,
  "name": "Pieter Wuille",
  "username": "sipa",
  "avatar_template": "/user_avatar/delvingbitcoin.org/sipa/{size}/102_2.png",
  "created_at": "2024-04-23T02:43:58.324Z",
  "cooked": "<h1><a name=\"limo-linearization-through-incremental-merging-of-optimizations-1\" class=\"anchor\" href=\"#limo-linearization-through-incremental-merging-of-optimizations-1\"></a>LIMO: Linearization through Incremental Merging of Optimizations</h1>\n<h3><a name=\"h-1-introduction-2\" class=\"anchor\" href=\"#h-1-introduction-2\"></a>1. Introduction</h3>\n<p>Consider the linearization algorithm as suggested in <a href=\"https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303\">How To Linearize Your Cluster</a>, excluding the approach from Section 1.2 (Bottleneck Splitting). In broad lines:</p>\n<ul>\n<li>While unlinearized transactions remain in the cluster:\n<ul>\n<li>Use a computationally-bounded search algorithm which pre-splits on the best remaining ancestor set to find a good topologically valid set of transactions in what remains of the cluster.</li>\n<li>Output the transactions of that topologically valid set as the next linearized transactions.</li>\n<li>Remove the set from the cluster and repeat.</li>\n</ul>\n</li>\n</ul>\n<p>One might expect that this algorithm will always produce a linearization which is (by the convex-hull <a href=\"https://delvingbitcoin.org/t/cluster-mempool-definitions-theory/202#linearizations-and-chunks-3\">feerate diagram</a> metric) at least as good as straight up picking the best ancestor sets of what remains, as this is just additionally doing search on top. It turns out this is <strong>not the case</strong>. In fact, it may (rarely) be strictly worse.</p>\n<p>Consider the following example cluster:</p>\n<pre data-code-height=\"234\" data-code-wrap=\"mermaid\"><code class=\"lang-mermaid\">graph BT\n  T0[\"B: 11\"];\n  T1[\"C: 7\"];\n  T2[\"D: 10\"];\n  T3[\"E: 7\"];\n  T4[\"A: 1\"];\n  T0 --&gt; T4;\n  T2 --&gt; T1 --&gt; T4;\n  T3 --&gt; T4;\n</code></pre>\n<p><strong>Ancestor-set based linearization.</strong> The consecutive remaining best ancestor sets are AB (6), CD (8.5), and E (7), and the resulting [A,B,C,D,E] linearization is in fact optimal, chunked as [ABCD (7.25), E (7)].</p>\n<p><strong>Computationally-bounded search.</strong> However, ACDE (6.25) has higher feerate than AB (but worse than ABCD), and thus a (very) bounded search might end up with ACDE as first set to include. The resulting [A,C,D,E,B] linearization, chunked as [ACDEB (7.2)], is not optimal, and strictly worse than [A,B,C,D,E].</p>\n<p>It is not a very satisfactory situation that an algorithm that performs a strict superset of the work can end up with a worse solution.</p>\n<h3><a name=\"h-2-incremental-merging-3\" class=\"anchor\" href=\"#h-2-incremental-merging-3\"></a>2. Incremental merging</h3>\n<p>Of course, we have a good algorithm for combining the best parts of two linearizations: <a href=\"https://delvingbitcoin.org/t/merging-incomparable-linearizations/209\">merging</a>.</p>\n<p>A possibility involving this is to compute two linearizations, one using just ancestor sets, and one using bounded search, and then merging the two. But that has downsides too; either:</p>\n<ul>\n<li>We only do the optimal ancestor set finding once, as part of the ancestor linearization, but then the bounded search cannot take \u201cadvantage\u201d of this information, as it\u2019s only available during the merge at the very end.</li>\n<li>We perform the search for optimal ancestor sets twice (once inside the ancestor linearization, and once inside the pre-splitting during search), meaning duplicate work.</li>\n</ul>\n<p>Overall, it feels like using merging to address this comes \u201ctoo late\u201d. Ideally, we incorporate the findings of ancestor sort as input into the search. This can be accomplished by turning the overall linearization algorithm into a improvement algorithm:</p>\n<ul>\n<li>Start with an initial linearization <span class=\"math\">L</span>, e.g. the ancestor-based linearization.</li>\n<li>While transactions remain in <span class=\"math\">L</span>:\n<ul>\n<li>Use bounded search to find a high-feerate topologically valid subset <span class=\"math\">S</span> of what remains of <span class=\"math\">L</span>.</li>\n<li>Perform an optimization step that reorders <span class=\"math\">L</span> without worsening it, and such that the initial part of its diagram is at least as good as the diagram of <span class=\"math\">L[S]</span>.</li>\n<li>Output the highest-feerate prefix of <span class=\"math\">L</span> and continue with what remains.</li>\n</ul>\n</li>\n</ul>\n<p>The optimization step can be implemented as <span class=\"math\">\\operatorname{merge}(L, L[S] + L[G \\setminus S])</span>, i.e., a merge of <span class=\"math\">L</span> with a version of itself that has <span class=\"math\">S</span> moved to its front.</p>\n<p>This is a strict improvement over the existing linearization algorithm: in addition to guaranteeing a result that is as good as the combinations of prefixes of found subsets, it also guarantees a result that is as good as the initial linearization. And contrary to the merge-at-the-end strategy, the subset searches get to take advantage of the quality of the initial linearization too, as it affects what remains in <span class=\"math\">L</span> (and more, see below).</p>\n<h3><a name=\"h-3-single-set-improvement-steps-4\" class=\"anchor\" href=\"#h-3-single-set-improvement-steps-4\"></a>3. Single-set improvement steps</h3>\n<p>The approach above requires performing a <span class=\"math\">\\operatorname{merge}</span> operation for every search step, which can be up to cubic in complexity, as <span class=\"math\">\\operatorname{merge}</span> runs in <span class=\"math\">\\mathcal{O}(n^2)</span> time, and we may need to run it up to <span class=\"math\">n</span> times. This would make the overall operation potentially significantly slower than just merging once at the end.</p>\n<p>To address that, observe that the merging algorithm itself works by incrementally moving high-feerate subsets to the front. If instead of performing a full merge in every step, we just determine what the first to-be-moved subset would be for the resulting merge, and output that before continuing with what remains of the linearization, we are back to quadratic complexity. The effective resulting LIMO algorithm is:</p>\n<ul>\n<li>Given an initial linearization <span class=\"math\">L</span>:\n<ul>\n<li>While there are transactions left in <span class=\"math\">L</span>:\n<ul>\n<li>Let <span class=\"math\">(l_1, \\ldots) = \\operatorname{chunks}(L)</span>, so <span class=\"math\">l_1</span> is the highest-feerate prefix of <span class=\"math\">L</span>.</li>\n<li>Find a high-feerate topologically-valid subset <span class=\"math\">S</span> of the transactions in <span class=\"math\">L</span> (search).</li>\n<li>Let <span class=\"math\">(s_1, \\ldots) = \\operatorname{chunks}(L[S])</span>, so <span class=\"math\">s_1</span> is the highest-feerate prefix of <span class=\"math\">L[S]</span>.</li>\n<li>Let <span class=\"math\">b</span> be the highest-feerate set among:\n<ul>\n<li><span class=\"math\">s_1</span></li>\n<li>All prefixes of <span class=\"math\">L[c_1 \\cap s_1] + L[c_1 \\setminus s_1]</span></li>\n</ul>\n</li>\n<li>Output <span class=\"math\">L[b]</span></li>\n<li>Remove <span class=\"math\">b</span> from <span class=\"math\">L</span> and repeat.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>As long as the consecutive <span class=\"math\">S</span> sets do not degrade in quality, the resulting linearization will be as good as all its combined prefixes.</p>\n<p>For every search step an initial guess <span class=\"math\">l_1</span> is known, the highest-feerate prefix of what remains of the initial linearization. This <span class=\"math\">l_1</span> can be used as the initial <span class=\"math\">\\operatorname{best}</span> inside the <a href=\"https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303\">search algorithm</a> (instead of <span class=\"math\">\\varnothing</span>), which allows earlier pruning of work queue items whose <span class=\"math\">\\operatorname{pot}</span> isn\u2019t better, and can reduce the initial size of <span class=\"math\">\\operatorname{imp}</span>.</p>\n<h3><a name=\"h-4-improving-existing-linearizations-5\" class=\"anchor\" href=\"#h-4-improving-existing-linearizations-5\"></a>4. Improving existing linearizations</h3>\n<p>So far, we have considered LIMO as a replacement for a \u201ccold-start\u201d linearizations, for clusters which do not have a linearization already, or for merging as a linearization retry with an existing one. It could however also be used to improve existing linearizations, by passing in that existing linearization as initial <span class=\"math\">L</span>, rather than an ancestor-based linearization.</p>\n<p>In that setting, it would be useful if the algorithm could merge in multiple distinct <span class=\"math\">S</span> sets in every iteration, e.g. one found through ancestor-set linearization and one through search, effectively moving the ancestor-set logic into the algorithm itself rather than using it as an input. Unfortunately, this appears complicated to do with strong guarantees. The difficulty lies in the fact that the first set of the outcome of two merging steps requires more than just the first set of that first merging outcome.</p>",
  "post_number": 1,
  "post_type": 1,
  "updated_at": "2024-04-23T02:48:28.999Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 1,
  "reads": 5,
  "readers_count": 4,
  "score": 6.0,
  "yours": false,
  "topic_id": 825,
  "topic_slug": "limo-combining-the-best-parts-of-linearization-search-and-merging",
  "topic_title": "LIMO: combining the best parts of linearization search and merging",
  "topic_html_title": "LIMO: combining the best parts of linearization search and merging",
  "category_id": 8,
  "display_username": "Pieter Wuille",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "# LIMO: Linearization through Incremental Merging of Optimizations\n\n### 1. Introduction\n\nConsider the linearization algorithm as suggested in [How To Linearize Your Cluster](https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303), excluding the approach from Section 1.2 (Bottleneck Splitting). In broad lines:\n* While unlinearized transactions remain in the cluster:\n  * Use a computationally-bounded search algorithm which pre-splits on the best remaining ancestor set to find a good topologically valid set of transactions in what remains of the cluster.\n  * Output the transactions of that topologically valid set as the next linearized transactions.\n  * Remove the set from the cluster and repeat.\n\nOne might expect that this algorithm will always produce a linearization which is (by the convex-hull [feerate diagram](https://delvingbitcoin.org/t/cluster-mempool-definitions-theory/202#linearizations-and-chunks-3) metric) at least as good as straight up picking the best ancestor sets of what remains, as this is just additionally doing search on top. It turns out this is **not the case**. In fact, it may (rarely) be strictly worse.\n\nConsider the following example cluster:\n\n```mermaid height=234,auto\ngraph BT\n  T0[\"B: 11\"];\n  T1[\"C: 7\"];\n  T2[\"D: 10\"];\n  T3[\"E: 7\"];\n  T4[\"A: 1\"];\n  T0 --> T4;\n  T2 --> T1 --> T4;\n  T3 --> T4;\n```\n\n**Ancestor-set based linearization.** The consecutive remaining best ancestor sets are AB (6), CD (8.5), and E (7), and the resulting [A,B,C,D,E] linearization is in fact optimal, chunked as [ABCD (7.25), E (7)].\n\n**Computationally-bounded search.** However, ACDE (6.25) has higher feerate than AB (but worse than ABCD), and thus a (very) bounded search might end up with ACDE as first set to include. The resulting [A,C,D,E,B] linearization, chunked as [ACDEB (7.2)], is not optimal, and strictly worse than [A,B,C,D,E].\n\nIt is not a very satisfactory situation that an algorithm that performs a strict superset of the work can end up with a worse solution.\n\n### 2. Incremental merging\n\nOf course, we have a good algorithm for combining the best parts of two linearizations: [merging](https://delvingbitcoin.org/t/merging-incomparable-linearizations/209).\n\nA possibility involving this is to compute two linearizations, one using just ancestor sets, and one using bounded search, and then merging the two. But that has downsides too; either:\n* We only do the optimal ancestor set finding once, as part of the ancestor linearization, but then the bounded search cannot take \"advantage\" of this information, as it's only available during the merge at the very end.\n* We perform the search for optimal ancestor sets twice (once inside the ancestor linearization, and once inside the pre-splitting during search), meaning duplicate work.\n\nOverall, it feels like using merging to address this comes \"too late\". Ideally, we incorporate the findings of ancestor sort as input into the search. This can be accomplished by turning the overall linearization algorithm into a improvement algorithm:\n* Start with an initial linearization $L$, e.g. the ancestor-based linearization.\n* While transactions remain in $L$:\n  * Use bounded search to find a high-feerate topologically valid subset $S$ of what remains of $L$.\n  * Perform an optimization step that reorders $L$ without worsening it, and such that the initial part of its diagram is at least as good as the diagram of $L[S]$.\n  * Output the highest-feerate prefix of $L$ and continue with what remains.\n\nThe optimization step can be implemented as $\\operatorname{merge}(L, L[S] + L[G \\setminus S])$, i.e., a merge of $L$ with a version of itself that has $S$ moved to its front.\n\nThis is a strict improvement over the existing linearization algorithm: in addition to guaranteeing a result that is as good as the combinations of prefixes of found subsets, it also guarantees a result that is as good as the initial linearization. And contrary to the merge-at-the-end strategy, the subset searches get to take advantage of the quality of the initial linearization too, as it affects what remains in $L$ (and more, see below).\n\n### 3. Single-set improvement steps\n\nThe approach above requires performing a $\\operatorname{merge}$ operation for every search step, which can be up to cubic in complexity, as $\\operatorname{merge}$ runs in $\\mathcal{O}(n^2)$ time, and we may need to run it up to $n$ times. This would make the overall operation potentially significantly slower than just merging once at the end.\n\nTo address that, observe that the merging algorithm itself works by incrementally moving high-feerate subsets to the front. If instead of performing a full merge in every step, we just determine what the first to-be-moved subset would be for the resulting merge, and output that before continuing with what remains of the linearization, we are back to quadratic complexity. The effective resulting LIMO algorithm is:\n\n* Given an initial linearization $L$:\n  * While there are transactions left in $L$:\n    * Let $(l_1, \\ldots) = \\operatorname{chunks}(L)$, so $l_1$ is the highest-feerate prefix of $L$.\n    * Find a high-feerate topologically-valid subset $S$ of the transactions in $L$ (search).\n    * Let $(s_1, \\ldots) = \\operatorname{chunks}(L[S])$, so $s_1$ is the highest-feerate prefix of $L[S]$.\n    * Let $b$ be the highest-feerate set among:\n      * $s_1$\n      * All prefixes of $L[c_1 \\cap s_1] + L[c_1 \\setminus s_1]$\n    * Output $L[b]$\n    * Remove $b$ from $L$ and repeat.\n\nAs long as the consecutive $S$ sets do not degrade in quality, the resulting linearization will be as good as all its combined prefixes.\n\nFor every search step an initial guess $l_1$ is known, the highest-feerate prefix of what remains of the initial linearization. This $l_1$ can be used as the initial $\\operatorname{best}$ inside the [search algorithm](https://delvingbitcoin.org/t/how-to-linearize-your-cluster/303) (instead of $\\varnothing$), which allows earlier pruning of work queue items whose $\\operatorname{pot}$ isn't better, and can reduce the initial size of $\\operatorname{imp}$.\n\n### 4. Improving existing linearizations\n\nSo far, we have considered LIMO as a replacement for a \"cold-start\" linearizations, for clusters which do not have a linearization already, or for merging as a linearization retry with an existing one. It could however also be used to improve existing linearizations, by passing in that existing linearization as initial $L$, rather than an ancestor-based linearization.\n\nIn that setting, it would be useful if the algorithm could merge in multiple distinct $S$ sets in every iteration, e.g. one found through ancestor-set linearization and one through search, effectively moving the ancestor-set logic into the algorithm itself rather than using it as an input. Unfortunately, this appears complicated to do with strong guarantees. The difficulty lies in the fact that the first set of the outcome of two merging steps requires more than just the first set of that first merging outcome.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 96,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false
}