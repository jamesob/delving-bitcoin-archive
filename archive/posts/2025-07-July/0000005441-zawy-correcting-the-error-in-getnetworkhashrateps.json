{
  "id": 5441,
  "name": "Zawy",
  "username": "zawy",
  "avatar_template": "/user_avatar/delvingbitcoin.org/zawy/{size}/750_2.png",
  "created_at": "2025-07-11T23:53:05.041Z",
  "cooked": "<p>I want to find the relation between chain_work and the lowest_hash (an N=1 situation).  The problem reminded me of being unable to estimate the expected solvetime (1/\u03bb) from a single solvetime. After playing around with it, it appears they are both based on the exponential distribution:</p>\n<p><span class=\"math\">D = \\frac{2^{256}}{\\text{lowest_hash}}</span></p>\n<p><span class=\"math\">W = \\text{chain_work} = \\lambda</span></p>\n<p><span class=\"math\">\\text{PDF}(1/D) = W e^{-\\frac{W}{D}}</span></p>\n<p>[ edit:\nGrok says I should have written this equivalently as:</p>\n<p><span class=\"math\">\\text{PDF}(\\text{lowest_hash}) = \\frac{W}{2^{256}} e^{-\\frac{W}{2^{256}}}</span></p>\n<p>The histogram of <span class=\"math\">\\frac{W}{D}</span> looks like the exponential PDF, the median  sticks around the expected ln(2), and the mean and StdDev are 1 as expected.</p>\n<p>My experiments didn\u2019t assume a constant hashrate or difficulty, but that the difficulty was correctly adjusted for the hashrate (i.e. the expected solvetimes were the block time).</p>\n<p>Edit:\nGrok insists W = D is a good unbiased estimator for the smallest hash value. This is primarily because if we\u2019re looking at the Bitcoin chain we have only 1 trial.  To support it\u2019s point, it provided the following equation that showed itself more accurate than mine that uses 1/6. It depends on the number of trial runs in my experiment:</p>\n<p>W = 2^256/lowest_hash/(ln(trials) + 0.577)</p>\n<p>It\u2019s almost 2x higher than W = D that Grok insists on for 1 trial. So presumably, this equation applies for at least 2 trials.</p>\n<p>I pointed out that W for the Nth lowest hash is 2^256 * (N-1) / Nth_lowest_hash. It agreed. So I pointed out that for N=1 this equation implies chain work would be 0 which doesn\u2019t seem right, but that my 1/6 factor is more in line with the trend of being a larger and larger correction downward, but that W=D is a sudden reversal of the trend. It disagreed.</p>",
  "post_number": 16,
  "post_type": 1,
  "posts_count": 16,
  "updated_at": "2025-07-12T19:31:00.092Z",
  "reply_count": 1,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 0,
  "reads": 6,
  "readers_count": 5,
  "score": 6.2,
  "yours": false,
  "topic_id": 1745,
  "topic_slug": "correcting-the-error-in-getnetworkhashrateps",
  "topic_title": "Correcting the error in getnetworkhashrateps",
  "topic_html_title": "Correcting the error in getnetworkhashrateps",
  "category_id": 7,
  "display_username": "Zawy",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 11,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "I want to find the relation between chain_work and the lowest_hash (an N=1 situation).  The problem reminded me of being unable to estimate the expected solvetime (1/&lambda;) from a single solvetime. After playing around with it, it appears they are both based on the exponential distribution:\n\n$D = \\frac{2^{256}}{\\text{lowest_hash}}$\n\n$W = \\text{chain_work} = \\lambda$\n\n$\\text{PDF}(1/D) = W e^{-\\frac{W}{D}}$\n\n[ edit: \nGrok says I should have written this equivalently as:\n\n$\\text{PDF}(\\text{lowest_hash}) = \\frac{W}{2^{256}} e^{-\\frac{W}{2^{256}}}$\n\nThe histogram of $\\frac{W}{D}$ looks like the exponential PDF, the median  sticks around the expected ln(2), and the mean and StdDev are 1 as expected. \n\nMy experiments didn't assume a constant hashrate or difficulty, but that the difficulty was correctly adjusted for the hashrate (i.e. the expected solvetimes were the block time). \n\nEdit: \nGrok insists W = D is a good unbiased estimator for the smallest hash value. This is primarily because if we're looking at the Bitcoin chain we have only 1 trial.  To support it's point, it provided the following equation that showed itself more accurate than mine that uses 1/6. It depends on the number of trial runs in my experiment:\n\nW = 2^256/lowest_hash/(ln(trials) + 0.577)\n\nIt's almost 2x higher than W = D that Grok insists on for 1 trial. So presumably, this equation applies for at least 2 trials.\n\nI pointed out that W for the Nth lowest hash is 2^256 * (N-1) / Nth_lowest_hash. It agreed. So I pointed out that for N=1 this equation implies chain work would be 0 which doesn't seem right, but that my 1/6 factor is more in line with the trend of being a larger and larger correction downward, but that W=D is a sudden reversal of the trend. It disagreed.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 502,
  "hidden": false,
  "trust_level": 1,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "I want to find the relation between chain_work and the lowest_hash (an N=1 situation).  The problem reminded me of being unable to estimate the expected solvetime (1/\u03bb) from a single solvetime. After playing around with it, it appears they are both based on the exponential distribution: \nD = \\frac{2&hellip;",
  "truncated": true,
  "post_url": "/t/correcting-the-error-in-getnetworkhashrateps/1745/16",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false
}