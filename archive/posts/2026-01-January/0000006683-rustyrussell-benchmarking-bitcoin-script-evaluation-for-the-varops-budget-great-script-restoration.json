{
  "id": 6683,
  "name": "Rusty Russell",
  "username": "rustyrussell",
  "avatar_template": "/user_avatar/delvingbitcoin.org/rustyrussell/{size}/154_2.png",
  "created_at": "2026-01-26T03:52:44.709Z",
  "cooked": "<aside class=\"quote no-group\" data-username=\"ajtowns\" data-post=\"14\" data-topic=\"2094\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/ajtowns/48/417_2.png\" class=\"avatar\"> ajtowns:</div>\n<blockquote>\n<p>That might be reasonable, but I don\u2019t think it\u2019s good enough: there\u2019s a difference between \u201cyou can waste X resources doing this silly pattern that serves no purpose except as an attack\u201d and \u201cyou can now waste X resources with arbitrary logic\u201d. In the former case, you\u2019re only vulnerable to deliberate attackers; the latter case you\u2019re vulnerable to everyone who comes up with new logic they want to deploy. It was easy to make 4MB blocks as soon as segwit was active; but until inscriptions came along, there was no real reason to do that. We don\u2019t want to have GSR result in a similar state change from \u201cit\u2019s possible to make slow blocks\u201d to \u201cit\u2019s profitable to make slow blocks\u201d, if you see what I mean.</p>\n</blockquote>\n</aside>\n<p>This is an important subtlety that I obviously didn\u2019t convey clearly in the BIP.</p>\n<p>There\u2019s a large gap between the worst case, and the average case, and we spec for the worst case.  In particular:</p>\n<p>Timings are done the largest possible objects (eg. 2MB objects for OP_ADD, since to do it repeatedly we need to DUP).  This doesn\u2019t seem like it will be typical usage for anything, and my initial benchmarking showed how absurdly fast the cases of 10k object was: significantly faster.  Hard to tell in practice with the current benchmarks, since we start getting dominated by script interpretation time, not the actual ops.</p>\n<p>Addition also assumes worst case: that we overflow and have to reallocate and thus copy the entire array (so we add 50% to the add cost).  This is rarely true: even if we do overflow, the allocator usually doesn\u2019t have to copy.</p>\n<p>Multiplication, division and modulus build on this add assumption, and create more: in particular, we assume the miss case in the divide guess, which is really hard to hit except by deliberate attempt.  Again, this means gross overcharging in the divide: I can\u2019t come close to the worst case varops cost with any numbers I tried.</p>\n<p>The result is a much more conservative approach for real usage.   But there\u2019s more, for complex operations which may conceivably reach these limits:</p>\n<p>The implementation is fairly optimal (treating things as 64-bit numbers, in particular), but it\u2019s also naive, to simplify evaluation and finding the worst case.  It uses a simple schoolbook multiply (and thus, divide), which can be fairly easily optimized using Karatsuba or Toom Cook, let alone the zoo of hyperspeed functions in libGMP.  Going further, optimizing the script interpreter is also possible.</p>\n<aside class=\"quote no-group\" data-username=\"ajtowns\" data-post=\"14\" data-topic=\"2094\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/ajtowns/48/417_2.png\" class=\"avatar\"> ajtowns:</div>\n<blockquote>\n<p>Limiting the worst case GSR execution costs to the equivalent of a block full of fairly normal transactions would be very safe, by contrast. I think that would be something on the order of ~13k signature operations per block rather than 80k (based on filling a block with ~3300 2-in, 2-out 2-of-3 multisig p2wsh transactions).</p>\n<p>Targeting a validation time of perhaps one or two microseconds per vbyte (so one or two seconds per block) or so would be a better target, I think, though obviously that would require picking some baseline hardware as a minimum standard.</p>\n</blockquote>\n</aside>\n<p>To be concrete: 80,000 signature checks takes 1.9 seconds on my laptop (i7-1280P, 2023). I know this has sped up since Schnorr was added: I tried to benchmark 0.13.1, to measure the original implementation, but I backed away after realizing that would need an old boost library, etc\u2026</p>\n<p>This is an important sanity check!  Even if the worst case becomes common, we\u2019re still under 2 seconds for a modern machine, which seems reasonable.</p>",
  "post_number": 15,
  "post_type": 1,
  "posts_count": 15,
  "updated_at": "2026-01-26T03:52:44.709Z",
  "reply_count": 0,
  "reply_to_post_number": 14,
  "quote_count": 1,
  "incoming_link_count": 0,
  "reads": 2,
  "readers_count": 1,
  "score": 0.4,
  "yours": false,
  "topic_id": 2094,
  "topic_slug": "benchmarking-bitcoin-script-evaluation-for-the-varops-budget-great-script-restoration",
  "topic_title": "Benchmarking Bitcoin Script Evaluation for the Varops Budget (Great Script Restoration)",
  "topic_html_title": "Benchmarking Bitcoin Script Evaluation for the Varops Budget (Great Script Restoration)",
  "category_id": 7,
  "display_username": "Rusty Russell",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "[quote=\"ajtowns, post:14, topic:2094\"]\nThat might be reasonable, but I don\u2019t think it\u2019s good enough: there\u2019s a difference between \u201cyou can waste X resources doing this silly pattern that serves no purpose except as an attack\u201d and \u201cyou can now waste X resources with arbitrary logic\u201d. In the former case, you\u2019re only vulnerable to deliberate attackers; the latter case you\u2019re vulnerable to everyone who comes up with new logic they want to deploy. It was easy to make 4MB blocks as soon as segwit was active; but until inscriptions came along, there was no real reason to do that. We don\u2019t want to have GSR result in a similar state change from \u201cit\u2019s possible to make slow blocks\u201d to \u201cit\u2019s profitable to make slow blocks\u201d, if you see what I mean.\n\n[/quote]\n\nThis is an important subtlety that I obviously didn\u2019t convey clearly in the BIP.  \n\nThere\u2019s a large gap between the worst case, and the average case, and we spec for the worst case.  In particular:\n\nTimings are done the largest possible objects (eg. 2MB objects for OP_ADD, since to do it repeatedly we need to DUP).  This doesn\u2019t seem like it will be typical usage for anything, and my initial benchmarking showed how absurdly fast the cases of 10k object was: significantly faster.  Hard to tell in practice with the current benchmarks, since we start getting dominated by script interpretation time, not the actual ops.\n\nAddition also assumes worst case: that we overflow and have to reallocate and thus copy the entire array (so we add 50% to the add cost).  This is rarely true: even if we do overflow, the allocator usually doesn\u2019t have to copy.\n\nMultiplication, division and modulus build on this add assumption, and create more: in particular, we assume the miss case in the divide guess, which is really hard to hit except by deliberate attempt.  Again, this means gross overcharging in the divide: I can\u2019t come close to the worst case varops cost with any numbers I tried.\n\nThe result is a much more conservative approach for real usage.   But there\u2019s more, for complex operations which may conceivably reach these limits:\n\nThe implementation is fairly optimal (treating things as 64-bit numbers, in particular), but it\u2019s also naive, to simplify evaluation and finding the worst case.  It uses a simple schoolbook multiply (and thus, divide), which can be fairly easily optimized using Karatsuba or Toom Cook, let alone the zoo of hyperspeed functions in libGMP.  Going further, optimizing the script interpreter is also possible.\n\n[quote=\"ajtowns, post:14, topic:2094\"]\nLimiting the worst case GSR execution costs to the equivalent of a block full of fairly normal transactions would be very safe, by contrast. I think that would be something on the order of \\~13k signature operations per block rather than 80k (based on filling a block with \\~3300 2-in, 2-out 2-of-3 multisig p2wsh transactions).\n\nTargeting a validation time of perhaps one or two microseconds per vbyte (so one or two seconds per block) or so would be a better target, I think, though obviously that would require picking some baseline hardware as a minimum standard.\n\n[/quote]\n\nTo be concrete: 80,000 signature checks takes 1.9 seconds on my laptop (i7-1280P, 2023). I know this has sped up since Schnorr was added: I tried to benchmark 0.13.1, to measure the original implementation, but I backed away after realizing that would need an old boost library, etc\u2026\n\nThis is an important sanity check!  Even if the worst case becomes common, we\u2019re still under 2 seconds for a modern machine, which seems reasonable.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 137,
  "hidden": false,
  "trust_level": 2,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "This is an important subtlety that I obviously didn\u2019t convey clearly in the BIP. \nThere\u2019s a large gap between the worst case, and the average case, and we spec for the worst case.  In particular: \nTimings are done the largest possible objects (eg. 2MB objects for OP_ADD, since to do it repeatedly w&hellip;",
  "truncated": true,
  "post_url": "/t/benchmarking-bitcoin-script-evaluation-for-the-varops-budget-great-script-restoration/2094/15",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}