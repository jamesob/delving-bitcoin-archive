{
  "id": 5708,
  "name": "Pieter Wuille",
  "username": "sipa",
  "avatar_template": "/user_avatar/delvingbitcoin.org/sipa/{size}/1100_2.png",
  "created_at": "2025-08-14T13:12:48.753Z",
  "cooked": "<blockquote>\n<p>I fixed b because you fixed k and I was wanting to see if the equations for b and k-1 were the same in the limit, i.e. if the kth lowest hash is exactly like a target for b</p>\n</blockquote>\n<p>Fair enough. However, I\u2019ve also come to the conclusion that using the k\u2019th lowest hash for estimating hashrate is inferior to using <code>(n-1)/n * sum(work)/sum(time)</code>, so while interesting, I don\u2019t think it\u2019s useful for actually measuring hashrate.</p>\n<blockquote>\n<p>Also, when someone runs getnetworkhashps, they\u2019re selecting a b number of blocks.</p>\n</blockquote>\n<p>Sure, but then you\u2019re talking about a random process where the number of blocks is fixed, and the time isn\u2019t. If you fix the time, the number of blocks isn\u2019t fixed anymore, and then indeed, it isn\u2019t directly applicable anymore in the current <code>getnetworkhashps</code> RPC. But intuitively, because in that case <code>sum(work in window)/(window length)</code> is such a simple and unbiased estimator, it seems that \u201cfixed time window, variable number of blocks\u201d is actually the better way of measuring hashrate if say you want to know the hashrate in the last week (as opposed to first checking how many blocks there in the window, and then running a fixed-number-of-block estimator on that, which - whether you want it or not - brings actually back to the fixed-blocks-variable-time case).</p>\n<p>To simulate:</p>\n<ul>\n<li>Pick a scenario, which is a <strong>fixed</strong> amount of time, which you subdivide into a number of fixed segments, each having a fixed hashrate and a fixed difficulty. I think that roughly matches reality, allowing the hashrate and difficulty to change as a function of time. Further down I\u2019ll relax this to allow them to be functions of the previous blocks\u2019 timestamps too.</li>\n<li>For each segment <span class=\"math\">i</span>, sample <span class=\"math\">\\mathrm{Poisson}(\\mathrm{duration}_i \\times \\mathrm{hashrate}_i / \\mathrm{difficulty}_i)</span>, representing the number of blocks <span class=\"math\">b_i</span> found in that segment.</li>\n<li>The hashrate estimate is <code>sum(work)/(window duration)</code>, so in this simulation that corresponds to <span class=\"math\">(\\sum_{i}b_i \\times \\mathrm{difficulty}_i) / \\sum_{i}\\mathrm{duration}_i</span>.</li>\n<li>The real average hashrate through the window is <span class=\"math\">(\\sum_{i} \\mathrm{duration}_i \\times \\mathrm{hashrate}_i) / \\sum_{i} \\mathrm{duration}_i</span></li>\n</ul>\n<p>My finding is that the expected value of the estimate exactly matches the real average hashrate, without any correction factor to make it unbiased. I also think this is easy to prove: the expected value of each <span class=\"math\">(b_i \\times \\mathrm{difficulty}_i)</span> is <span class=\"math\">(\\mathrm{duration}_i \\times \\mathrm{hashrate}_i)</span>, and the expectation of a sum is the sum of the expectations. Lastly, dividing by <span class=\"math\">\\sum_{i} \\mathrm{duration}_i</span> is just dividing the expectation by a constant (it\u2019s this last step that is much harder in the variable-time case, because it\u2019s not a constant anymore then).</p>\n<p>We can generalize this to making the difficulty a function of the prior blocks\u2019 timestamps without losing the result. First, observe that we can arbitrarily split each segment into multiple smaller segments with all the same hashrate and difficulty. In the limit, we can think of there being an infinite number of tiny segments, which can each have their own hashrate and difficulty, i.e., this approaches making hashrate and difficulty a <em>function of time</em>.</p>\n<p>Secondly, observe that the difficulty just does not matter. It affects the number <span class=\"math\">b_i</span> of blocks within a segment, but is cancelled out in the expression of the expected value of <span class=\"math\">\\sum_{i} b_i \\times \\mathrm{difficulty}_i</span>. So the <span class=\"math\">\\mathrm{difficulty}_i</span> can be made a function of the prior <span class=\"math\">b_j</span> for <span class=\"math\">j &lt; i</span>.</p>\n<p>Combining these, we can treat the difficulty as an arbitrarily function of the previous block\u2019s timestamps (which are encoded in the <span class=\"math\">b_i</span> values, if the segments become infinitesimal), changing every time <span class=\"math\">N</span> blocks are found (well, infinitesimally after that block is found, during the next segment).</p>\n<p>Making the hashrate a function of previous blocks\u2019 timestamps is a bit more complicated, but the result still holds. This does not cancel out in the expression for the expectation of <span class=\"math\">b_i</span>, and thus these are no longer independent. However, since the expectation of a sum is still the sum of the expectations, even when the terms being added are not independent, this does not affect the result.</p>\n<p>So, I conclude that <code>sum(work in window) / (window duration)</code> is an unbiased estimator for the hashate during <strong>a fixed window</strong>, even when the hashrate and difficulty can change as a function of both time and prior blocks\u2019 timestamps.</p>",
  "post_number": 30,
  "post_type": 1,
  "posts_count": 28,
  "updated_at": "2025-08-14T13:31:15.729Z",
  "reply_count": 0,
  "reply_to_post_number": 29,
  "quote_count": 0,
  "incoming_link_count": 0,
  "reads": 4,
  "readers_count": 3,
  "score": 0.8,
  "yours": false,
  "topic_id": 1745,
  "topic_slug": "correcting-the-error-in-getnetworkhashrateps",
  "topic_title": "Correcting the error in getnetworkhashrateps",
  "topic_html_title": "Correcting the error in getnetworkhashrateps",
  "category_id": 7,
  "display_username": "Pieter Wuille",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 2,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "reply_to_user": {
    "id": 502,
    "username": "zawy",
    "name": "Zawy",
    "avatar_template": "/user_avatar/delvingbitcoin.org/zawy/{size}/750_2.png"
  },
  "bookmarked": false,
  "raw": "> I fixed b because you fixed k and I was wanting to see if the equations for b and k-1 were the same in the limit, i.e. if the kth lowest hash is exactly like a target for b\n\nFair enough. However, I've also come to the conclusion that using the k'th lowest hash for estimating hashrate is inferior to using `(n-1)/n * sum(work)/sum(time)`, so while interesting, I don't think it's useful for actually measuring hashrate.\n\n>  Also, when someone runs getnetworkhashps, they\u2019re selecting a b number of blocks.\n\nSure, but then you're talking about a random process where the number of blocks is fixed, and the time isn't. If you fix the time, the number of blocks isn't fixed anymore, and then indeed, it isn't directly applicable anymore in the current `getnetworkhashps` RPC. But intuitively, because in that case `sum(work in window)/(window length)` is such a simple and unbiased estimator, it seems that \"fixed time window, variable number of blocks\" is actually the better way of measuring hashrate if say you want to know the hashrate in the last week (as opposed to first checking how many blocks there in the window, and then running a fixed-number-of-block estimator on that, which - whether you want it or not - brings actually back to the fixed-blocks-variable-time case).\n\nTo simulate:\n* Pick a scenario, which is a **fixed** amount of time, which you subdivide into a number of fixed segments, each having a fixed hashrate and a fixed difficulty. I think that roughly matches reality, allowing the hashrate and difficulty to change as a function of time. Further down I'll relax this to allow them to be functions of the previous blocks' timestamps too.\n* For each segment $i$, sample $\\mathrm{Poisson}(\\mathrm{duration}_i \\times \\mathrm{hashrate}_i / \\mathrm{difficulty}_i)$, representing the number of blocks $b_i$ found in that segment.\n* The hashrate estimate is `sum(work)/(window duration)`, so in this simulation that corresponds to $(\\sum_{i}b_i \\times \\mathrm{difficulty}_i) / \\sum_{i}\\mathrm{duration}_i$.\n* The real average hashrate through the window is $(\\sum_{i} \\mathrm{duration}_i \\times \\mathrm{hashrate}_i) / \\sum_{i} \\mathrm{duration}_i$\n\nMy finding is that the expected value of the estimate exactly matches the real average hashrate, without any correction factor to make it unbiased. I also think this is easy to prove: the expected value of each $(b_i \\times \\mathrm{difficulty}_i)$ is $(\\mathrm{duration}_i \\times \\mathrm{hashrate}_i)$, and the expectation of a sum is the sum of the expectations. Lastly, dividing by $\\sum_{i} \\mathrm{duration}_i$ is just dividing the expectation by a constant (it's this last step that is much harder in the variable-time case, because it's not a constant anymore then).\n\nWe can generalize this to making the difficulty a function of the prior blocks' timestamps without losing the result. First, observe that we can arbitrarily split each segment into multiple smaller segments with all the same hashrate and difficulty. In the limit, we can think of there being an infinite number of tiny segments, which can each have their own hashrate and difficulty, i.e., this approaches making hashrate and difficulty a *function of time*.\n\nSecondly, observe that the difficulty just does not matter. It affects the number $b_i$ of blocks within a segment, but is cancelled out in the expression of the expected value of $\\sum_{i} b_i \\times \\mathrm{difficulty}_i$. So the $\\mathrm{difficulty}_i$ can be made a function of the prior $b_j$ for $j < i$.\n\nCombining these, we can treat the difficulty as an arbitrarily function of the previous block's timestamps (which are encoded in the $b_i$ values, if the segments become infinitesimal), changing every time $N$ blocks are found (well, infinitesimally after that block is found, during the next segment).\n\nMaking the hashrate a function of previous blocks' timestamps is a bit more complicated, but the result still holds. This does not cancel out in the expression for the expectation of $b_i$, and thus these are no longer independent. However, since the expectation of a sum is still the sum of the expectations, even when the terms being added are not independent, this does not affect the result.\n\nSo, I conclude that `sum(work in window) / (window duration)` is an unbiased estimator for the hashate during **a fixed window**, even when the hashrate and difficulty can change as a function of both time and prior blocks' timestamps.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 96,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "I fixed b because you fixed k and I was wanting to see if the equations for b and k-1 were the same in the limit, i.e. if the kth lowest hash is exactly like a target for b \n\nFair enough. However, I\u2019ve also come to the conclusion that using the k\u2019th lowest hash for estimating hashrate is inferior t&hellip;",
  "truncated": true,
  "post_url": "/t/correcting-the-error-in-getnetworkhashrateps/1745/30",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}