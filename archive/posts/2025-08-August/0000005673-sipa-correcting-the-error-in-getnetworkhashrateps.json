{
  "id": 5673,
  "name": "Pieter Wuille",
  "username": "sipa",
  "avatar_template": "/user_avatar/delvingbitcoin.org/sipa/{size}/1100_2.png",
  "created_at": "2025-08-07T20:03:14.481Z",
  "cooked": "<p>Interesting. This works with any \u201ck\u2019th lowest hash\u201d, actually.</p>\n<p>If <span class=\"math\">H_{(k)}</span> is the k\u2019th lowest hash seen (the lowest one having <span class=\"math\">k=1</span>), normalized to <span class=\"math\">[0 \\ldots 1]</span>, then <span class=\"math\">\\frac{k-1}{H_{(k)}}</span> is an estimate for the amount of work.</p>\n<p>If we model this as <span class=\"math\">n</span> uniformly random samples being taken from <span class=\"math\">[0 \\ldots 1]</span> (which is what each hash attempt is), then</p>\n<div class=\"math\">\nH_{(k)} \\sim  \\mathrm{Beta}(k, n-k+1)\n</div>\n<p>For such a distribution</p>\n<div class=\"math\">\n\\mathrm{E}\\left[\\frac{k-1}{H_{(k)}}\\right] = n \\\\\n\\mathrm{Var}\\left[\\frac{k-1}{H_{(k)}}\\right] = \\frac{n(n-k+1)}{k-2} \\approx \\frac{n^2}{k-2}\n</div>\n<p>which means that using higher <span class=\"math\">k</span> gives a better estimate.</p>\n<p><s>However, for any small constant <span class=\"math\">k</span>, this variance is still <span class=\"math\">\\mathcal{O}(n^2)</span>. Using the number of blocks is a much better estimators.</s> Let <span class=\"math\">b</span> be the number of blocks seen, and <span class=\"math\">w</span> the expected amount of work per block:</p>\n<div class=\"math\">\nw = \\frac{2^{256}-1}{\\mathrm{target}}\n</div>\n<p>Then</p>\n<div class=\"math\">\nb \\sim \\mathrm{Poisson}\\left(\\frac{n}{w}\\right)\n</div>\n<p>And we can esimate the amount of work as <span class=\"math\">bw</span>:</p>\n<div class=\"math\">\n\\mathrm{E}\\left[bw\\right] = n \\\\\n\\mathrm{Var}\\left[bw\\right] = nw \\approx \\frac{n^2}{b}\n</div>\n<p><s>Which has just <span class=\"math\">\\mathcal{O}(n)</span> variance.</s></p>\n<p>EDIT: I made a mistake in the variance of the <span class=\"math\">bw</span> estimator, it\u2019s <span class=\"math\">nw</span>, not <span class=\"math\">n</span>. The two approaches do seem similar in terms of variance:</p>\n<ul>\n<li><span class=\"math\">\\frac{k-1}{H_{(k)}}</span> has standard deviation roughly equal to <span class=\"math\">\\frac{n}{\\sqrt{k-2}}</span></li>\n<li><span class=\"math\">bw</span> has standard deviation roughly equal to <span class=\"math\">\\frac{n}{\\sqrt{b}}</span>.</li>\n</ul>",
  "post_number": 19,
  "post_type": 1,
  "posts_count": 19,
  "updated_at": "2025-08-08T01:35:39.344Z",
  "reply_count": 0,
  "reply_to_post_number": 18,
  "quote_count": 0,
  "incoming_link_count": 1,
  "reads": 7,
  "readers_count": 6,
  "score": 6.4,
  "yours": false,
  "topic_id": 1745,
  "topic_slug": "correcting-the-error-in-getnetworkhashrateps",
  "topic_title": "Correcting the error in getnetworkhashrateps",
  "topic_html_title": "Correcting the error in getnetworkhashrateps",
  "category_id": 7,
  "display_username": "Pieter Wuille",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 4,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "reply_to_user": {
    "id": 502,
    "username": "zawy",
    "name": "Zawy",
    "avatar_template": "/user_avatar/delvingbitcoin.org/zawy/{size}/750_2.png"
  },
  "bookmarked": false,
  "raw": "Interesting. This works with any \u201ck'th lowest hash\u201d, actually.\n\nIf $H_{(k)}$ is the k\u2019th lowest hash seen (the lowest one having $k=1$), normalized to $[0 \\ldots 1]$, then $\\frac{k-1}{H_{(k)}}$ is an estimate for the amount of work.\n\nIf we model this as $n$ uniformly random samples being taken from $[0 \\ldots 1]$ (which is what each hash attempt is), then\n$$\nH_{(k)} \\sim  \\mathrm{Beta}(k, n-k+1)\n$$\nFor such a distribution\n$$\n\\mathrm{E}\\left[\\frac{k-1}{H_{(k)}}\\right] = n \\\\\n\\mathrm{Var}\\left[\\frac{k-1}{H_{(k)}}\\right] = \\frac{n(n-k+1)}{k-2} \\approx \\frac{n^2}{k-2}\n$$\nwhich means that using higher $k$ gives a better estimate.\n\n~~However, for any small constant $k$, this variance is still $\\mathcal{O}(n^2)$. Using the number of blocks is a much better estimators.~~ Let $b$ be the number of blocks seen, and $w$ the expected amount of work per block:\n$$\nw = \\frac{2^{256}-1}{\\mathrm{target}}\n$$\nThen\n$$\nb \\sim \\mathrm{Poisson}\\left(\\frac{n}{w}\\right)\n$$\nAnd we can esimate the amount of work as $bw$:\n$$\n\\mathrm{E}\\left[bw\\right] = n \\\\\n\\mathrm{Var}\\left[bw\\right] = nw \\approx \\frac{n^2}{b}\n$$\n~~Which has just $\\mathcal{O}(n)$ variance.~~\n\nEDIT: I made a mistake in the variance of the $bw$ estimator, it's $nw$, not $n$. The two approaches do seem similar in terms of variance:\n* $\\frac{k-1}{H_{(k)}}$ has standard deviation roughly equal to $\\frac{n}{\\sqrt{k-2}}$\n* $bw$ has standard deviation roughly equal to $\\frac{n}{\\sqrt{b}}$.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 96,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "Interesting. This works with any \u201ck\u2019th lowest hash\u201d, actually. \nIf H_{(k)} is the k\u2019th lowest hash seen (the lowest one having k=1), normalized to [0 \\ldots 1], then \\frac{k-1}{H_{(k)}} is an estimate for the amount of work. \nIf we model this as n uniformly random samples being taken from [0 \\ldots &hellip;",
  "truncated": true,
  "post_url": "/t/correcting-the-error-in-getnetworkhashrateps/1745/19",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}