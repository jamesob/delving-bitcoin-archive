{
  "id": 5717,
  "name": "Zawy",
  "username": "zawy",
  "avatar_template": "/user_avatar/delvingbitcoin.org/zawy/{size}/750_2.png",
  "created_at": "2025-08-15T11:24:31.467Z",
  "cooked": "<p>I believe this is the best-possible estimate of <em>current</em> hashrate for an arbitrarily-small fixed time period t when given only the lowest hash seen in t.</p>\n<p><span class=\"math\">L = \\text{lowest hash seen in time t}</span></p>\n<p><span class=\"math\">W = \\text{work in t}</span></p>\n<p><span class=\"math\">\\lambda = \\frac{W}{2^{256}}</span></p>\n<p><span class=\"math\">\\text{exponential CDF}(L) = 1 - e^{-\\lambda L}</span></p>\n<p><span class=\"math\">E = \\text{error signal} = \\text{CDF}(L) - 0.5</span></p>\n<p>The error signal is the probability that the prior estimate of W was wrong. 0.5 is the median observation if the prior estimate was correct, so there would be no error. Use the error signal in the EMA equation.</p>\n<p><span class=\"math\">h = \\text{height of t time segments}</span></p>\n<p><span class=\"math\">W_{h} = W_{h-1} \\cdot e^{-\\frac{E}{N}}</span></p>\n<p><span class=\"math\">\\text{Stdev} \\approx \\frac{W}{\\sqrt{2N}}</span></p>\n<p><span class=\"math\">W</span> and <span class=\"math\">L</span> in my lambda are at <span class=\"math\">h -1</span></p>\n<p>N = \u201cmean lifetime\u201d of the EMA estimate in units of t. You choose N to get your desired stability / slowness of the estimate.  Divide by t to get hashrate.</p>\n<p>To simplify the equation, use <span class=\"math\">e^{-x} \\approx 1-x</span> for small <span class=\"math\">x = \\frac{E}{N}</span>:</p>\n<p><span class=\"math\">W_{h} = W_{h-1} \\cdot ( 1 + \\frac{e^{-L \\cdot W_{h-1}}}{N} - \\frac{1}{2N})</span></p>\n<p>The median L is expected to be <span class=\"math\">ln(2) \\cdot  W_{h-1}</span> which would be no correction. The smallest-possible L makes the largest-possible correction:</p>\n<p><span class=\"math\">W_{h} = W_{h-1} \\cdot ( 1 + \\frac{1}{2N})</span></p>\n<p>A large L can has a large correction, but an accidentally-large L isn\u2019t possible like a small L. This is a spot check on my math and the legitimacy of the idea The idea comes from my search for the mathematically-perfect difficulty algorithm which similarly uses the exponential CDF of solvetimes to adjust difficulty every block and experiments have shown it is the best-known (fastest response time to changes in hashrate with the least variation).</p>\n<p><code>hashrate = sum(work)/(time duration)</code> from <span class=\"math\">h - N</span> to <span class=\"math\">h</span> gives a better estimate of hashrate at <span class=\"math\">h -N/2</span>.  The EMA needs a starting W. If it starts at <span class=\"math\">h-N</span> The starting <span class=\"math\">W_{h-N}</span> could be obtained by <code>sum(work) </code> from <span class=\"math\">h-\\frac{3N}{2}</span> to <span class=\"math\">h-\\frac{N}{2}</span> and dividing by N.</p>",
  "post_number": 32,
  "post_type": 1,
  "posts_count": 30,
  "updated_at": "2025-08-16T13:01:43.434Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 0,
  "reads": 7,
  "readers_count": 6,
  "score": 1.4,
  "yours": false,
  "topic_id": 1745,
  "topic_slug": "correcting-the-error-in-getnetworkhashrateps",
  "topic_title": "Correcting the error in getnetworkhashrateps",
  "topic_html_title": "Correcting the error in getnetworkhashrateps",
  "category_id": 7,
  "display_username": "Zawy",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 6,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "I believe this is the best-possible estimate of _current_ hashrate for an arbitrarily-small fixed time period t when given only the lowest hash seen in t. \n\n$L = \\text{lowest hash seen in time t}$\n\n$W = \\text{work in t}$\n\n$\\lambda = \\frac{W}{2^{256}}$\n\n$\\text{exponential CDF}(L) = 1 - e^{-\\lambda L}$\n\n$E = \\text{error signal} = \\text{CDF}(L) - 0.5$\n\nThe error signal is the probability that the prior estimate of W was wrong. 0.5 is the median observation if the prior estimate was correct, so there would be no error. Use the error signal in the EMA equation. \n\n$h = \\text{height of t time segments}$\n\n$W_{h} = W_{h-1} \\cdot e^{-\\frac{E}{N}}$\n\n$\\text{Stdev} \\approx \\frac{W}{\\sqrt{2N}}$\n\n$W$ and $L$ in my lambda are at $h -1$\n\nN = \"mean lifetime\" of the EMA estimate in units of t. You choose N to get your desired stability / slowness of the estimate.  Divide by t to get hashrate. \n\nTo simplify the equation, use $e^{-x} \\approx 1-x$ for small $x = \\frac{E}{N}$:\n\n$W_{h} = W_{h-1} \\cdot ( 1 + \\frac{e^{-L \\cdot W_{h-1}}}{N} - \\frac{1}{2N})$\n\nThe median L is expected to be $ln(2) \\cdot  W_{h-1}$ which would be no correction. The smallest-possible L makes the largest-possible correction:\n\n$W_{h} = W_{h-1} \\cdot ( 1 + \\frac{1}{2N})$\n\nA large L can has a large correction, but an accidentally-large L isn't possible like a small L. This is a spot check on my math and the legitimacy of the idea The idea comes from my search for the mathematically-perfect difficulty algorithm which similarly uses the exponential CDF of solvetimes to adjust difficulty every block and experiments have shown it is the best-known (fastest response time to changes in hashrate with the least variation).\n\n```hashrate = sum(work)/(time duration)``` from $h - N$ to $h$ gives a better estimate of hashrate at $h -N/2$.  The EMA needs a starting W. If it starts at $h-N$ The starting $W_{h-N}$ could be obtained by ```sum(work) ``` from $h-\\frac{3N}{2}$ to $h-\\frac{N}{2}$ and dividing by N.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 502,
  "hidden": false,
  "trust_level": 2,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "I believe this is the best-possible estimate of current hashrate for an arbitrarily-small fixed time period t when given only the lowest hash seen in t. \nL = \\text{lowest hash seen in time t} \nW = \\text{work in t} \n\\lambda = \\frac{W}{2^{256}} \n\\text{exponential CDF}(L) = 1 - e^{-\\lambda L} \nE = \\tex&hellip;",
  "truncated": true,
  "post_url": "/t/correcting-the-error-in-getnetworkhashrateps/1745/32",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}