{
  "id": 3465,
  "name": "josie",
  "username": "josibake",
  "avatar_template": "/user_avatar/delvingbitcoin.org/josibake/{size}/83_2.png",
  "created_at": "2024-11-04T10:51:22.279Z",
  "cooked": "<p>Thanks for writing this! I found the bridging of the terminology particularly helpful. I was finding it difficult to conceptualise the Libbitcoin data model from your description, so I made an attempt at diagraming it, let me know if this is an accurate representation <sup class=\"footnote-ref\"><a href=\"#footnote-3465-1\" id=\"footnote-ref-3465-1\">[1]</a></sup>.</p>\n<h2><a name=\"p-3465-my-understanding-of-libbitcoin-1\" class=\"anchor\" href=\"#p-3465-my-understanding-of-libbitcoin-1\"></a>My understanding of Libbitcoin</h2>\n<pre data-code-height=\"829\" data-code-wrap=\"mermaid\"><code class=\"lang-mermaid\">erDiagram\n    Headers {\n        header_hash BYTES PK\n    }\n\n    ChainIndex {\n        block_height INT PK\n        header_hash BYTES FK\n    }\n\n    Transactions {\n        txid BYTES PK\n        header_hash BYTES FK\n        other_fields TEXT \"Transaction data fields\"\n    }\n\n    Outputs {\n        outpoint BYTES PK \"txid + index\"\n        txid BYTES FK\n    }\n\n    Inputs {\n        txid BYTES PK\n        index INT PK\n        outpoint BYTES FK \"outpoint being spent\"\n    }\n\n    ConfirmedTransactions {\n        txid BYTES PK\n        block_height INT FK\n    }\n\n    Headers ||--o{ Transactions : contains\n    ChainIndex ||--|| Headers : \"best chain\"\n    ChainIndex ||--o{ ConfirmedTransactions : \"confirms at height\"\n    Transactions ||--o{ Outputs : creates\n    Transactions ||--o{ Inputs : contains\n    Outputs ||--o{ Inputs : \"spent by\"\n</code></pre>\n<p>(also been awhile since I\u2019ve written an ER diagram so likely contains mistakes <img src=\"https://delvingbitcoin.org/images/emoji/twitter/sweat_smile.png?v=12\" title=\":sweat_smile:\" class=\"emoji\" alt=\":sweat_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> ). To check my understanding against this data model (under the <code>-assumevalid</code> / milestone model):</p>\n<ol>\n<li>Non-overlapping ranges of blocks (e.g., 1000 blocks at a time) are downloaded and and unordered checks are done in parallel across multiple ranges</li>\n<li>The first range can also start confirmability checks (genesis to block 999), after which transactions from this range are committed to the <code>Transactions</code> table and the <code>ConfirmationIndex</code> is updated</li>\n<li>Now that the first range has finished with it\u2019s confirmability checks, the second range of blocks can start its confirmability checks, and so on</li>\n</ol>\n<p>Conceptually, this seems close to a MapReduce algorithm, where unordered checks are mapped on to each range, and then the Reduce step requires sequential ordering of each range of blocks for the confirmability checks <sup class=\"footnote-ref\"><a href=\"#footnote-3465-2\" id=\"footnote-ref-3465-2\">[2]</a></sup>. Clear separation between un-ordered and fully ordered checks is what makes this possible, from what I can tell. The partially ordered checks are not done under the <code>-assumevalid</code> / milestone model.</p>\n<p>Before commenting on how this compares to Bitcoin Core, I want to say hats off to the Libbitcoin engineering team! If my understanding is correct, this is an elegantly designed event driven system, using MapReduce for data processing. My intuition is that the speedups are coming from the more aggressive peer utilisation during download, and the clear separation of unordered vs ordered checks to take advantage of parallelism.</p>\n<h2><a name=\"p-3465-comparing-to-bitcoin-core-2\" class=\"anchor\" href=\"#p-3465-comparing-to-bitcoin-core-2\"></a>Comparing to Bitcoin Core</h2>\n<p>While attempting to write the ER diagrams, it occurred to me Libbitcoin is using a <em>Transaction</em> based data model, as opposed to Core\u2019s <em>Block</em> based data model. I know in other places this has been called a \u201cUTXO set\u201d data model, but it\u2019s more helpful for me to think of it as <em>Block</em> based for this comparison. A few differences that jump out to me:</p>\n<ol>\n<li>When a Libbitcoin node serves a block to a peer, it must reconstruct the block into its serialised representation, which will require some compute for each block request. Bitcoin Core, however, stores the blocks on disk in their serialised format and will simply read the block from disk and send it to the peer</li>\n<li>When updating the chaintip, it seems Bitcoin Core will be faster at fully validating the new block (unordered, partially ordered, and fully ordered checks) and updating the chaintip, whereas Libbitcoin will be slower as most of its speed-ups rely on processing ranges of blocks under the milestone model</li>\n</ol>\n<p>These are very high-level handwavey claims, and as its been mentioned already, Libbitcoin might be able to close this gap on fully validating new blocks after implementing libsecp / SHANI optimisations that are currently in Core. But I wanted to highlight the differences in data model because my intuition is a perfectly optimised <em>Transaction</em> based data model will <em>always</em> perform faster during IBD than a perfectly optimised <em>Block</em> based data model, and a perfectly optimised <em>Block</em> based data model will always perform faster than the <em>Transaction</em> model when it comes to processing new blocks and serving them to peers.</p>\n<h2><a name=\"p-3465-closing-thoughts-3\" class=\"anchor\" href=\"#p-3465-closing-thoughts-3\"></a>Closing thoughts</h2>\n<p>I don\u2019t want to make claims (and hope I haven\u2019t implied) in this post that one is better than the other. My personal view on the role of a Bitcoin node is that its primary purpose is to validate and propagate new blocks as quickly as possible, such that all nodes and miners on the network can quickly come to agreement on what the longest/heaviest PoW chain is. This is why I favour the <em>Block</em> based data model. However, it\u2019s also clear that many other services / use cases need fast IBD and have more of a transaction based usecase for bitcoin, namely any block explorer, wallet backend, payment processing, etc. It feels like there is likely some middleground between the <em>Transaction</em> based model and the <em>Block</em> based model that could serve both use cases.</p>\n<hr class=\"footnotes-sep\">\n\n<ol class=\"footnotes-list\">\n<li id=\"footnote-3465-1\" class=\"footnote-item\"><p>I realise I\u2019m mostly just rephrasing the original post in my own language. My hope is this is seen as useful and not a critique of the original post, which I found extremely helpful in understanding the differences between Core and Libbitcoin <a href=\"#footnote-ref-3465-1\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n<li id=\"footnote-3465-2\" class=\"footnote-item\"><p>This is not quite MapReduce in that I don\u2019t think classic MapReduce requires a strict sequential ordering, which the confirmability checks do. Still, I found it to be a helpful mental model for trying to better understand the Libbitcoin approach <a href=\"#footnote-ref-3465-2\" class=\"footnote-backref\">\u21a9\ufe0e</a></p>\n</li>\n</ol>",
  "post_number": 5,
  "post_type": 1,
  "updated_at": "2024-11-04T10:51:22.279Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 0,
  "reads": 1,
  "readers_count": 0,
  "score": 0,
  "yours": false,
  "topic_id": 1222,
  "topic_slug": "libbitcoin-for-core-people",
  "topic_title": "Libbitcoin for Core people",
  "topic_html_title": "Libbitcoin for Core people",
  "category_id": 8,
  "display_username": "josie",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "Thanks for writing this! I found the bridging of the terminology particularly helpful. I was finding it difficult to conceptualise the Libbitcoin data model from your description, so I made an attempt at diagraming it, let me know if this is an accurate representation [^1].\n\n## My understanding of Libbitcoin\n\n```mermaid height=829,auto\nerDiagram\n    Headers {\n        header_hash BYTES PK\n    }\n\n    ChainIndex {\n        block_height INT PK\n        header_hash BYTES FK\n    }\n\n    Transactions {\n        txid BYTES PK\n        header_hash BYTES FK\n        other_fields TEXT \"Transaction data fields\"\n    }\n\n    Outputs {\n        outpoint BYTES PK \"txid + index\"\n        txid BYTES FK\n    }\n\n    Inputs {\n        txid BYTES PK\n        index INT PK\n        outpoint BYTES FK \"outpoint being spent\"\n    }\n\n    ConfirmedTransactions {\n        txid BYTES PK\n        block_height INT FK\n    }\n\n    Headers ||--o{ Transactions : contains\n    ChainIndex ||--|| Headers : \"best chain\"\n    ChainIndex ||--o{ ConfirmedTransactions : \"confirms at height\"\n    Transactions ||--o{ Outputs : creates\n    Transactions ||--o{ Inputs : contains\n    Outputs ||--o{ Inputs : \"spent by\"\n```\n\n(also been awhile since I've written an ER diagram so likely contains mistakes :sweat_smile: ). To check my understanding against this data model (under the `-assumevalid` / milestone model):\n\n1. Non-overlapping ranges of blocks (e.g., 1000 blocks at a time) are downloaded and and unordered checks are done in parallel across multiple ranges\n2. The first range can also start confirmability checks (genesis to block 999), after which transactions from this range are committed to the `Transactions` table and the `ConfirmationIndex` is updated\n3. Now that the first range has finished with it's confirmability checks, the second range of blocks can start its confirmability checks, and so on\n\nConceptually, this seems close to a MapReduce algorithm, where unordered checks are mapped on to each range, and then the Reduce step requires sequential ordering of each range of blocks for the confirmability checks [^2]. Clear separation between un-ordered and fully ordered checks is what makes this possible, from what I can tell. The partially ordered checks are not done under the `-assumevalid` / milestone model.\n\nBefore commenting on how this compares to Bitcoin Core, I want to say hats off to the Libbitcoin engineering team! If my understanding is correct, this is an elegantly designed event driven system, using MapReduce for data processing. My intuition is that the speedups are coming from the more aggressive peer utilisation during download, and the clear separation of unordered vs ordered checks to take advantage of parallelism.\n\n## Comparing to Bitcoin Core\n\nWhile attempting to write the ER diagrams, it occurred to me Libbitcoin is using a _Transaction_ based data model, as opposed to Core's _Block_ based data model. I know in other places this has been called a \"UTXO set\" data model, but it's more helpful for me to think of it as _Block_ based for this comparison. A few differences that jump out to me:\n\n1. When a Libbitcoin node serves a block to a peer, it must reconstruct the block into its serialised representation, which will require some compute for each block request. Bitcoin Core, however, stores the blocks on disk in their serialised format and will simply read the block from disk and send it to the peer\n2. When updating the chaintip, it seems Bitcoin Core will be faster at fully validating the new block (unordered, partially ordered, and fully ordered checks) and updating the chaintip, whereas Libbitcoin will be slower as most of its speed-ups rely on processing ranges of blocks under the milestone model\n\nThese are very high-level handwavey claims, and as its been mentioned already, Libbitcoin might be able to close this gap on fully validating new blocks after implementing libsecp / SHANI optimisations that are currently in Core. But I wanted to highlight the differences in data model because my intuition is a perfectly optimised _Transaction_ based data model will _always_ perform faster during IBD than a perfectly optimised _Block_ based data model, and a perfectly optimised _Block_ based data model will always perform faster than the _Transaction_ model when it comes to processing new blocks and serving them to peers.\n\n## Closing thoughts\n\nI don't want to make claims (and hope I haven't implied) in this post that one is better than the other. My personal view on the role of a Bitcoin node is that its primary purpose is to validate and propagate new blocks as quickly as possible, such that all nodes and miners on the network can quickly come to agreement on what the longest/heaviest PoW chain is. This is why I favour the _Block_ based data model. However, it's also clear that many other services / use cases need fast IBD and have more of a transaction based usecase for bitcoin, namely any block explorer, wallet backend, payment processing, etc. It feels like there is likely some middleground between the _Transaction_ based model and the _Block_ based model that could serve both use cases.\n\n[^1]: I realise I'm mostly just rephrasing the original post in my own language. My hope is this is seen as useful and not a critique of the original post, which I found extremely helpful in understanding the differences between Core and Libbitcoin\n[^2]: This is not quite MapReduce in that I don't think classic MapReduce requires a strict sequential ordering, which the confirmability checks do. Still, I found it to be a helpful mental model for trying to better understand the Libbitcoin approach",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 92,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false
}