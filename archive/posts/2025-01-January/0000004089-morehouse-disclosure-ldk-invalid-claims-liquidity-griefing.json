{
  "id": 4089,
  "name": "Matt Morehouse",
  "username": "morehouse",
  "avatar_template": "/letter_avatar_proxy/v4/letter/m/df705f/{size}.png",
  "created_at": "2025-01-23T16:07:46.832Z",
  "cooked": "<p><em>The following disclosure is copied verbatim from a <a href=\"https://morehouse.github.io/lightning/ldk-invalid-claims-liquidity-griefing/\" rel=\"noopener nofollow ugc\">blog post</a> on <a href=\"http://morehouse.github.io\" rel=\"noopener nofollow ugc\">morehouse.github.io</a>, reproduced here to facilitate discussion.</em></p>\n<p>LDK 0.0.125 and below are vulnerable to a liquidity griefing attack against anchor channels.\nThe attack locks up funds such that they can only be recovered by manually constructing and broadcasting a valid claim transaction.\nAffected users can unlock their funds by upgrading to <a href=\"https://github.com/lightningdevkit/rust-lightning/releases/tag/v0.1\" rel=\"noopener nofollow ugc\">LDK 0.1</a> and replaying the sequence of commitment and HTLC transactions that led to the lock up.</p>\n<h1><a name=\"p-4089-background-1\" class=\"anchor\" href=\"#p-4089-background-1\"></a>Background</h1>\n<p>When a channel is force closed, LDK creates and broadcasts transactions to claim any HTLCs it can from the commitment transaction that confirmed on chain.\nTo save on fees, some HTLC claims are aggregated and broadcast together in the same transaction.</p>\n<p>If the channel counterparty is able to get a competing HTLC claim confirmed first, it can cause one of LDK\u2019s aggregated transactions to become invalid, since the corresponding HTLC input has already been spent by the counterparty\u2019s claim.\nLDK contains logic to detect this scenario and remove the already-claimed input from its aggregated claim transaction.\nWhen everything works correctly, the aggregated transaction becomes valid again and LDK is able to claim the remaining HTLCs.</p>\n<h1><a name=\"p-4089-the-invalid-claims-bug-2\" class=\"anchor\" href=\"#p-4089-the-invalid-claims-bug-2\"></a>The Invalid Claims Bug</h1>\n<p>Prior to LDK 0.1, the logic to detect conflicting claims works like this:</p>\n<pre data-code-wrap=\"python\"><code class=\"lang-python\">for confirmed_transaction in confirmed_block:\n  for input in confirmed_transaction:\n    if claimable_outpoints.contains(input.prevout):\n      agg_tx = get_aggregated_transaction_from_outpoint(input.prevout)\n      agg_tx.remove_matching_inputs(confirmed_transaction)\n      break  # This is the bug.\n</code></pre>\n<p>Note that this logic stops processing a confirmed transaction after finding the first aggregated transaction that conflicts with it.\nIf the confirmed transaction conflicts with <em>multiple</em> aggregated transactions, conflicting inputs are only removed from the <em>first</em> matching aggregated transaction, and any other conflicting aggregated transactions are left invalid.</p>\n<p>Any HTLCs claimed by invalid aggregated transactions get locked up and can only be recovered by manually constructing and broadcasting valid claim transactions.</p>\n<h1><a name=\"p-4089-liquidity-griefing-3\" class=\"anchor\" href=\"#p-4089-liquidity-griefing-3\"></a>Liquidity Griefing</h1>\n<p>Prior to LDK 0.1, there are only two types of HTLC claims that are aggregated:</p>\n<ul>\n<li>HTLC preimage claims</li>\n<li>revoked commitment HTLC claims</li>\n</ul>\n<p>For HTLC preimage claims, LDK takes care to confirm them before their HTLCs time out, so there\u2019s no reliable way for an attacker to confirm a conflicting timeout claim and trigger the invalid claims bug.</p>\n<p>For revoked commitment transactions, however, an attacker can immediately spend any incoming HTLC outputs via HTLC-Success transactions.\nAlthough LDK is then able to claim the HTLC-Success outputs via the revocation key, the attacker can exploit the invalid claims bug to lock up any remaining HTLCs on the revoked commitment transaction.</p>\n<h2><a name=\"p-4089-setup-4\" class=\"anchor\" href=\"#p-4089-setup-4\"></a>Setup</h2>\n<p>The attacker opens an anchor channel with the victim, creating a network topology as follows:</p>\n<pre><code class=\"lang-auto\">A -- B -- M\n</code></pre>\n<p>In this case <code>B</code> is the victim LDK node and <code>M</code> is the node controlled by the attacker.\nThe attacker must use an anchor channel so that they can spend multiple HTLC claims in the same transaction and trigger the invalid claims bug.</p>\n<p>The attacker then routes HTLCs along the path <code>A-&gt;B-&gt;M</code> as follows:</p>\n<ol>\n<li>1 small HTLC with CLTV of <code>X</code></li>\n<li>1 small HTLC with CLTV of <code>X+1</code></li>\n<li>1 large HTLC with CLTV of <code>X+1</code>  (this is the one the attacker will lock up)</li>\n</ol>\n<p>The attacker knows preimages for all HTLCs but withholds them for now.</p>\n<p>To complete the setup, the attacker routes some other HTLC through the channel, causing the commitment transaction with the above HTLCs to be revoked.</p>\n<h2><a name=\"p-4089-forcing-multiple-aggregations-5\" class=\"anchor\" href=\"#p-4089-forcing-multiple-aggregations-5\"></a>Forcing Multiple Aggregations</h2>\n<p>Next the attacker waits until block <code>X-13</code> and force closes the <code>B-M</code> channel using their revoked commitment transaction, being sure to get it confirmed in block <code>X-12</code>.\nBy confirming in this specific block, the attacker can exploit LDK\u2019s buggy aggregation logic prior to v0.1 (see below), causing LDK to aggregate HTLC justice claims as follows:</p>\n<ul>\n<li><strong>Transaction 1:</strong>  HTLC 1</li>\n<li><strong>Transaction 2:</strong>  HTLCs 2 and 3</li>\n</ul>\n<h3><a name=\"p-4089-buggy-aggregation-logic-6\" class=\"anchor\" href=\"#p-4089-buggy-aggregation-logic-6\"></a>Buggy Aggregation Logic</h3>\n<p>Prior to v0.1, LDK only aggregates HTLC claims if their timeouts are more than 12 blocks in the future.\nPresumably 12 blocks was deemed \u201ctoo soon\u201d to guarantee that LDK can confirm preimage claims before the HTLCs time out, and once one HTLC times out the counterparty can pin a competing timeout claim in mempools, thereby preventing confirmation of <em>all</em> the aggregated preimage claims.\nIn other words, by claiming HTLCs separately in this scenario, LDK limits the damage the counterparty could do if one of those HTLCs expires before LDK successfully claims it.</p>\n<p>Unfortunately, this aggregation strategy makes no sense when LDK is trying to group justice claims that the counterparty can spend immediately via HTLC-Success, since the timeout on those HTLCs does not apply to the counterparty.\nNevertheless, prior to LDK 0.1, the same 12 block aggregation check applies equally to all justice claims, regardless of whether the counterparty can spend them immediately or must wait to spend via HTLC-Timeout.</p>\n<p>An attacker can exploit this buggy aggregation logic to make LDK create multiple claim transactions, as described above.</p>\n<h2><a name=\"p-4089-locking-up-funds-7\" class=\"anchor\" href=\"#p-4089-locking-up-funds-7\"></a>Locking Up Funds</h2>\n<p>Finally, the attacker broadcasts and confirms a transaction spending HTLCs 1 and 2 via HTLC-Success.\nThe attacker\u2019s transaction conflicts with both Transaction 1 and Transaction 2, but due to the invalid claims bug, LDK only notices the conflict with Transaction 1.\nLDK continues to fee bump and rebroadcast Transaction 2 indefinitely, even though it can never be mined.</p>\n<p>As a result, the funds in HTLC 3 remain inaccessible until a valid claim transaction is manually constructed and broadcast.</p>\n<p>Note that if the attacker ever tries to claim HTLC 3 via HTLC-Success, LDK is able to immediately recover it via the revocation key.\nSo while the attacker can lock up HTLC 3, they cannot actually steal it once the upstream HTLC times out.</p>\n<h2><a name=\"p-4089-attack-cost-8\" class=\"anchor\" href=\"#p-4089-attack-cost-8\"></a>Attack Cost</h2>\n<p>When the attacker\u2019s revoked commitment transaction confirms, LDK is able to immediately claim the attacker\u2019s channel balance.\nLDK is also able to claim HTLCs 1 and 2 via the revocation key on the <code>B-M</code> channel, while also claiming them via the preimage on the upstream <code>A-B</code> channel.</p>\n<p>Thus a smart attacker would minimize costs by spending their channel balance down to the 1% reserve before carrying out the attack and would then set the amounts of HTLCs 1 and 2 to just above the dust threshold.\nThe attacker would also maximize the pain inflicted on the victim by setting HTLC 3 to the maximum allowed amount.</p>\n<h1><a name=\"p-4089-stealing-htlcs-in-01-beta-9\" class=\"anchor\" href=\"#p-4089-stealing-htlcs-in-01-beta-9\"></a>Stealing HTLCs in 0.1-beta</h1>\n<p>Beginning in <a href=\"https://github.com/lightningdevkit/rust-lightning/releases/tag/v0.1.0-beta1\" rel=\"noopener nofollow ugc\">v0.1-beta</a>, LDK <a href=\"https://github.com/lightningdevkit/rust-lightning/pull/3340\" rel=\"noopener nofollow ugc\">started</a> aggregating HTLC timeout claims that have compatible locktimes.\nAs a result, the beta release is vulnerable to a variant of the liquidity griefing attack that enables the attacker to steal funds.\nThankfully the invalid claims bug was fixed between the 0.1-beta and 0.1 releases, so the final LDK 0.1 release is not vulnerable to this attack.</p>\n<p>The fund-stealing variant for LDK 0.1-beta works as follows.</p>\n<h2><a name=\"p-4089-setup-10\" class=\"anchor\" href=\"#p-4089-setup-10\"></a>Setup</h2>\n<p>The attack setup is identical to the liquidity griefing attack, except that the attacker does not cause its commitment transaction to be revoked.</p>\n<h2><a name=\"p-4089-forcing-multiple-aggregations-11\" class=\"anchor\" href=\"#p-4089-forcing-multiple-aggregations-11\"></a>Forcing Multiple Aggregations</h2>\n<p>The attacker then force closes the <code>B-M</code> channel.\nDue to differing locktimes, LDK creates HTLC timeout claims as follows:</p>\n<ul>\n<li><strong>Transaction 1:</strong>  HTLC 1  (locktime <code>X</code>)</li>\n<li><strong>Transaction 2:</strong>  HTLCs 2 and 3  (locktime <code>X+1</code>)</li>\n</ul>\n<p>Once height <code>X</code> is reached, LDK broadcasts Transaction 1.\nAt height <code>X+1</code>, LDK broadcasts Transaction 2.</p>\n<p>At this point, if Transaction 1 confirmed immediately in block <code>X+1</code>, the attack fails since the attacker can no longer spend HTLCs 1 and 2 together in the same transaction.\nBut if Transaction 1 did not confirm immediately (which is more likely), the attack can continue.</p>\n<h2><a name=\"p-4089-stealing-funds-12\" class=\"anchor\" href=\"#p-4089-stealing-funds-12\"></a>Stealing Funds</h2>\n<p>The attacker broadcasts and confirms a transaction spending HTLCs 1 and 2 via HTLC-Success.\nThis transaction conflicts with both Transaction 1 and Transaction 2, but due to the invalid claims bug, LDK only notices the conflict with Transaction 1.\nLDK continues to fee bump and rebroadcast Transaction 2 indefinitely, even though it can never be mined.</p>\n<p>Once HTLC 3\u2019s upstream timeout expires, node <code>A</code> force closes and claims a refund, leaving the coast clear for the attacker to claim the downstream HTLC via preimage.</p>\n<h1><a name=\"p-4089-the-fix-13\" class=\"anchor\" href=\"#p-4089-the-fix-13\"></a>The Fix</h1>\n<p>The invalid claims bug was fixed by a <a href=\"https://github.com/lightningdevkit/rust-lightning/pull/3538\" rel=\"noopener nofollow ugc\">one-line patch</a> just prior to the LDK 0.1 release.</p>\n<h1><a name=\"p-4089-discovery-14\" class=\"anchor\" href=\"#p-4089-discovery-14\"></a>Discovery</h1>\n<p>This vulnerability was discovered during an audit of LDK\u2019s chain module.</p>\n<h2><a name=\"p-4089-timeline-15\" class=\"anchor\" href=\"#p-4089-timeline-15\"></a>Timeline</h2>\n<ul>\n<li><strong>2024-12-23:</strong> Vulnerability reported to the LDK security mailing list.</li>\n<li><strong>2025-01-15:</strong> Fix <a href=\"https://github.com/lightningdevkit/rust-lightning/pull/3538\" rel=\"noopener nofollow ugc\">merged</a>.</li>\n<li><strong>2025-01-16:</strong> LDK 0.1 released containing the fix, with public disclosure in release notes.</li>\n<li><strong>2025-01-23:</strong> Detailed description of vulnerability published.</li>\n</ul>\n<h1><a name=\"p-4089-prevention-16\" class=\"anchor\" href=\"#p-4089-prevention-16\"></a>Prevention</h1>\n<p>The invalid claims bug is fundamentally a problem of incorrect control flow \u2013 a <code>break</code> statement was inserted into a loop where it shouldn\u2019t have been.\nWhy wasn\u2019t it caught during initial code review, and why wasn\u2019t it noticed for years after that?</p>\n<p>The <code>break</code> statement was <a href=\"https://github.com/lightningdevkit/rust-lightning/commit/feb472dc9ef971b926b19d27e1ad05a79423778f\" rel=\"noopener nofollow ugc\">introduced</a> back in 2019, long before LDK supported anchor channels.\nThe code was actually correct back then, because before anchor channels there was no way for the counterparty to construct a transaction that conflicted with two of LDK\u2019s aggregated transactions.\nBut even after <a href=\"https://github.com/lightningdevkit/rust-lightning/releases/tag/v0.0.116\" rel=\"noopener nofollow ugc\">LDK 0.0.116</a> added support for anchor channels, the bug went unnoticed for over two years, despite multiple changes being made to the surrounding code in that time frame.</p>\n<p>It\u2019s impossible to say exactly what kept the bug hidden, but I think the complexity and unreadability of the surrounding code was a likely contributor.\nHere\u2019s the for-loop containing the <a href=\"https://github.com/lightningdevkit/rust-lightning/blob/ad462bd9c8237e505f463c227c9ac98ebd3fbb16/lightning/src/chain/onchaintx.rs#L896-L984\" rel=\"noopener nofollow ugc\">buggy code</a>:</p>\n<pre data-code-wrap=\"rust\"><code class=\"lang-rust\">let mut bump_candidates = new_hash_map();\nif !txn_matched.is_empty() { maybe_log_intro(); }\nfor tx in txn_matched {\n    // Scan all input to verify is one of the outpoint spent is of interest for us\n    let mut claimed_outputs_material = Vec::new();\n    for inp in &amp;tx.input {\n        if let Some((claim_id, _)) = self.claimable_outpoints.get(&amp;inp.previous_output) {\n            // If outpoint has claim request pending on it...\n            if let Some(request) = self.pending_claim_requests.get_mut(claim_id) {\n                //... we need to check if the pending claim was for a subset of the outputs\n                // spent by the confirmed transaction. If so, we can drop the pending claim\n                // after ANTI_REORG_DELAY blocks, otherwise we need to split it and retry\n                // claiming the remaining outputs.\n                let mut is_claim_subset_of_tx = true;\n                let mut tx_inputs = tx.input.iter().map(|input| &amp;input.previous_output).collect::&lt;Vec&lt;_&gt;&gt;();\n                tx_inputs.sort_unstable();\n                for request_input in request.outpoints() {\n                    if tx_inputs.binary_search(&amp;request_input).is_err() {\n                        is_claim_subset_of_tx = false;\n                        break;\n                    }\n                }\n\n                macro_rules! clean_claim_request_after_safety_delay {\n                    () =&gt; {\n                        let entry = OnchainEventEntry {\n                            txid: tx.compute_txid(),\n                            height: conf_height,\n                            block_hash: Some(conf_hash),\n                            event: OnchainEvent::Claim { claim_id: *claim_id }\n                        };\n                        if !self.onchain_events_awaiting_threshold_conf.contains(&amp;entry) {\n                            self.onchain_events_awaiting_threshold_conf.push(entry);\n                        }\n                    }\n                }\n\n                // If this is our transaction (or our counterparty spent all the outputs\n                // before we could anyway with same inputs order than us), wait for\n                // ANTI_REORG_DELAY and clean the RBF tracking map.\n                if is_claim_subset_of_tx {\n                    clean_claim_request_after_safety_delay!();\n                } else { // If false, generate new claim request with update outpoint set\n                    let mut at_least_one_drop = false;\n                    for input in tx.input.iter() {\n                        if let Some(package) = request.split_package(&amp;input.previous_output) {\n                            claimed_outputs_material.push(package);\n                            at_least_one_drop = true;\n                        }\n                        // If there are no outpoints left to claim in this request, drop it entirely after ANTI_REORG_DELAY.\n                        if request.outpoints().is_empty() {\n                            clean_claim_request_after_safety_delay!();\n                        }\n                    }\n                    //TODO: recompute soonest_timelock to avoid wasting a bit on fees\n                    if at_least_one_drop {\n                        bump_candidates.insert(*claim_id, request.clone());\n                        // If we have any pending claim events for the request being updated\n                        // that have yet to be consumed, we'll remove them since they will\n                        // end up producing an invalid transaction by double spending\n                        // input(s) that already have a confirmed spend. If such spend is\n                        // reorged out of the chain, then we'll attempt to re-spend the\n                        // inputs once we see it.\n                        #[cfg(debug_assertions)] {\n                            let existing = self.pending_claim_events.iter()\n                                .filter(|entry| entry.0 == *claim_id).count();\n                            assert!(existing == 0 || existing == 1);\n                        }\n                        self.pending_claim_events.retain(|entry| entry.0 != *claim_id);\n                    }\n                }\n                break; //No need to iterate further, either tx is our or their\n            } else {\n                panic!(\"Inconsistencies between pending_claim_requests map and claimable_outpoints map\");\n            }\n        }\n    }\n    for package in claimed_outputs_material.drain(..) {\n        let entry = OnchainEventEntry {\n            txid: tx.compute_txid(),\n            height: conf_height,\n            block_hash: Some(conf_hash),\n            event: OnchainEvent::ContentiousOutpoint { package },\n        };\n        if !self.onchain_events_awaiting_threshold_conf.contains(&amp;entry) {\n            self.onchain_events_awaiting_threshold_conf.push(entry);\n        }\n    }\n}\n</code></pre>\n<p><br><br></p>\n<p>Perhaps others have a better mental parser than me, but I find this code quite difficult to read and understand.\nThe loop is so long, with so much nesting and so many low-level implementation details that by the time I get to the buggy <code>break</code> statement, I\u2019ve completely forgotten what loop it applies to.\nAnd since the comment attached to the break statement gives a believable explanation, it\u2019s easy to gloss right over it.</p>\n<p>Perhaps the buggy control flow would be easier to spot if the loop was simpler and more compact.\nBy hand-waving some helper functions into existence and refactoring, the same code could be written as follows:</p>\n<pre data-code-wrap=\"rust\"><code class=\"lang-rust\">maybe_log_intro();\n\nlet mut bump_candidates = new_hash_map();\nfor tx in txn_matched {\n    for inp in &amp;tx.input {\n        if let Some(claim_request) = self.get_mut_claim_request_from_outpoint(inp.previous_output) {\n            let split_requests = claim_request.split_off_matching_inputs(&amp;tx.input);\n            debug_assert!(!split_requests.is_empty());\n\n            if claim_request.outpoints().is_empty() {\n                // Request has been fully claimed.\n                self.mark_request_claimed(claim_request, tx, conf_height, conf_hash);\n                break;\n            }\n\n            // After removing conflicting inputs, there's still more to claim.  Add the modified\n            // request to bump_candidates so it gets fee bumped and rebroadcast.\n            self.remove_pending_claim_events(claim_request);\n            bump_candidates.insert(claim_request.clone());\n\n            self.mark_requests_contentious(split_requests, tx, conf_height, conf_hash);\n            break;\n        }\n    }\n</code></pre>\n<p>The control flow in this version is much more apparent to the reader.\nAnd although there\u2019s no guarantee that the buggy <code>break</code> statements would have been discovered sooner if the code was written this way, I do think the odds would have been much better.</p>\n<h1><a name=\"p-4089-takeaways-17\" class=\"anchor\" href=\"#p-4089-takeaways-17\"></a>Takeaways</h1>\n<ul>\n<li>Code readability matters for preventing bugs.</li>\n<li>Update to LDK 0.1 for the vulnerability fix.</li>\n</ul>",
  "post_number": 1,
  "post_type": 1,
  "updated_at": "2025-01-23T16:08:59.376Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 5,
  "reads": 22,
  "readers_count": 21,
  "score": 68.8,
  "yours": false,
  "topic_id": 1400,
  "topic_slug": "disclosure-ldk-invalid-claims-liquidity-griefing",
  "topic_title": "Disclosure: LDK Invalid Claims Liquidity Griefing",
  "topic_html_title": "Disclosure: LDK Invalid Claims Liquidity Griefing",
  "category_id": 8,
  "display_username": "Matt Morehouse",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "*The following disclosure is copied verbatim from a [blog post](https://morehouse.github.io/lightning/ldk-invalid-claims-liquidity-griefing/) on morehouse.github.io, reproduced here to facilitate discussion.*\n\nLDK 0.0.125 and below are vulnerable to a liquidity griefing attack against anchor channels.\nThe attack locks up funds such that they can only be recovered by manually constructing and broadcasting a valid claim transaction.\nAffected users can unlock their funds by upgrading to [LDK 0.1](https://github.com/lightningdevkit/rust-lightning/releases/tag/v0.1) and replaying the sequence of commitment and HTLC transactions that led to the lock up.\n\n# Background\n\nWhen a channel is force closed, LDK creates and broadcasts transactions to claim any HTLCs it can from the commitment transaction that confirmed on chain.\nTo save on fees, some HTLC claims are aggregated and broadcast together in the same transaction.\n\nIf the channel counterparty is able to get a competing HTLC claim confirmed first, it can cause one of LDK's aggregated transactions to become invalid, since the corresponding HTLC input has already been spent by the counterparty's claim.\nLDK contains logic to detect this scenario and remove the already-claimed input from its aggregated claim transaction.\nWhen everything works correctly, the aggregated transaction becomes valid again and LDK is able to claim the remaining HTLCs.\n\n# The Invalid Claims Bug\n\nPrior to LDK 0.1, the logic to detect conflicting claims works like this:\n\n```python\nfor confirmed_transaction in confirmed_block:\n  for input in confirmed_transaction:\n    if claimable_outpoints.contains(input.prevout):\n      agg_tx = get_aggregated_transaction_from_outpoint(input.prevout)\n      agg_tx.remove_matching_inputs(confirmed_transaction)\n      break  # This is the bug.\n```\n\nNote that this logic stops processing a confirmed transaction after finding the first aggregated transaction that conflicts with it.\nIf the confirmed transaction conflicts with *multiple* aggregated transactions, conflicting inputs are only removed from the *first* matching aggregated transaction, and any other conflicting aggregated transactions are left invalid.\n\nAny HTLCs claimed by invalid aggregated transactions get locked up and can only be recovered by manually constructing and broadcasting valid claim transactions.\n\n# Liquidity Griefing\n\nPrior to LDK 0.1, there are only two types of HTLC claims that are aggregated:\n\n- HTLC preimage claims\n- revoked commitment HTLC claims\n\nFor HTLC preimage claims, LDK takes care to confirm them before their HTLCs time out, so there's no reliable way for an attacker to confirm a conflicting timeout claim and trigger the invalid claims bug.\n\nFor revoked commitment transactions, however, an attacker can immediately spend any incoming HTLC outputs via HTLC-Success transactions.\nAlthough LDK is then able to claim the HTLC-Success outputs via the revocation key, the attacker can exploit the invalid claims bug to lock up any remaining HTLCs on the revoked commitment transaction.\n\n## Setup\n\nThe attacker opens an anchor channel with the victim, creating a network topology as follows:\n```\nA -- B -- M\n```\nIn this case `B` is the victim LDK node and `M` is the node controlled by the attacker.\nThe attacker must use an anchor channel so that they can spend multiple HTLC claims in the same transaction and trigger the invalid claims bug.\n\nThe attacker then routes HTLCs along the path `A->B->M` as follows:\n\n1. 1 small HTLC with CLTV of `X`\n2. 1 small HTLC with CLTV of `X+1`\n3. 1 large HTLC with CLTV of `X+1`  (this is the one the attacker will lock up)\n\nThe attacker knows preimages for all HTLCs but withholds them for now.\n\nTo complete the setup, the attacker routes some other HTLC through the channel, causing the commitment transaction with the above HTLCs to be revoked.\n\n## Forcing Multiple Aggregations\n\nNext the attacker waits until block `X-13` and force closes the `B-M` channel using their revoked commitment transaction, being sure to get it confirmed in block `X-12`.\nBy confirming in this specific block, the attacker can exploit LDK's buggy aggregation logic prior to v0.1 (see below), causing LDK to aggregate HTLC justice claims as follows:\n\n- **Transaction 1:**  HTLC 1\n- **Transaction 2:**  HTLCs 2 and 3\n\n### Buggy Aggregation Logic\n\nPrior to v0.1, LDK only aggregates HTLC claims if their timeouts are more than 12 blocks in the future.\nPresumably 12 blocks was deemed \"too soon\" to guarantee that LDK can confirm preimage claims before the HTLCs time out, and once one HTLC times out the counterparty can pin a competing timeout claim in mempools, thereby preventing confirmation of *all* the aggregated preimage claims.\nIn other words, by claiming HTLCs separately in this scenario, LDK limits the damage the counterparty could do if one of those HTLCs expires before LDK successfully claims it.\n\nUnfortunately, this aggregation strategy makes no sense when LDK is trying to group justice claims that the counterparty can spend immediately via HTLC-Success, since the timeout on those HTLCs does not apply to the counterparty.\nNevertheless, prior to LDK 0.1, the same 12 block aggregation check applies equally to all justice claims, regardless of whether the counterparty can spend them immediately or must wait to spend via HTLC-Timeout.\n\nAn attacker can exploit this buggy aggregation logic to make LDK create multiple claim transactions, as described above.\n\n## Locking Up Funds\n\nFinally, the attacker broadcasts and confirms a transaction spending HTLCs 1 and 2 via HTLC-Success.\nThe attacker's transaction conflicts with both Transaction 1 and Transaction 2, but due to the invalid claims bug, LDK only notices the conflict with Transaction 1.\nLDK continues to fee bump and rebroadcast Transaction 2 indefinitely, even though it can never be mined.\n\nAs a result, the funds in HTLC 3 remain inaccessible until a valid claim transaction is manually constructed and broadcast.\n\nNote that if the attacker ever tries to claim HTLC 3 via HTLC-Success, LDK is able to immediately recover it via the revocation key.\nSo while the attacker can lock up HTLC 3, they cannot actually steal it once the upstream HTLC times out.\n\n## Attack Cost\n\nWhen the attacker's revoked commitment transaction confirms, LDK is able to immediately claim the attacker's channel balance.\nLDK is also able to claim HTLCs 1 and 2 via the revocation key on the `B-M` channel, while also claiming them via the preimage on the upstream `A-B` channel.\n\nThus a smart attacker would minimize costs by spending their channel balance down to the 1% reserve before carrying out the attack and would then set the amounts of HTLCs 1 and 2 to just above the dust threshold.\nThe attacker would also maximize the pain inflicted on the victim by setting HTLC 3 to the maximum allowed amount.\n\n# Stealing HTLCs in 0.1-beta\n\nBeginning in [v0.1-beta](https://github.com/lightningdevkit/rust-lightning/releases/tag/v0.1.0-beta1), LDK [started](https://github.com/lightningdevkit/rust-lightning/pull/3340) aggregating HTLC timeout claims that have compatible locktimes.\nAs a result, the beta release is vulnerable to a variant of the liquidity griefing attack that enables the attacker to steal funds.\nThankfully the invalid claims bug was fixed between the 0.1-beta and 0.1 releases, so the final LDK 0.1 release is not vulnerable to this attack.\n\nThe fund-stealing variant for LDK 0.1-beta works as follows.\n\n## Setup\n\nThe attack setup is identical to the liquidity griefing attack, except that the attacker does not cause its commitment transaction to be revoked.\n\n## Forcing Multiple Aggregations\n\nThe attacker then force closes the `B-M` channel.\nDue to differing locktimes, LDK creates HTLC timeout claims as follows:\n\n- **Transaction 1:**  HTLC 1  (locktime `X`)\n- **Transaction 2:**  HTLCs 2 and 3  (locktime `X+1`)\n\nOnce height `X` is reached, LDK broadcasts Transaction 1.\nAt height `X+1`, LDK broadcasts Transaction 2.\n\nAt this point, if Transaction 1 confirmed immediately in block `X+1`, the attack fails since the attacker can no longer spend HTLCs 1 and 2 together in the same transaction.\nBut if Transaction 1 did not confirm immediately (which is more likely), the attack can continue.\n\n## Stealing Funds\n\nThe attacker broadcasts and confirms a transaction spending HTLCs 1 and 2 via HTLC-Success.\nThis transaction conflicts with both Transaction 1 and Transaction 2, but due to the invalid claims bug, LDK only notices the conflict with Transaction 1.\nLDK continues to fee bump and rebroadcast Transaction 2 indefinitely, even though it can never be mined.\n\nOnce HTLC 3's upstream timeout expires, node `A` force closes and claims a refund, leaving the coast clear for the attacker to claim the downstream HTLC via preimage.\n\n# The Fix\n\nThe invalid claims bug was fixed by a [one-line patch](https://github.com/lightningdevkit/rust-lightning/pull/3538) just prior to the LDK 0.1 release.\n\n# Discovery\n\nThis vulnerability was discovered during an audit of LDK's chain module.\n\n## Timeline\n\n- **2024-12-23:** Vulnerability reported to the LDK security mailing list.\n- **2025-01-15:** Fix [merged](https://github.com/lightningdevkit/rust-lightning/pull/3538).\n- **2025-01-16:** LDK 0.1 released containing the fix, with public disclosure in release notes.\n- **2025-01-23:** Detailed description of vulnerability published.\n\n# Prevention\n\nThe invalid claims bug is fundamentally a problem of incorrect control flow -- a `break` statement was inserted into a loop where it shouldn't have been.\nWhy wasn't it caught during initial code review, and why wasn't it noticed for years after that?\n\nThe `break` statement was [introduced](https://github.com/lightningdevkit/rust-lightning/commit/feb472dc9ef971b926b19d27e1ad05a79423778f) back in 2019, long before LDK supported anchor channels.\nThe code was actually correct back then, because before anchor channels there was no way for the counterparty to construct a transaction that conflicted with two of LDK's aggregated transactions.\nBut even after [LDK 0.0.116](https://github.com/lightningdevkit/rust-lightning/releases/tag/v0.0.116) added support for anchor channels, the bug went unnoticed for over two years, despite multiple changes being made to the surrounding code in that time frame.\n\nIt's impossible to say exactly what kept the bug hidden, but I think the complexity and unreadability of the surrounding code was a likely contributor.\nHere's the for-loop containing the [buggy code](https://github.com/lightningdevkit/rust-lightning/blob/ad462bd9c8237e505f463c227c9ac98ebd3fbb16/lightning/src/chain/onchaintx.rs#L896-L984):\n\n```rust\nlet mut bump_candidates = new_hash_map();\nif !txn_matched.is_empty() { maybe_log_intro(); }\nfor tx in txn_matched {\n    // Scan all input to verify is one of the outpoint spent is of interest for us\n    let mut claimed_outputs_material = Vec::new();\n    for inp in &tx.input {\n        if let Some((claim_id, _)) = self.claimable_outpoints.get(&inp.previous_output) {\n            // If outpoint has claim request pending on it...\n            if let Some(request) = self.pending_claim_requests.get_mut(claim_id) {\n                //... we need to check if the pending claim was for a subset of the outputs\n                // spent by the confirmed transaction. If so, we can drop the pending claim\n                // after ANTI_REORG_DELAY blocks, otherwise we need to split it and retry\n                // claiming the remaining outputs.\n                let mut is_claim_subset_of_tx = true;\n                let mut tx_inputs = tx.input.iter().map(|input| &input.previous_output).collect::<Vec<_>>();\n                tx_inputs.sort_unstable();\n                for request_input in request.outpoints() {\n                    if tx_inputs.binary_search(&request_input).is_err() {\n                        is_claim_subset_of_tx = false;\n                        break;\n                    }\n                }\n\n                macro_rules! clean_claim_request_after_safety_delay {\n                    () => {\n                        let entry = OnchainEventEntry {\n                            txid: tx.compute_txid(),\n                            height: conf_height,\n                            block_hash: Some(conf_hash),\n                            event: OnchainEvent::Claim { claim_id: *claim_id }\n                        };\n                        if !self.onchain_events_awaiting_threshold_conf.contains(&entry) {\n                            self.onchain_events_awaiting_threshold_conf.push(entry);\n                        }\n                    }\n                }\n\n                // If this is our transaction (or our counterparty spent all the outputs\n                // before we could anyway with same inputs order than us), wait for\n                // ANTI_REORG_DELAY and clean the RBF tracking map.\n                if is_claim_subset_of_tx {\n                    clean_claim_request_after_safety_delay!();\n                } else { // If false, generate new claim request with update outpoint set\n                    let mut at_least_one_drop = false;\n                    for input in tx.input.iter() {\n                        if let Some(package) = request.split_package(&input.previous_output) {\n                            claimed_outputs_material.push(package);\n                            at_least_one_drop = true;\n                        }\n                        // If there are no outpoints left to claim in this request, drop it entirely after ANTI_REORG_DELAY.\n                        if request.outpoints().is_empty() {\n                            clean_claim_request_after_safety_delay!();\n                        }\n                    }\n                    //TODO: recompute soonest_timelock to avoid wasting a bit on fees\n                    if at_least_one_drop {\n                        bump_candidates.insert(*claim_id, request.clone());\n                        // If we have any pending claim events for the request being updated\n                        // that have yet to be consumed, we'll remove them since they will\n                        // end up producing an invalid transaction by double spending\n                        // input(s) that already have a confirmed spend. If such spend is\n                        // reorged out of the chain, then we'll attempt to re-spend the\n                        // inputs once we see it.\n                        #[cfg(debug_assertions)] {\n                            let existing = self.pending_claim_events.iter()\n                                .filter(|entry| entry.0 == *claim_id).count();\n                            assert!(existing == 0 || existing == 1);\n                        }\n                        self.pending_claim_events.retain(|entry| entry.0 != *claim_id);\n                    }\n                }\n                break; //No need to iterate further, either tx is our or their\n            } else {\n                panic!(\"Inconsistencies between pending_claim_requests map and claimable_outpoints map\");\n            }\n        }\n    }\n    for package in claimed_outputs_material.drain(..) {\n        let entry = OnchainEventEntry {\n            txid: tx.compute_txid(),\n            height: conf_height,\n            block_hash: Some(conf_hash),\n            event: OnchainEvent::ContentiousOutpoint { package },\n        };\n        if !self.onchain_events_awaiting_threshold_conf.contains(&entry) {\n            self.onchain_events_awaiting_threshold_conf.push(entry);\n        }\n    }\n}\n```\n\n<br/><br/>\n\nPerhaps others have a better mental parser than me, but I find this code quite difficult to read and understand.\nThe loop is so long, with so much nesting and so many low-level implementation details that by the time I get to the buggy `break` statement, I've completely forgotten what loop it applies to.\nAnd since the comment attached to the break statement gives a believable explanation, it's easy to gloss right over it.\n\nPerhaps the buggy control flow would be easier to spot if the loop was simpler and more compact.\nBy hand-waving some helper functions into existence and refactoring, the same code could be written as follows:\n\n```rust\nmaybe_log_intro();\n\nlet mut bump_candidates = new_hash_map();\nfor tx in txn_matched {\n    for inp in &tx.input {\n        if let Some(claim_request) = self.get_mut_claim_request_from_outpoint(inp.previous_output) {\n            let split_requests = claim_request.split_off_matching_inputs(&tx.input);\n            debug_assert!(!split_requests.is_empty());\n\n            if claim_request.outpoints().is_empty() {\n                // Request has been fully claimed.\n                self.mark_request_claimed(claim_request, tx, conf_height, conf_hash);\n                break;\n            }\n\n            // After removing conflicting inputs, there's still more to claim.  Add the modified\n            // request to bump_candidates so it gets fee bumped and rebroadcast.\n            self.remove_pending_claim_events(claim_request);\n            bump_candidates.insert(claim_request.clone());\n\n            self.mark_requests_contentious(split_requests, tx, conf_height, conf_hash);\n            break;\n        }\n    }\n```\n\nThe control flow in this version is much more apparent to the reader.\nAnd although there's no guarantee that the buggy `break` statements would have been discovered sooner if the code was written this way, I do think the odds would have been much better.\n\n# Takeaways\n\n- Code readability matters for preventing bugs.\n- Update to LDK 0.1 for the vulnerability fix.",
  "actions_summary": [
    {
      "id": 2,
      "count": 3
    }
  ],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 47,
  "hidden": false,
  "trust_level": 2,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "reactions": [
    {
      "id": "clap",
      "type": "emoji",
      "count": 3
    }
  ],
  "current_user_reaction": null,
  "reaction_users_count": 3,
  "current_user_used_main_reaction": false
}