{
  "id": 4109,
  "name": "Pieter Wuille",
  "username": "sipa",
  "avatar_template": "/user_avatar/delvingbitcoin.org/sipa/{size}/102_2.png",
  "created_at": "2025-01-29T14:05:48.215Z",
  "cooked": "<p>Hi <a class=\"mention\" href=\"/u/stefanwouldgo\">@stefanwouldgo</a>, crazy! I see a vague relation with min-cut / max-flow problems, but would never had thought to look in this direction.</p>\n<aside class=\"quote no-group\" data-username=\"stefanwouldgo\" data-post=\"9\" data-topic=\"303\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/stefanwouldgo/48/664_2.png\" class=\"avatar\"> stefanwouldgo:</div>\n<blockquote>\n<p>Finding a highest-feerate topologically-valid subset is possible in <span class=\"math\">O(nm \\log (n^2/m))</span> time</p>\n</blockquote>\n</aside>\n<p>I haven\u2019t been able to find the publication yet, based on what I find for related algorithm, but I assume <span class=\"math\">n</span> is the number of nodes (transactions), and <span class=\"math\">m</span> is the number of arcs (dependencies)? If so, that sounds great if it is practical as well. Note that the number of dependencies itself may be up to quadratic in the number of transactions, so this is essentially cubic in <span class=\"math\">n</span>?</p>\n<p>In the abstract of <a href=\"https://pubsonline.informs.org/doi/10.1287/opre.37.5.748\">this paper</a>, an additional condition is present (emphasis mine):</p>\n<blockquote>\n<p>We present a simple sequential algorithm for the maximum flow problem on a network with <span class=\"math\">n</span> nodes, <span class=\"math\">m</span> arcs, and integer arc capacities bounded by <span class=\"math\">U</span>. Under the practical assumption that <strong><em>U</em> is polynomially bounded in <em>n</em></strong>, our algorithm runs in time <span class=\"math\">O(nm + n^2 \\log n)</span>.</p>\n</blockquote>\n<p>It\u2019s not clear if that condition is also present in the algorithm you\u2019re citing, but if it is, that may be a problem.</p>\n<aside class=\"quote no-group\" data-username=\"stefanwouldgo\" data-post=\"9\" data-topic=\"303\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/stefanwouldgo/48/664_2.png\" class=\"avatar\"> stefanwouldgo:</div>\n<blockquote>\n<p>the only difference being the direction of the arrows in the graph.</p>\n</blockquote>\n</aside>\n<p>Difference between what?</p>\n<blockquote>\n<p>This seems great news to me. It should mean that we can accomodate much larger clusters.</p>\n</blockquote>\n<p>Maybe. The cluster size bound is really informed by how big of a cluster we can linearize with \u201cacceptable\u201d quality (scare-quotes, because it isn\u2019t all that clear what acceptable should mean), not what we can linearize optimally, in an extremely small amount of time (we\u2019ve imposed a 50 \u00b5s ourselves, because a single transaction may simultaneously affect many clusters, which would all require re-linearization).</p>\n<p>So far, we\u2019ve interpreted acceptable as \u201cat least as good as what we had before\u201d (through <a href=\"https://delvingbitcoin.org/t/limo-combining-the-best-parts-of-linearization-search-and-merging/825\">LIMO</a>), and \u201cat least as good as ancestor sort\u201d (the current CPFP-accomodating mining algorithm, which is inherently <span class=\"math\">O(n^2)</span>. But (so far) all the ideas in this thread are \u201cextra\u201d, in the sense that they\u2019d only be applied when we have time left, or in a background re-linearization process, that does not act until after transaction relay.</p>\n<p>Now, it is quite possible that this algoritm is <em>so</em> fast that it can linearize larger clusters <em>optimally</em> in the time that ancestor sort + LIMO may need for smaller ones. That would obviously move the needle. Or if it\u2019s possible that a reasonable argument can be made that a time-bounded version of this algorithm (only performing a lower-than-optimal number of cuts, for example, and then stopping) results in something that is practically as good as ancestor-sort (e.g., sufficient to make CPFP in typical, but potentially adverserial, scenarios work).</p>\n<aside class=\"quote no-group\" data-username=\"stefanwouldgo\" data-post=\"9\" data-topic=\"303\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/stefanwouldgo/48/664_2.png\" class=\"avatar\"> stefanwouldgo:</div>\n<blockquote>\n<p>The alternative would be to write the push-relabel parameterized min-cut algorithm from scratch, but as I said, this is not trivial.</p>\n</blockquote>\n</aside>\n<p>That\u2019s a concern, because we don\u2019t just care about asymptotic complexity, but real performance for relatively small problems too. And complicated algorithms, with complicated data structures, tend to result in things with high start-up costs (to convert the problem into the right representation), or with high constant factors (e.g., for some small problems, (sorted) lists can be several times faster than hash maps).</p>\n<hr>\n<p>This is probably the point where I should reveal that we\u2019ve been working on a new cluster linearization algorithm too (which I will write a bigger post about in time). It\u2019s a result of a conversation we had at the Bitcoin Research Week, where Dongning Guo and Aviv Zohar pointed out that the \u201cfind highest-feerate topologically-valid subset\u201d problem can be formulated as a Linear Programming problem, on which all LP solving methods are applicable. This implies two things:</p>\n<ul>\n<li>Through <a href=\"https://en.wikipedia.org/wiki/Interior-point_method\">Interior-Point Methods</a>, the problem can be solved in <span class=\"math\">O((n+m)^{2.5} \\log S)</span> time, where <span class=\"math\">S</span> is the sum of transaction sizes, or essentially <span class=\"math\">O(n^5)</span> in just <span class=\"math\">n</span>.</li>\n<li>The simplex algorithm, while worst-case exponential in the general case but practically very fast, becomes applicable. And it is possible that for our specific problem, these exponential cases don\u2019t exist as well.</li>\n</ul>\n<p>Inspired by this last point, by observing what the simplex algorithm \u201csteps\u201d translate to in our problem space, in terms of sets and fees and feerates rather than matrix row-operations, removing some apparently useless transitions, and observing that it effectively finds a full linearization rather than just a single topologically-valid highest-feerate subset, we obtain the following:</p>\n<ul>\n<li>Input: <span class=\"math\">n</span> transactions with fees and sizes, and <span class=\"math\">m</span> dependencies between them (as <span class=\"math\">(parent, child)</span> pairs).</li>\n<li>Output: a list of sets (the chunks), forming a graph partition, with all the consecutive highest-feerate topologically-valid subsets of what remains after the previous ones.</li>\n<li>Algorithm:\n<ul>\n<li>For every dependency, have a boolean \u201cactive\u201d; initially all dependencies are inactive.\n<ul>\n<li>These partition the graph into components (when two transactions are reachable from one another by travelling only over active dependencies, up or down, they are in the same component).</li>\n<li>Thus, the initial state has every transaction in its own singleton component.</li>\n<li>As an additional invariant, no (undirected) cycles of active dependencies are allowed, so the component\u2019s active dependencies form a spanning tree for that component. The entire state can thus be described as an (undirected) spanning forest, which is a subgraph of the problem graph.</li>\n</ul>\n</li>\n<li>Keep performing any of the following steps as long as any apply:\n<ul>\n<li>If a dependency is inactive, and is between two distinct components, and the \u201cchild\u201d component has higher feerate than the \u201cparent\u201d component, make it active.</li>\n<li>If a dependency is active, and making it inactive would split the component it is in in two components (due to no-cycles property, this is true for every active dependency), where the parent component has higher feerate than the child component, make it inactive.</li>\n</ul>\n</li>\n<li>Finally, output all the component in decreasing feerate order.</li>\n</ul>\n</li>\n</ul>\n<p>This appears to be very fast in practice, and easy to implement. Further, it can be proven that if it terminates, the result is indeed an optimal linearization. However, we don\u2019t have a proof it always terminates, and certainly no bound on its runtime. A lot seems to depend on the policy on <em>how</em> to pick which dependency to make active or inactive in every step, but this needs a lot more investigation.</p>",
  "post_number": 10,
  "post_type": 1,
  "updated_at": "2025-01-29T14:05:48.215Z",
  "reply_count": 1,
  "reply_to_post_number": null,
  "quote_count": 1,
  "incoming_link_count": 0,
  "reads": 2,
  "readers_count": 1,
  "score": 5.4,
  "yours": false,
  "topic_id": 303,
  "topic_slug": "how-to-linearize-your-cluster",
  "topic_title": "How to linearize your cluster",
  "topic_html_title": "How to linearize your cluster",
  "category_id": 8,
  "display_username": "Pieter Wuille",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "Hi @stefanwouldgo, crazy! I see a vague relation with min-cut / max-flow problems, but would never had thought to look in this direction.\n\n[quote=\"stefanwouldgo, post:9, topic:303\"]\nFinding a highest-feerate topologically-valid subset is possible in $O(nm \\log (n^2/m))$ time\n[/quote]\n\nI haven't been able to find the publication yet, based on what I find for related algorithm, but I assume $n$ is the number of nodes (transactions), and $m$ is the number of arcs (dependencies)? If so, that sounds great if it is practical as well. Note that the number of dependencies itself may be up to quadratic in the number of transactions, so this is essentially cubic in $n$?\n\nIn the abstract of [this paper](https://pubsonline.informs.org/doi/10.1287/opre.37.5.748), an additional condition is present (emphasis mine):\n\n> We present a simple sequential algorithm for the maximum flow problem on a network with $n$ nodes, $m$ arcs, and integer arc capacities bounded by $U$. Under the practical assumption that ***U* is polynomially bounded in *n***, our algorithm runs in time $O(nm + n^2 \\log n)$.\n\nIt's not clear if that condition is also present in the algorithm you're citing, but if it is, that may be a problem.\n\n[quote=\"stefanwouldgo, post:9, topic:303\"]\nthe only difference being the direction of the arrows in the graph.\n[/quote]\n\nDifference between what?\n\n> This seems great news to me. It should mean that we can accomodate much larger clusters.\n\nMaybe. The cluster size bound is really informed by how big of a cluster we can linearize with \"acceptable\" quality (scare-quotes, because it isn't all that clear what acceptable should mean), not what we can linearize optimally, in an extremely small amount of time (we've imposed a 50 \u00b5s ourselves, because a single transaction may simultaneously affect many clusters, which would all require re-linearization).\n\nSo far, we've interpreted acceptable as \"at least as good as what we had before\" (through [LIMO](https://delvingbitcoin.org/t/limo-combining-the-best-parts-of-linearization-search-and-merging/825)), and \"at least as good as ancestor sort\" (the current CPFP-accomodating mining algorithm, which is inherently $O(n^2)$. But (so far) all the ideas in this thread are \"extra\", in the sense that they'd only be applied when we have time left, or in a background re-linearization process, that does not act until after transaction relay.\n\nNow, it is quite possible that this algoritm is *so* fast that it can linearize larger clusters *optimally* in the time that ancestor sort + LIMO may need for smaller ones. That would obviously move the needle. Or if it's possible that a reasonable argument can be made that a time-bounded version of this algorithm (only performing a lower-than-optimal number of cuts, for example, and then stopping) results in something that is practically as good as ancestor-sort (e.g., sufficient to make CPFP in typical, but potentially adverserial, scenarios work).\n\n[quote=\"stefanwouldgo, post:9, topic:303\"]\nThe alternative would be to write the push-relabel parameterized min-cut algorithm from scratch, but as I said, this is not trivial.\n[/quote]\n\nThat's a concern, because we don't just care about asymptotic complexity, but real performance for relatively small problems too. And complicated algorithms, with complicated data structures, tend to result in things with high start-up costs (to convert the problem into the right representation), or with high constant factors (e.g., for some small problems, (sorted) lists can be several times faster than hash maps).\n\n---\n\nThis is probably the point where I should reveal that we've been working on a new cluster linearization algorithm too (which I will write a bigger post about in time). It's a result of a conversation we had at the Bitcoin Research Week, where Dongning Guo and Aviv Zohar pointed out that the \"find highest-feerate topologically-valid subset\" problem can be formulated as a Linear Programming problem, on which all LP solving methods are applicable. This implies two things:\n* Through [Interior-Point Methods](https://en.wikipedia.org/wiki/Interior-point_method), the problem can be solved in $O((n+m)^{2.5} \\log S)$ time, where $S$ is the sum of transaction sizes, or essentially $O(n^5)$ in just $n$.\n* The simplex algorithm, while worst-case exponential in the general case but practically very fast, becomes applicable. And it is possible that for our specific problem, these exponential cases don't exist as well.\n\nInspired by this last point, by observing what the simplex algorithm \"steps\" translate to in our problem space, in terms of sets and fees and feerates rather than matrix row-operations, removing some apparently useless transitions, and observing that it effectively finds a full linearization rather than just a single topologically-valid highest-feerate subset, we obtain the following:\n\n* Input: $n$ transactions with fees and sizes, and $m$ dependencies between them (as $(parent, child)$ pairs).\n* Output: a list of sets (the chunks), forming a graph partition, with all the consecutive highest-feerate topologically-valid subsets of what remains after the previous ones.\n* Algorithm:\n  * For every dependency, have a boolean \"active\"; initially all dependencies are inactive.\n    * These partition the graph into components (when two transactions are reachable from one another by travelling only over active dependencies, up or down, they are in the same component).\n    * Thus, the initial state has every transaction in its own singleton component.\n    * As an additional invariant, no (undirected) cycles of active dependencies are allowed, so the component's active dependencies form a spanning tree for that component. The entire state can thus be described as an (undirected) spanning forest, which is a subgraph of the problem graph.\n  * Keep performing any of the following steps as long as any apply:\n    * If a dependency is inactive, and is between two distinct components, and the \"child\" component has higher feerate than the \"parent\" component, make it active.\n    * If a dependency is active, and making it inactive would split the component it is in in two components (due to no-cycles property, this is true for every active dependency), where the parent component has higher feerate than the child component, make it inactive.\n  * Finally, output all the component in decreasing feerate order.\n\nThis appears to be very fast in practice, and easy to implement. Further, it can be proven that if it terminates, the result is indeed an optimal linearization. However, we don't have a proof it always terminates, and certainly no bound on its runtime. A lot seems to depend on the policy on *how* to pick which dependency to make active or inactive in every step, but this needs a lot more investigation.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 96,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false
}