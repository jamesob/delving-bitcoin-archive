{
  "id": 6440,
  "name": "Jonathan Harvey-Buschel",
  "username": "jonhbit",
  "avatar_template": "/user_avatar/delvingbitcoin.org/jonhbit/{size}/1661_2.png",
  "created_at": "2025-12-16T16:26:15.798Z",
  "cooked": "<aside class=\"quote no-group\" data-username=\"jpjuni0r\" data-post=\"17\" data-topic=\"2105\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jpjuni0r/48/1719_2.png\" class=\"avatar\"> jpjuni0r:</div>\n<blockquote>\n<p>Without derailing your discussion about the protocol specifics, I would like to understand exactly which problem the messaging based on sketches solves, and which will persist compared to the gossip approach.</p>\n</blockquote>\n</aside>\n<p>No problem, happy to have more feedback! There are some other problems I\u2019ve heard of across multiple anecdotes talking to implementers, that aren\u2019t really covered in prior literature (that I\u2019m aware of):</p>\n<ul>\n<li>For a well-connected node, like an lightning service provider (LSP) that sells channels, or a routing node trying to earn a profit, adding more P2P connections and accepting gossip messages from all peers increases CPU usage significantly without improving their view of the network. As a result, bigger nodes take on extra complexity in filtering/rejecting gossip from peers, and maintaining their network view with some secondary system.</li>\n<li>Even with how well-connected I expect the P2P network to be now, given the default number of connections implementations make (5+), there are reports of nodes missing messages related to entire subgraphs / neighborhoods of the payment network. So propagation of some messages may not be working reliably. This could also be caused by implementation-specific message filtering, that could be removed when moving to a sketch-based protocol.</li>\n<li>A <em>lot</em> of implementation complexity, across all implementations, concerning when to use gossip query messages to stay in sync with peers. As well as policies around when to forward gossip messages.</li>\n</ul>\n<aside class=\"quote no-group quote-modified\" data-username=\"jpjuni0r\" data-post=\"17\" data-topic=\"2105\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jpjuni0r/48/1719_2.png\" class=\"avatar\"> jpjuni0r:</div>\n<blockquote>\n<aside class=\"quote no-group\" data-username=\"jonhbit\" data-post=\"2\" data-topic=\"2105\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jonhbit/48/1661_2.png\" class=\"avatar\"> jonhbit:</div>\n<blockquote>\n<p>A related subject is how we could switch the LN P2P network from message flooding to something closer to the design outlined in the Erlay paper and BIP.</p>\n</blockquote>\n</aside>\n<p>Based on my understanding, the approach based on minisketch (only) reduces the bandwidth required to synchronize channel graphs. Is that correct? If so, what would be an \u201cacceptable\u201d amount of bandwidth?</p>\n</blockquote>\n</aside>\n<p>There should also be some savings of \u2018CPU usage per P2P connection\u2019, depending on the specifics of the sketch-based protocol.</p>\n<p>I don\u2019t have a concrete number for a maximum amount of bandwidth implementations would tolerate to be honest. Though the minimum amount of bandwidth needed (total volume of unique messages) will grow as the network continues to grow. And we know that flooding is already \u2018bad\u2019 enough with the current number of P2P connections, and that we\u2019ve accrued many workarounds so far.</p>\n<aside class=\"quote no-group\" data-username=\"jpjuni0r\" data-post=\"17\" data-topic=\"2105\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jpjuni0r/48/1719_2.png\" class=\"avatar\"> jpjuni0r:</div>\n<blockquote>\n<p>Bandwidth usage certainly is certainly one point worth looking at. Others could be:</p>\n<ul>\n<li>\n<p><strong>Reducing message delays</strong>: Based on measurements from <a class=\"mention\" href=\"/u/jonhbit\">@jonhbit</a>, about 95% of nodes receive a message after 600 sec.</p>\n<p>Note that this measurement includes only those nodes that <em>do</em> forward a given gossip message to the observer node. Furthermore, the observer node had 900 concurrent connections at peak time. However, there are around 1,250 nodes in the channel graph with known IP socket addresses. This raises the question why 28% of IP nodes were not reachable.</p>\n</li>\n</ul>\n</blockquote>\n</aside>\n<p>True - that value of 900 initial connections was just an arbitrary starting point tbh. I\u2019m planning to have something more thought-out for the upcoming version of the observer <img src=\"https://delvingbitcoin.org/images/emoji/twitter/slight_smile.png?v=14\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Re: reachability - one theory I heard recently is that many of the nodes that have both Tor and clearnet address in their node_announcment have misconfigured routers / firewalls, such that they broadcast an IPv4 address in their node_announcement but can\u2019t accept inbound IPv4 connections. I know Bitcoin Core has spent a lot of effort on this, with the (deprecated) UPnP support, and now NAT-PMP / PCP support. I suspect that implementations may broadcast IPv4 IPs without verifying that they can accept such connections.</p>\n<p>This may not affect normal operations like channel opens or broadcasting channel updates, since they can make an outbound connection to their counterparty and use keep-alives to work around NAT / router constraints. Or they just connect to their counterparty over Tor.</p>\n<p>The upcoming version of the observer should have Tor support, which would also help reach a better percentage of the network.</p>\n<aside class=\"quote no-group\" data-username=\"jpjuni0r\" data-post=\"17\" data-topic=\"2105\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jpjuni0r/48/1719_2.png\" class=\"avatar\"> jpjuni0r:</div>\n<blockquote>\n<ul>\n<li><strong>Increasing reliability of sending messages</strong>: Based on the results from the <a href=\"https://github.com/jharveyb/gossip_observer/blob/54320cca572afbf78873749e0f5d2f3c997d379a/README.md#how-many-peers-sent-us-the-same-message\" rel=\"noopener nofollow ugc\">gossip_observer repo</a>, there are only very few messages that &gt;500 nodes forward to the observer node. In the graph, there are various peaks in the ranges of 0, 100, 700 and 900 nodes. Importantly, most messages are only received by &lt;500 of nodes whereas in a perfect network, each of the 900 nodes receives every message.</li>\n</ul>\n</blockquote>\n</aside>\n<p>Some of that may be that my connection count was changing (decreasing) over time, to a final count of ~700 peers IIRC. So that peak at 700 likely still represents reliable propagation.</p>\n<p>For the peak of 100, I think that may be related to different propagation behavior for certain message types; I\u2019ll try to follow up on that.</p>\n<aside class=\"quote no-group\" data-username=\"jpjuni0r\" data-post=\"17\" data-topic=\"2105\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jpjuni0r/48/1719_2.png\" class=\"avatar\"> jpjuni0r:</div>\n<blockquote>\n<p>My goal is to understand the practical side, how much optimizations in the latter two points are warranted and what would be considered \u201cgood enough\u201d for the network.</p>\n</blockquote>\n</aside>\n<p>Based on offline feedback from implementers (and my own opinion), reliability in converging to a full network view / being in sync, is much more important than the convergence delay. Followed by resource usage and implementation complexity.</p>\n<p>I should be able to better observe the difference in network views over time once I start collecting data from multiple \u2018observers\u2019 at different positions in the P2P network.</p>\n<p>Achieving higher reliability via even more flooding connections is not a trade people want to make, and I think the gossip query behavior is the current substitute for that, where a node may periodically query a peer for all messages from a certain timespan (e.x. send all messages from the last hour) to make sure it didn\u2019t miss messages from only flooding.</p>",
  "post_number": 18,
  "post_type": 1,
  "posts_count": 20,
  "updated_at": "2025-12-16T16:26:15.798Z",
  "reply_count": 1,
  "reply_to_post_number": 17,
  "quote_count": 2,
  "incoming_link_count": 0,
  "reads": 16,
  "readers_count": 15,
  "score": 8.2,
  "yours": false,
  "topic_id": 2105,
  "topic_slug": "gossip-observer-new-project-to-monitor-the-lightning-p2p-network",
  "topic_title": "Gossip Observer: New project to monitor the Lightning P2P network",
  "topic_html_title": "Gossip Observer: New project to monitor the Lightning P2P network",
  "category_id": 7,
  "display_username": "Jonathan Harvey-Buschel",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "[quote=\"jpjuni0r, post:17, topic:2105, full:true\"]\nWithout derailing your discussion about the protocol specifics, I would like to understand exactly which problem the messaging based on sketches solves, and which will persist compared to the gossip approach.\n[/quote]\n\nNo problem, happy to have more feedback! There are some other problems I've heard of across multiple anecdotes talking to implementers, that aren't really covered in prior literature (that I'm aware of):\n\n- For a well-connected node, like an lightning service provider (LSP) that sells channels, or a routing node trying to earn a profit, adding more P2P connections and accepting gossip messages from all peers increases CPU usage significantly without improving their view of the network. As a result, bigger nodes take on extra complexity in filtering/rejecting gossip from peers, and maintaining their network view with some secondary system.\n- Even with how well-connected I expect the P2P network to be now, given the default number of connections implementations make (5+), there are reports of nodes missing messages related to entire subgraphs / neighborhoods of the payment network. So propagation of some messages may not be working reliably. This could also be caused by implementation-specific message filtering, that could be removed when moving to a sketch-based protocol.\n- A _lot_ of implementation complexity, across all implementations, concerning when to use gossip query messages to stay in sync with peers. As well as policies around when to forward gossip messages.\n\n[quote=\"jpjuni0r, post:17, topic:2105, full:true\"]\n[quote=\"jonhbit, post:2, topic:2105\"]\nA related subject is how we could switch the LN P2P network from message flooding to something closer to the design outlined in the Erlay paper and BIP.\n[/quote]\n\nBased on my understanding, the approach based on minisketch (only) reduces the bandwidth required to synchronize channel graphs. Is that correct? If so, what would be an \u201cacceptable\u201d amount of bandwidth?\n[/quote]\n\nThere should also be some savings of 'CPU usage per P2P connection', depending on the specifics of the sketch-based protocol.\n\nI don't have a concrete number for a maximum amount of bandwidth implementations would tolerate to be honest. Though the minimum amount of bandwidth needed (total volume of unique messages) will grow as the network continues to grow. And we know that flooding is already 'bad' enough with the current number of P2P connections, and that we've accrued many workarounds so far.\n\n[quote=\"jpjuni0r, post:17, topic:2105, full:true\"]\nBandwidth usage certainly is certainly one point worth looking at. Others could be:\n\n* **Reducing message delays**: Based on measurements from @jonhbit, about 95% of nodes receive a message after 600 sec.\n\n  Note that this measurement includes only those nodes that *do* forward a given gossip message to the observer node. Furthermore, the observer node had 900 concurrent connections at peak time. However, there are around 1,250 nodes in the channel graph with known IP socket addresses. This raises the question why 28% of IP nodes were not reachable.\n[/quote]\n\nTrue - that value of 900 initial connections was just an arbitrary starting point tbh. I'm planning to have something more thought-out for the upcoming version of the observer :slight_smile: \n\nRe: reachability - one theory I heard recently is that many of the nodes that have both Tor and clearnet address in their node_announcment have misconfigured routers / firewalls, such that they broadcast an IPv4 address in their node_announcement but can't accept inbound IPv4 connections. I know Bitcoin Core has spent a lot of effort on this, with the (deprecated) UPnP support, and now NAT-PMP / PCP support. I suspect that implementations may broadcast IPv4 IPs without verifying that they can accept such connections.\n\nThis may not affect normal operations like channel opens or broadcasting channel updates, since they can make an outbound connection to their counterparty and use keep-alives to work around NAT / router constraints. Or they just connect to their counterparty over Tor.\n\nThe upcoming version of the observer should have Tor support, which would also help reach a better percentage of the network.\n\n[quote=\"jpjuni0r, post:17, topic:2105, full:true\"]\n* **Increasing reliability of sending messages**: Based on the results from the [gossip_observer repo](https://github.com/jharveyb/gossip_observer/blob/54320cca572afbf78873749e0f5d2f3c997d379a/README.md#how-many-peers-sent-us-the-same-message), there are only very few messages that >500 nodes forward to the observer node. In the graph, there are various peaks in the ranges of 0, 100, 700 and 900 nodes. Importantly, most messages are only received by <500 of nodes whereas in a perfect network, each of the 900 nodes receives every message.\n[/quote]\n\nSome of that may be that my connection count was changing (decreasing) over time, to a final count of ~700 peers IIRC. So that peak at 700 likely still represents reliable propagation.\n\nFor the peak of 100, I think that may be related to different propagation behavior for certain message types; I'll try to follow up on that.\n\n[quote=\"jpjuni0r, post:17, topic:2105, full:true\"]\nMy goal is to understand the practical side, how much optimizations in the latter two points are warranted and what would be considered \u201cgood enough\u201d for the network.\n[/quote]\n\nBased on offline feedback from implementers (and my own opinion), reliability in converging to a full network view / being in sync, is much more important than the convergence delay. Followed by resource usage and implementation complexity.\n\nI should be able to better observe the difference in network views over time once I start collecting data from multiple 'observers' at different positions in the P2P network.\n\nAchieving higher reliability via even more flooding connections is not a trade people want to make, and I think the gossip query behavior is the current substitute for that, where a node may periodically query a peer for all messages from a certain timespan (e.x. send all messages from the last hour) to make sure it didn't miss messages from only flooding.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 982,
  "hidden": false,
  "trust_level": 1,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "No problem, happy to have more feedback! There are some other problems I\u2019ve heard of across multiple anecdotes talking to implementers, that aren\u2019t really covered in prior literature (that I\u2019m aware of): \n\nFor a well-connected node, like an lightning service provider (LSP) that sells channels, or a&hellip;",
  "truncated": true,
  "post_url": "/t/gossip-observer-new-project-to-monitor-the-lightning-p2p-network/2105/18",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}