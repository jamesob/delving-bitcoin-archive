{
  "id": 6458,
  "name": "Jan-Philipp",
  "username": "jpjuni0r",
  "avatar_template": "/user_avatar/delvingbitcoin.org/jpjuni0r/{size}/1719_2.png",
  "created_at": "2025-12-18T08:07:24.887Z",
  "cooked": "<p>You raise some quite interesting points!</p>\n<aside class=\"quote no-group\" data-username=\"jonhbit\" data-post=\"18\" data-topic=\"2105\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jonhbit/48/1661_2.png\" class=\"avatar\"> jonhbit:</div>\n<blockquote>\n<p>Even with how well-connected I expect the P2P network to be now, given the default number of connections implementations make (5+), there are reports of nodes missing messages related to entire subgraphs / neighborhoods of the payment network.</p>\n</blockquote>\n</aside>\n<p>Since you are talking about neighborhoods, do you think that the gossip graph is partitioned? I think that three random gossip syncers should be enough that there is only a single partition, though that assumes that every node works reliably.</p>\n<aside class=\"quote no-group quote-modified\" data-username=\"jonhbit\" data-post=\"18\" data-topic=\"2105\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jonhbit/48/1661_2.png\" class=\"avatar\"> jonhbit:</div>\n<blockquote>\n<aside class=\"quote no-group\" data-username=\"jpjuni0r\" data-post=\"17\" data-topic=\"2105\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jpjuni0r/48/1719_2.png\" class=\"avatar\"> jpjuni0r:</div>\n<blockquote>\n<p>Bandwidth usage certainly is certainly one point worth looking at. Others could be:</p>\n<ul>\n<li><strong>Reducing message delays</strong>: Based on measurements from <a class=\"mention\" href=\"/u/jonhbit\">@jonhbit</a>, about 95% of nodes receive a message after 600 sec.Note that this measurement includes only those nodes that <em>do</em> forward a given gossip message to the observer node. Furthermore, the observer node had 900 concurrent connections at peak time. However, there are around 1,250 nodes in the channel graph with known IP socket addresses. This raises the question why 28% of IP nodes were not reachable.</li>\n</ul>\n</blockquote>\n</aside>\n<p>True - that value of 900 initial connections was just an arbitrary starting point tbh. I\u2019m planning to have something more thought-out for the upcoming version of the observer <img src=\"https://delvingbitcoin.org/images/emoji/twitter/slight_smile.png?v=14\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Re: reachability - one theory I heard recently is that many of the nodes that have both Tor and clearnet address in their node_announcment have misconfigured routers / firewalls, such that they broadcast an IPv4 address in their node_announcement but can\u2019t accept inbound IPv4 connections. I know Bitcoin Core has spent a lot of effort on this, with the (deprecated) UPnP support, and now NAT-PMP / PCP support. I suspect that implementations may broadcast IPv4 IPs without verifying that they can accept such connections.</p>\n</blockquote>\n</aside>\n<p>Concerning the topic of \u201cnode liveness\u201d, I can provide some additional numbers from a similar measurement, compared to yours, that I did a few months ago. While the results generally align with yours, my gossip observer also connected to Tor nodes and we let it run for longer (10 days continuously).</p>\n<p>Overall, the gossip observer had an average of 4,195 peers out of 16,626 peers in the channel graph (Aug. 16, 2025). Also, we find that 4,859 nodes created at least one new gossip message during that time.</p>\n<p>Looking at a larger timeframe of 55 days, we find that 7,579 were reachable at least once, and we made connection attempts to 13,165 nodes. The latter number is smaller than the number of nodes in the channel graph, since 5,607 nodes are private nodes in the channel graph with no socket addresses advertised.</p>\n<p>Still, even when ignoring the private nodes, I find that it is not ideal that there are nodes in the network that are not reachable, and it raises the question, whether they are able to process payments. That number of 4,859 nodes that recently created a new gossip message also was unexpected for me. For a typical channel, I find that they typically create a new channel update at least every 24 hours.</p>\n<p>Another related point to your goal minimizing bandwidth: I found that there is a small minority of nodes that are very \u201cchatty\u201d. Those frequently create redundant channel updates or node announcements, that differ only in their timestamp and signature fields. In once case, there is a node creating a new node announcement every 30 sec.</p>\n<aside class=\"quote no-group quote-modified\" data-username=\"jonhbit\" data-post=\"18\" data-topic=\"2105\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jonhbit/48/1661_2.png\" class=\"avatar\"> jonhbit:</div>\n<blockquote>\n<aside class=\"quote no-group\" data-username=\"jpjuni0r\" data-post=\"17\" data-topic=\"2105\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jpjuni0r/48/1719_2.png\" class=\"avatar\"> jpjuni0r:</div>\n<blockquote>\n<p>My goal is to understand the practical side, how much optimizations in the latter two points are warranted and what would be considered \u201cgood enough\u201d for the network.</p>\n</blockquote>\n</aside>\n<p>Based on offline feedback from implementers (and my own opinion), reliability in converging to a full network view / being in sync, is much more important than the convergence delay. Followed by resource usage and implementation complexity.</p>\n<p>I should be able to better observe the difference in network views over time once I start collecting data from multiple \u2018observers\u2019 at different positions in the P2P network.</p>\n</blockquote>\n</aside>\n<p>What exactly do you mean by \u201cdifferent positions\u201d in the P2P network? Is that just a different selection of gossip syncers?</p>\n<p>To give you another idea of experimentation: By using a special <code>gossip_timestamp_filter</code> message, you could request the entire channel graph from each node and then check, which information is missing.</p>\n<aside class=\"quote no-group quote-modified\" data-username=\"jonhbit\" data-post=\"18\" data-topic=\"2105\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jonhbit/48/1661_2.png\" class=\"avatar\"> jonhbit:</div>\n<blockquote>\n<aside class=\"quote no-group\" data-username=\"jpjuni0r\" data-post=\"17\" data-topic=\"2105\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/jpjuni0r/48/1719_2.png\" class=\"avatar\"> jpjuni0r:</div>\n<blockquote>\n<ul>\n<li><strong>Increasing reliability of sending messages</strong>: Based on the results from the <a href=\"https://github.com/jharveyb/gossip_observer/blob/54320cca572afbf78873749e0f5d2f3c997d379a/README.md#how-many-peers-sent-us-the-same-message\" rel=\"noopener nofollow ugc\">gossip_observer repo</a>, there are only very few messages that &gt;500 nodes forward to the observer node. In the graph, there are various peaks in the ranges of 0, 100, 700 and 900 nodes. Importantly, most messages are only received by &lt;500 of nodes whereas in a perfect network, each of the 900 nodes receives every message.</li>\n</ul>\n</blockquote>\n</aside>\n<p>Some of that may be that my connection count was changing (decreasing) over time, to a final count of ~700 peers IIRC. So that peak at 700 likely still represents reliable propagation.</p>\n<p>For the peak of 100, I think that may be related to different propagation behavior for certain message types; I\u2019ll try to follow up on that.</p>\n</blockquote>\n</aside>\n<p>Looking forward to your investigation! I can also share that analysis based on my dataset (below), where there are only two peaks: One around 0 and the other one around the number of average peers that the node had. Though I did not include all messages received in the results, but made two methodological adjustments:</p>\n<p>In the first plot labeled \u201cMessages\u201d, I already excluded messages that were historical. That is, when before starting the measurement, I first let the monitor synchronize its channel graph with the other nodes. When starting the actual measurement, I only include those messages that the node has not seen before. Since there are <a href=\"https://github.com/lightningdevkit/rust-lightning/blob/2efb0098793ce2632006c5573f9f0dae7fb6a22a/lightning/src/routing/gossip.rs#L780-L839\" rel=\"noopener nofollow ugc\">some nodes</a>, sending their entire channel graph for new connection, it skews the results.</p>\n<p>Another influence I looked at (plotted as \u201cwith deduplication\u201d) is the staggered broadcast mechanism, where a node shall only forward a gossip message with the greatest known timestamp field. As this is one mechanism, where messages could be dropped, I changed the methodology to consider a node \u201creceiving\u201d a message when forwarding message <em>m</em>, or any of its successors m\u2019 (where timestamp field of <em>m\u2019</em> is greater than that of <em>m</em>) to the gossip observer node. That further changes the results to what would be expected from the network: That every one of the ~4,500 connected nodes received each message. Though I cannot explain why there are so many messages in the 0\u2013500 node range on the left that, apparently, starve.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://delvingbitcoin.org/uploads/default/original/2X/3/3e9e0c006cabb4871edd0cf44656a5f1c5a04fec.png\" data-download-href=\"https://delvingbitcoin.org/uploads/default/3e9e0c006cabb4871edd0cf44656a5f1c5a04fec\" title=\"diagram\"><img src=\"https://delvingbitcoin.org/uploads/default/optimized/2X/3/3e9e0c006cabb4871edd0cf44656a5f1c5a04fec_2_517x258.png\" alt=\"diagram\" data-base62-sha1=\"8VWdsl1PdKqQjUcSAcmq0kYip3u\" width=\"517\" height=\"258\" srcset=\"https://delvingbitcoin.org/uploads/default/optimized/2X/3/3e9e0c006cabb4871edd0cf44656a5f1c5a04fec_2_517x258.png, https://delvingbitcoin.org/uploads/default/optimized/2X/3/3e9e0c006cabb4871edd0cf44656a5f1c5a04fec_2_775x387.png 1.5x, https://delvingbitcoin.org/uploads/default/optimized/2X/3/3e9e0c006cabb4871edd0cf44656a5f1c5a04fec_2_1034x516.png 2x\" data-dominant-color=\"F3F2F1\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">diagram</span><span class=\"informations\">2798\u00d71398 296 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div></p>",
  "post_number": 21,
  "post_type": 1,
  "posts_count": 20,
  "updated_at": "2025-12-18T08:07:24.887Z",
  "reply_count": 0,
  "reply_to_post_number": 18,
  "quote_count": 2,
  "incoming_link_count": 0,
  "reads": 16,
  "readers_count": 15,
  "score": 3.2,
  "yours": false,
  "topic_id": 2105,
  "topic_slug": "gossip-observer-new-project-to-monitor-the-lightning-p2p-network",
  "topic_title": "Gossip Observer: New project to monitor the Lightning P2P network",
  "topic_html_title": "Gossip Observer: New project to monitor the Lightning P2P network",
  "category_id": 7,
  "display_username": "Jan-Philipp",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "You raise some quite interesting points!\n\n[quote=\"jonhbit, post:18, topic:2105\"]\nEven with how well-connected I expect the P2P network to be now, given the default number of connections implementations make (5+), there are reports of nodes missing messages related to entire subgraphs / neighborhoods of the payment network.\n\n[/quote]\n\nSince you are talking about neighborhoods, do you think that the gossip graph is partitioned? I think that three random gossip syncers should be enough that there is only a single partition, though that assumes that every node works reliably.\n\n[quote=\"jonhbit, post:18, topic:2105\"]\n[quote=\"jpjuni0r, post:17, topic:2105\"]\nBandwidth usage certainly is certainly one point worth looking at. Others could be:\n\n* **Reducing message delays**: Based on measurements from @jonhbit, about 95% of nodes receive a message after 600 sec.Note that this measurement includes only those nodes that *do* forward a given gossip message to the observer node. Furthermore, the observer node had 900 concurrent connections at peak time. However, there are around 1,250 nodes in the channel graph with known IP socket addresses. This raises the question why 28% of IP nodes were not reachable.\n\n[/quote]\n\nTrue - that value of 900 initial connections was just an arbitrary starting point tbh. I\u2019m planning to have something more thought-out for the upcoming version of the observer :slight_smile:\n\nRe: reachability - one theory I heard recently is that many of the nodes that have both Tor and clearnet address in their node_announcment have misconfigured routers / firewalls, such that they broadcast an IPv4 address in their node_announcement but can\u2019t accept inbound IPv4 connections. I know Bitcoin Core has spent a lot of effort on this, with the (deprecated) UPnP support, and now NAT-PMP / PCP support. I suspect that implementations may broadcast IPv4 IPs without verifying that they can accept such connections.\n\n[/quote]\n\nConcerning the topic of \u201cnode liveness\u201d, I can provide some additional numbers from a similar measurement, compared to yours, that I did a few months ago. While the results generally align with yours, my gossip observer also connected to Tor nodes and we let it run for longer (10 days continuously).\n\nOverall, the gossip observer had an average of 4,195 peers out of 16,626 peers in the channel graph (Aug. 16, 2025). Also, we find that 4,859 nodes created at least one new gossip message during that time.\n\nLooking at a larger timeframe of 55 days, we find that 7,579 were reachable at least once, and we made connection attempts to 13,165 nodes. The latter number is smaller than the number of nodes in the channel graph, since 5,607 nodes are private nodes in the channel graph with no socket addresses advertised.\n\nStill, even when ignoring the private nodes, I find that it is not ideal that there are nodes in the network that are not reachable, and it raises the question, whether they are able to process payments. That number of 4,859 nodes that recently created a new gossip message also was unexpected for me. For a typical channel, I find that they typically create a new channel update at least every 24 hours.\n\nAnother related point to your goal minimizing bandwidth: I found that there is a small minority of nodes that are very \u201cchatty\u201d. Those frequently create redundant channel updates or node announcements, that differ only in their timestamp and signature fields. In once case, there is a node creating a new node announcement every 30 sec.\n\n[quote=\"jonhbit, post:18, topic:2105\"]\n[quote=\"jpjuni0r, post:17, topic:2105\"]\nMy goal is to understand the practical side, how much optimizations in the latter two points are warranted and what would be considered \u201cgood enough\u201d for the network.\n\n[/quote]\n\nBased on offline feedback from implementers (and my own opinion), reliability in converging to a full network view / being in sync, is much more important than the convergence delay. Followed by resource usage and implementation complexity.\n\nI should be able to better observe the difference in network views over time once I start collecting data from multiple \u2018observers\u2019 at different positions in the P2P network.\n\n[/quote]\n\nWhat exactly do you mean by \u201cdifferent positions\u201d in the P2P network? Is that just a different selection of gossip syncers?\n\nTo give you another idea of experimentation: By using a special `gossip_timestamp_filter` message, you could request the entire channel graph from each node and then check, which information is missing.\n\n[quote=\"jonhbit, post:18, topic:2105\"]\n[quote=\"jpjuni0r, post:17, topic:2105\"]\n* **Increasing reliability of sending messages**: Based on the results from the [gossip_observer repo](https://github.com/jharveyb/gossip_observer/blob/54320cca572afbf78873749e0f5d2f3c997d379a/README.md#how-many-peers-sent-us-the-same-message), there are only very few messages that >500 nodes forward to the observer node. In the graph, there are various peaks in the ranges of 0, 100, 700 and 900 nodes. Importantly, most messages are only received by <500 of nodes whereas in a perfect network, each of the 900 nodes receives every message.\n\n[/quote]\n\nSome of that may be that my connection count was changing (decreasing) over time, to a final count of \\~700 peers IIRC. So that peak at 700 likely still represents reliable propagation.\n\nFor the peak of 100, I think that may be related to different propagation behavior for certain message types; I\u2019ll try to follow up on that.\n\n[/quote]\n\nLooking forward to your investigation! I can also share that analysis based on my dataset (below), where there are only two peaks: One around 0 and the other one around the number of average peers that the node had. Though I did not include all messages received in the results, but made two methodological adjustments:\n\nIn the first plot labeled \u201cMessages\u201d, I already excluded messages that were historical. That is, when before starting the measurement, I first let the monitor synchronize its channel graph with the other nodes. When starting the actual measurement, I only include those messages that the node has not seen before. Since there are [some nodes](https://github.com/lightningdevkit/rust-lightning/blob/2efb0098793ce2632006c5573f9f0dae7fb6a22a/lightning/src/routing/gossip.rs#L780-L839), sending their entire channel graph for new connection, it skews the results.\n\nAnother influence I looked at (plotted as \u201cwith deduplication\u201d) is the staggered broadcast mechanism, where a node shall only forward a gossip message with the greatest known timestamp field. As this is one mechanism, where messages could be dropped, I changed the methodology to consider a node \u201creceiving\u201d a message when forwarding message *m*, or any of its successors m\u2019 (where timestamp field of *m\u2019* is greater than that of *m*) to the gossip observer node. That further changes the results to what would be expected from the network: That every one of the \\~4,500 connected nodes received each message. Though I cannot explain why there are so many messages in the 0\u2013500 node range on the left that, apparently, starve.\n\n![diagram|690x344, 75%](upload://8VWdsl1PdKqQjUcSAcmq0kYip3u.png)",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 749,
  "hidden": false,
  "trust_level": 1,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "You raise some quite interesting points! \n\nSince you are talking about neighborhoods, do you think that the gossip graph is partitioned? I think that three random gossip syncers should be enough that there is only a single partition, though that assumes that every node works reliably. \n\nTrue - that &hellip;",
  "truncated": true,
  "post_url": "/t/gossip-observer-new-project-to-monitor-the-lightning-p2p-network/2105/21",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}