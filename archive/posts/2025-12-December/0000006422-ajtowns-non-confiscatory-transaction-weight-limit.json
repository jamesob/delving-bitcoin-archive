{
  "id": 6422,
  "name": "Anthony Towns",
  "username": "ajtowns",
  "avatar_template": "/user_avatar/delvingbitcoin.org/ajtowns/{size}/417_2.png",
  "created_at": "2025-12-11T23:37:47.100Z",
  "cooked": "<aside class=\"quote no-group\" data-username=\"sipa\" data-post=\"8\" data-topic=\"1732\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/sipa/48/1100_2.png\" class=\"avatar\"> sipa:</div>\n<blockquote>\n<p>I\u2019m not convinced about the approach suggested here - it feels like it should lead to similar problems still, but thinking practically I can\u2019t really see many issues. At least the \u201conly 1 large transaction + coinbase in a block\u201d rule is effectively equivalent to \u201ctreat every transaction as if it had ~1 MvB in size\u201d,</p>\n</blockquote>\n</aside>\n<p>Having every tx greater than 100kvB be treated as being ~1000kvB seems like it would encourage stuffing \u2013 \u201cI need a 120kvB tx, but I have to pay for 880kvB extra anyway, so might as well find some garbage to fill that up with\u201d. Perhaps you could tweak this somewhat so that other people\u2019s txs could still what\u2019s used to fill up the block. Rough idea:</p>\n<h2><a name=\"p-6422-consensus-rules-1\" class=\"anchor\" href=\"#p-6422-consensus-rules-1\"></a>Consensus rules</h2>\n<ul>\n<li>both the coinbase and the last tx in a block can be arbitrarily large, but every other tx must have a weight less than 400000</li>\n<li>if the last tx in a block has weight more than 400,000:\n<ul>\n<li>it cannot spend an output that was created in the block (ie, it\u2019s treated as if it had an relative lock time of 1 block)</li>\n<li>its weight is rounded up to 5000 below the next multiple of 100k, ie <span class=\"math\">w' \\equiv 95000 \\pmod{100000}</span>. This makes the max tx size 3,995,000 weight units or 998,750vB.</li>\n</ul>\n</li>\n</ul>\n<h2><a name=\"p-6422-mempool-acceptance-storage-2\" class=\"anchor\" href=\"#p-6422-mempool-acceptance-storage-2\"></a>Mempool acceptance, storage</h2>\n<p>When accepting large txs to the mempool, they cannot have in-mempool ancestors or descendants, so always have a cluster size of one.</p>\n<p>Because their weights are rounded up for consensus purposes, each large tx in the mempool can be put into one of 36 buckets matching its rounded up weight (495,000 weight units through 3,995,000), and only the highest fee tx in each of those buckets needs to be considered at any point in time. Further, if the best tx in a higher weight unit bucket has lower fee than the best tx in a lower weight unit bucket, it can be ignored. Keep track of these ~36 non-ignored txs.</p>\n<h2><a name=\"p-6422-mining-3\" class=\"anchor\" href=\"#p-6422-mining-3\"></a>Mining</h2>\n<p>When mining, take the ~36 non-ignored large txs from your mempool ordered from largest to smallest, put them in a <code>large_buckets</code> list, and run something like this algorithm:</p>\n<pre data-code-wrap=\"python\"><code class=\"lang-python\">    block = []\n    ignored_txs = 0\n    large_bucket = 0\n    while ignored_txs &lt; 1000:\n        chunk = mempool.get_next_chunk()\n        if not chunk: break\n        if block.weight() + chunk.weight() &gt; MAX_WEIGHT:\n            mempool.ignored_chunk()\n            ignored_txs += 1\n            continue\n        while large_bucket &lt; large_buckets.size()\n            if block.weight() + chunk.weight() + large_buckets[large_bucket].weight() &gt; MAX_WEIGHT:\n                large_buckets[large_bucket].block = block.copy()\n                large_bucket += 1\n            else:\n                break\n        block.add(chunk)\n        mempool.accepted_chunk()\n        ignored_txs = 0\n    if large_bucket &lt; large_buckets.size():\n        large_buckets[large_bucket].block = block.copy()\n    for large_bucket in range(large_buckets):\n        if not lb.block: break\n        lb = large_buckets[large_bucket]\n        if lb.block.fee + lb.tx.fee &gt; block.fee:\n             block = lb.block + lb.tx\n    return block\n</code></pre>\n<p>That misses out on getting the best possible final 25kvB into a block that includes a large tx, but otherwise seems fairly feasible?</p>",
  "post_number": 10,
  "post_type": 1,
  "posts_count": 12,
  "updated_at": "2025-12-11T23:37:47.100Z",
  "reply_count": 0,
  "reply_to_post_number": 8,
  "quote_count": 1,
  "incoming_link_count": 0,
  "reads": 25,
  "readers_count": 24,
  "score": 5.0,
  "yours": false,
  "topic_id": 1732,
  "topic_slug": "non-confiscatory-transaction-weight-limit",
  "topic_title": "Non-confiscatory Transaction Weight Limit",
  "topic_html_title": "Non-confiscatory Transaction Weight Limit",
  "category_id": 7,
  "display_username": "Anthony Towns",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "[quote=\"sipa, post:8, topic:1732\"]\nI\u2019m not convinced about the approach suggested here - it feels like it should lead to similar problems still, but thinking practically I can\u2019t really see many issues. At least the \u201conly 1 large transaction + coinbase in a block\u201d rule is effectively equivalent to \u201ctreat every transaction as if it had ~1 MvB in size\u201d,\n[/quote]\n\nHaving every tx greater than 100kvB be treated as being ~1000kvB seems like it would encourage stuffing -- \"I need a 120kvB tx, but I have to pay for 880kvB extra anyway, so might as well find some garbage to fill that up with\". Perhaps you could tweak this somewhat so that other people's txs could still what's used to fill up the block. Rough idea:\n\n## Consensus rules\n\n * both the coinbase and the last tx in a block can be arbitrarily large, but every other tx must have a weight less than 400000\n * if the last tx in a block has weight more than 400,000:\n   * it cannot spend an output that was created in the block (ie, it's treated as if it had an relative lock time of 1 block)\n   * its weight is rounded up to 5000 below the next multiple of 100k, ie $w' \\equiv 95000 \\pmod{100000}$. This makes the max tx size 3,995,000 weight units or 998,750vB.\n\n## Mempool acceptance, storage\n\nWhen accepting large txs to the mempool, they cannot have in-mempool ancestors or descendants, so always have a cluster size of one.\n\nBecause their weights are rounded up for consensus purposes, each large tx in the mempool can be put into one of 36 buckets matching its rounded up weight (495,000 weight units through 3,995,000), and only the highest fee tx in each of those buckets needs to be considered at any point in time. Further, if the best tx in a higher weight unit bucket has lower fee than the best tx in a lower weight unit bucket, it can be ignored. Keep track of these ~36 non-ignored txs.\n\n## Mining\n\nWhen mining, take the ~36 non-ignored large txs from your mempool ordered from largest to smallest, put them in a `large_buckets` list, and run something like this algorithm:\n\n```python\n    block = []\n    ignored_txs = 0\n    large_bucket = 0\n    while ignored_txs < 1000:\n        chunk = mempool.get_next_chunk()\n        if not chunk: break\n        if block.weight() + chunk.weight() > MAX_WEIGHT:\n            mempool.ignored_chunk()\n            ignored_txs += 1\n            continue\n        while large_bucket < large_buckets.size()\n            if block.weight() + chunk.weight() + large_buckets[large_bucket].weight() > MAX_WEIGHT:\n                large_buckets[large_bucket].block = block.copy()\n                large_bucket += 1\n            else:\n                break\n        block.add(chunk)\n        mempool.accepted_chunk()\n        ignored_txs = 0\n    if large_bucket < large_buckets.size():\n        large_buckets[large_bucket].block = block.copy()\n    for large_bucket in range(large_buckets):\n        if not lb.block: break\n        lb = large_buckets[large_bucket]\n        if lb.block.fee + lb.tx.fee > block.fee:\n             block = lb.block + lb.tx\n    return block\n```\n\nThat misses out on getting the best possible final 25kvB into a block that includes a large tx, but otherwise seems fairly feasible?",
  "actions_summary": [],
  "moderator": true,
  "admin": true,
  "staff": true,
  "user_id": 3,
  "hidden": false,
  "trust_level": 4,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "Having every tx greater than 100kvB be treated as being ~1000kvB seems like it would encourage stuffing \u2013 \u201cI need a 120kvB tx, but I have to pay for 880kvB extra anyway, so might as well find some garbage to fill that up with\u201d. Perhaps you could tweak this somewhat so that other people\u2019s txs could &hellip;",
  "truncated": true,
  "post_url": "/t/non-confiscatory-transaction-weight-limit/1732/10",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}