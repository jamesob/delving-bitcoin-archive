{
  "id": 1894,
  "name": "Fabian",
  "username": "fjahr",
  "avatar_template": "/letter_avatar_proxy/v4/letter/f/ea5d25/{size}.png",
  "created_at": "2024-03-14T19:55:23.909Z",
  "cooked": "<p>Thanks for the comments!</p>\n<aside class=\"quote no-group\" data-username=\"0xB10C\" data-post=\"2\" data-topic=\"624\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/0xb10c/48/15_2.png\" class=\"avatar\"> 0xB10C:</div>\n<blockquote>\n<p>Would it be possible to run two instances where one is importing while the other is displaying and then switch them over once the other is done importing and so on? Could also do this on a 48h timer (you mentioned the import takes 36h).</p>\n</blockquote>\n</aside>\n<p>Yes, that would be possible, we wouldn\u2019t even need to switch, we could have one that constantly clones from Github and then every time it is done the second one can get a GitLab to GitLab clone which is a lot faster because it\u2019s just dumping the databases and not using any APIs. This should be more comfortable for viewers because they would only have one go-to url. Though I am not sure if it\u2019s worth the effort honestly. I don\u2019t expect a lot of people to want to look at the data before we actually need it. Did you have specific use cases in mind for making the data viewable? I guess we should somehow be checking that the backup still works and there is no garbage data coming in but I think that should also be doable via a script.</p>",
  "post_number": 3,
  "post_type": 1,
  "updated_at": "2024-03-14T19:55:23.909Z",
  "reply_count": 1,
  "reply_to_post_number": 2,
  "quote_count": 1,
  "incoming_link_count": 0,
  "reads": 9,
  "readers_count": 8,
  "score": 6.8,
  "yours": false,
  "topic_id": 624,
  "topic_slug": "gitlab-backups-for-bitcoin-core-repository",
  "topic_title": "GitLab Backups for Bitcoin Core repository",
  "topic_html_title": "GitLab Backups for Bitcoin Core repository",
  "category_id": 5,
  "display_username": "Fabian",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "Thanks for the comments!\n\n[quote=\"0xB10C, post:2, topic:624\"]\nWould it be possible to run two instances where one is importing while the other is displaying and then switch them over once the other is done importing and so on? Could also do this on a 48h timer (you mentioned the import takes 36h).\n[/quote]\n\nYes, that would be possible, we wouldn't even need to switch, we could have one that constantly clones from Github and then every time it is done the second one can get a GitLab to GitLab clone which is a lot faster because it's just dumping the databases and not using any APIs. This should be more comfortable for viewers because they would only have one go-to url. Though I am not sure if it's worth the effort honestly. I don't expect a lot of people to want to look at the data before we actually need it. Did you have specific use cases in mind for making the data viewable? I guess we should somehow be checking that the backup still works and there is no garbage data coming in but I think that should also be doable via a script.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 205,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false
}