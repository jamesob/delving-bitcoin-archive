{
  "id": 6210,
  "name": "Anthony Towns",
  "username": "ajtowns",
  "avatar_template": "/user_avatar/delvingbitcoin.org/ajtowns/{size}/417_2.png",
  "created_at": "2025-11-10T09:02:45.772Z",
  "cooked": "<aside class=\"quote no-group\" data-username=\"Julian\" data-post=\"1\" data-topic=\"2094\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/letter_avatar_proxy/v4/letter/j/74df32/48.png\" class=\"avatar\"> Julian:</div>\n<blockquote>\n<ol start=\"2\">\n<li>Compiling with benchmarks enabled (-DBUILD_BENCH=ON)</li>\n</ol>\n</blockquote>\n</aside>\n<p>Probably want to specify the build options more completely if you want benchmark results to be comparable? (<code>CMAKE_BUILD_TYPE</code> as something other than <code>Debug</code> in particular)</p>\n<p>(Also <code>#ifdef DEBUG</code> and <code>#ifdef USE_GMP</code> instead of <code>#if</code> in various val64 code)</p>\n<aside class=\"quote no-group\" data-username=\"Julian\" data-post=\"1\" data-topic=\"2094\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/letter_avatar_proxy/v4/letter/j/74df32/48.png\" class=\"avatar\"> Julian:</div>\n<blockquote>\n<p>It would be very helpful if you shared your results so we can analyze the data across different systems</p>\n</blockquote>\n</aside>\n<p>Probably should provide a git repo to add the CSV\u2019s to via PR?</p>\n<p>I\u2019m seeing errors, which look like they lead to useless data, fwiw:</p>\n<pre data-code-wrap=\"txt\"><code class=\"lang-txt\">Script error: OP_DIV or OP_MOD by zero\n653/875: MOD_DUP_100Bx2                 0.000 seconds (     0 Schnorrs,    0.0% varops used)\n</code></pre>\n<p>May want to fix those first?</p>\n<aside class=\"quote no-group quote-modified\" data-username=\"Julian\" data-post=\"1\" data-topic=\"2094\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/letter_avatar_proxy/v4/letter/j/74df32/48.png\" class=\"avatar\"> Julian:</div>\n<blockquote>\n<p>If we want the varops budget to limit script execution time to be \u2026</p>\n</blockquote>\n</aside>\n<p>Capping at 100% varops budget seems to make the data less useful in evaluating whether the varops ratios are accurate, than if no capping was in place? eg these XOR tests:</p>\n<pre data-code-wrap=\"txt\"><code class=\"lang-txt\">511/875: DUP_XOR_DROP_DUP_10KBx2        0.514 seconds ( 12714 Schnorrs,  100.0% varops used)\n509/875: DUP_XOR_DROP_DUP_100KBx2       0.446 seconds ( 11051 Schnorrs,  100.0% varops used)\n514/875: DUP_XOR_DROP_DUP_1MBx2         0.477 seconds ( 11807 Schnorrs,  100.0% varops used)\n516/875: DUP_XOR_DROP_DUP_2MBx2         0.475 seconds ( 11769 Schnorrs,  100.0% varops used) \n</code></pre>\n<p>don\u2019t seem to give any useful information about relative performance at different input sizes, because in each case the varops limit is hit?</p>\n<p>I would have expected to be able to use the data generated either to produce per-hardware calculations of the constant-and-per-byte factors for each opcode (and thus review the hardcoded varops factors) or alternatively to be able to judge what would cause the worst case blocks for my hardware (particularly for old hardware) \u2013 eg, \u201ca block full of schnorr checks will take 5s to validate, but a block of X hash256 ops with Y bytes of data each will take 10s to validate\u201d.</p>\n<p>Looking at &lt;100% varops hashing results, I think I can calculate:</p>\n<ul>\n<li>ripemd: constant = 0.322, per byte = 0.00251</li>\n<li>sha256: constant = 0.362, per byte = 0.00306</li>\n<li>hash160: constant = 0.549, per byte = 0.00299</li>\n</ul>\n<p>Or normalized from \u201cseconds per block over-filled with repeating script\u201d to \u201cschnorr ops per op\u201d, perhaps:</p>\n<ul>\n<li>ripemd: constant = 0.006 schnorr ops per op, plus 0.000047 schnorr ops per byte</li>\n<li>sha256: constant = 0.007 plus 0.000057 per byte</li>\n<li>hash160: constant = 0.010 plus 0.000056 per byte</li>\n</ul>\n<p>Or normalized from 80k schnorr ops per block to 20.8B compute units per block:</p>\n<ul>\n<li>ripemd: constant = 1554, per byte = 12.1</li>\n<li>sha256: constant = 1747, per byte= 14.8</li>\n<li>hash160: constant = 2650, per byte= 14.4</li>\n</ul>\n<p>Those compare to current varops figures of constant=0, per byte=10, I believe. I\u2019m surprised that ripemd seems to be faster than sha256 for me. Presumably either taking the DUP costs into account or setting up an absurdly large stack so that DUP is unnecessary would generate more accurate figures than I\u2019ve done above though. FWIW DUP/DROP benchmarks seems to imply figures of 276 compute units per operation plus 0.05 compute units per byte for me (as opposed to 0 + 1 per byte).</p>",
  "post_number": 2,
  "post_type": 1,
  "posts_count": 3,
  "updated_at": "2025-11-10T09:02:45.772Z",
  "reply_count": 1,
  "reply_to_post_number": null,
  "quote_count": 1,
  "incoming_link_count": 0,
  "reads": 11,
  "readers_count": 10,
  "score": 7.2,
  "yours": false,
  "topic_id": 2094,
  "topic_slug": "benchmarking-bitcoin-script-evaluation-for-the-varops-budget-great-script-restoration",
  "topic_title": "Benchmarking Bitcoin Script Evaluation for the Varops Budget (Great Script Restoration)",
  "topic_html_title": "Benchmarking Bitcoin Script Evaluation for the Varops Budget (Great Script Restoration)",
  "category_id": 7,
  "display_username": "Anthony Towns",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "[quote=\"Julian, post:1, topic:2094\"]\n2. Compiling with benchmarks enabled (-DBUILD_BENCH=ON)\n[/quote]\n\nProbably want to specify the build options more completely if you want benchmark results to be comparable? (`CMAKE_BUILD_TYPE` as something other than `Debug` in particular)\n\n(Also `#ifdef DEBUG` and `#ifdef USE_GMP` instead of `#if` in various val64 code)\n\n[quote=\"Julian, post:1, topic:2094\"]\nIt would be very helpful if you shared your results so we can analyze the data across different systems\n[/quote]\n\nProbably should provide a git repo to add the CSV's to via PR?\n\nI'm seeing errors, which look like they lead to useless data, fwiw:\n\n```txt\nScript error: OP_DIV or OP_MOD by zero\n653/875: MOD_DUP_100Bx2                 0.000 seconds (     0 Schnorrs,    0.0% varops used)\n```\n\nMay want to fix those first?\n\n[quote=\"Julian, post:1, topic:2094\"]\nIf we want the varops budget to limit script execution time to be ...\n[/quote]\n\nCapping at 100% varops budget seems to make the data less useful in evaluating whether the varops ratios are accurate, than if no capping was in place? eg these XOR tests:\n\n```txt\n511/875: DUP_XOR_DROP_DUP_10KBx2        0.514 seconds ( 12714 Schnorrs,  100.0% varops used)\n509/875: DUP_XOR_DROP_DUP_100KBx2       0.446 seconds ( 11051 Schnorrs,  100.0% varops used)\n514/875: DUP_XOR_DROP_DUP_1MBx2         0.477 seconds ( 11807 Schnorrs,  100.0% varops used)\n516/875: DUP_XOR_DROP_DUP_2MBx2         0.475 seconds ( 11769 Schnorrs,  100.0% varops used) \n```\n\ndon't seem to give any useful information about relative performance at different input sizes, because in each case the varops limit is hit?\n\nI would have expected to be able to use the data generated either to produce per-hardware calculations of the constant-and-per-byte factors for each opcode (and thus review the hardcoded varops factors) or alternatively to be able to judge what would cause the worst case blocks for my hardware (particularly for old hardware) -- eg, \"a block full of schnorr checks will take 5s to validate, but a block of X hash256 ops with Y bytes of data each will take 10s to validate\".\n\nLooking at <100% varops hashing results, I think I can calculate:\n\n * ripemd: constant = 0.322, per byte = 0.00251\n * sha256: constant = 0.362, per byte = 0.00306\n * hash160: constant = 0.549, per byte = 0.00299\n\nOr normalized from \"seconds per block over-filled with repeating script\" to \"schnorr ops per op\", perhaps:\n\n * ripemd: constant = 0.006 schnorr ops per op, plus 0.000047 schnorr ops per byte\n * sha256: constant = 0.007 plus 0.000057 per byte\n * hash160: constant = 0.010 plus 0.000056 per byte\n\nOr normalized from 80k schnorr ops per block to 20.8B compute units per block:\n\n * ripemd: constant = 1554, per byte = 12.1\n * sha256: constant = 1747, per byte= 14.8\n * hash160: constant = 2650, per byte= 14.4\n\nThose compare to current varops figures of constant=0, per byte=10, I believe. I'm surprised that ripemd seems to be faster than sha256 for me. Presumably either taking the DUP costs into account or setting up an absurdly large stack so that DUP is unnecessary would generate more accurate figures than I've done above though. FWIW DUP/DROP benchmarks seems to imply figures of 276 compute units per operation plus 0.05 compute units per byte for me (as opposed to 0 + 1 per byte).",
  "actions_summary": [],
  "moderator": true,
  "admin": true,
  "staff": true,
  "user_id": 3,
  "hidden": false,
  "trust_level": 4,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "Probably want to specify the build options more completely if you want benchmark results to be comparable? (CMAKE_BUILD_TYPE as something other than Debug in particular) \n(Also #ifdef DEBUG and #ifdef USE_GMP instead of #if in various val64 code) \n\nProbably should provide a git repo to add the CSV\u2019&hellip;",
  "truncated": true,
  "post_url": "/t/benchmarking-bitcoin-script-evaluation-for-the-varops-budget-great-script-restoration/2094/2",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}