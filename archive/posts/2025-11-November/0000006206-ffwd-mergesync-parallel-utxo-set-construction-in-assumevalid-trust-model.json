{
  "id": 6206,
  "name": "ffwd",
  "username": "ffwd",
  "avatar_template": "/letter_avatar_proxy/v4/letter/f/e36b37/{size}.png",
  "created_at": "2025-11-09T18:15:03.938Z",
  "cooked": "<p>As the Bitcoin block chain grows, the time required to set up a new node from scratch also increases.</p>\n<p>At block height 910_000, just over 16 years into the Bitcoin experiment, more than 1.2 billion on-chain transactions have been recorded. Including witness data, this amounts to almost <strong>700GiB</strong> of data, enough to fill roughly 1000 CDs. Those transactions created over <strong>3 billion UTXOs</strong>, of which about <strong>95%</strong> have either already been spent or are provably unspendable. One goal of syncing a new node from scratch is therefore to determine <em>which</em> UTXOs survive valid transfers, so that new blocks can be checked quickly against the current UTXO-set.</p>\n<p>Current implementations locate a valid block header chain and then download and validate each block sequentially. This process is slow; depending on CPU, RAM, disk, and network speed it can take anywhere from a few hours to several days.</p>\n<p>Numerous ideas for improving Bitcoin scalability, specifically initial block download (IBD) speed, have been proposed over the years - some have even been implemented.</p>\n<p>Earlier this year an observation was made that current bottleneck is not CPU speed but I/O. Because we process blocks in sequence, we constantly write, read, delete entries in the <code>chainstate</code> database on disk. Using an SSD for the <code>chainstate</code> folder and increasing <code>-dbcache</code> can accelerate sync considerably, but this remains a sequential process.\nSome researchers explored a new approach called <strong>SwiftSync</strong>, originally published by Ruben Somsen. Reading about SwiftSync provides useful background, and inspired our work, but it is not required to understand the method described here.</p>\n<hr>\n<h2><a name=\"p-6206-algorithm-1\" class=\"anchor\" href=\"#p-6206-algorithm-1\"></a>Algorithm</h2>\n<p>We propose a map/reduce approach that lets a <code>-assumevalid</code> trusting node speed up UTXO-set construction using a parallel data pipeline.</p>\n<p>Let\u2019s start with a simpler question: What is the cardinality of the UTXO-set at height 922_527?</p>\n<p>block hash:<code>00000000000000000000a08e6723dc9eac9a108d50d05eb7c94885f3f10696bc</code></p>\n<ol start=\"0\">\n<li>Assume a valid copy of the block chain is available locally.</li>\n<li><strong>Shard</strong> all block data so that each of the <strong>N</strong> available cores processes roughly the same amount of data. For every transaction, <strong>extract</strong> every <em>OutPoint</em> (<code>txid:vout</code>) from both inputs and outputs. Each core appends its extracted OutPoints to a CSV file with two columns (<code>txid</code>, <code>vout</code>). Provably unspendable outputs and coin-base inputs are skipped.</li>\n<li><strong>Sort</strong> each CSV file in lexical order (by line) in parallel.</li>\n<li>Perform a <strong>single pass</strong> over each sorted file to eliminate duplicate lines; because the files are presorted, duplicates are guaranteed to be adjacent.</li>\n<li><strong>Recursively</strong> merge two or more surviving CSV files (subject to available RAM), then repeat steps 2-4 until only one file remains.</li>\n<li>After the final iteration, the line count of the remaining file is the answer to the question above.</li>\n</ol>\n<p>To obtain a full UTXO-set in a usable format, additional metadata (such as amounts, output scripts, creation height, etc.) can be added as extra columns to the CSV files. Keep in mind that plain HEX/CSV is not the most efficient encoding, it only served as simple explanation of the algorithm. External sorting and other techniques can be employed to reduce memory usage on constrained systems. Implementation is left as an exercise for the interested reader.</p>\n<h3><a name=\"p-6206-known-security-tradeoffs-2\" class=\"anchor\" href=\"#p-6206-known-security-tradeoffs-2\"></a>Known security tradeoffs</h3>\n<ul>\n<li>No signatures are processed or verified. This is unsafe unless another trustless node has performed full sequential validation, thereby confirming that the <code>-assumevalid</code> block hash is indeed user-valid.</li>\n</ul>\n<h3><a name=\"p-6206-open-questions-3\" class=\"anchor\" href=\"#p-6206-open-questions-3\"></a>Open questions</h3>\n<ul>\n<li>Are there additional attack vectors we have not considered?</li>\n</ul>\n<hr>\n<p><em>MergeSync</em> demonstrates that, under an assumevalid trust model, parallel processing and simple set operations can reduce wall clock for UTXO-set construction. Further work is needed to quantify performance gains on various hardware and to formalize security assumptions.</p>",
  "post_number": 1,
  "post_type": 1,
  "posts_count": 1,
  "updated_at": "2025-11-09T18:16:24.744Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 0,
  "reads": 9,
  "readers_count": 8,
  "score": 1.8,
  "yours": false,
  "topic_id": 2097,
  "topic_slug": "mergesync-parallel-utxo-set-construction-in-assumevalid-trust-model",
  "topic_title": "MergeSync: Parallel UTXO-set construction in assumevalid trust model",
  "topic_html_title": "MergeSync: Parallel UTXO-set construction in assumevalid trust model",
  "category_id": 8,
  "display_username": "ffwd",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "As the Bitcoin block chain grows, the time required to set up a new node from scratch also increases.\n\nAt block height 910_000, just over 16 years into the Bitcoin experiment, more than 1.2 billion on-chain transactions have been recorded. Including witness data, this amounts to almost **700GiB** of data, enough to fill roughly 1000 CDs. Those transactions created over **3 billion UTXOs**, of which about **95%** have either already been spent or are provably unspendable. One goal of syncing a new node from scratch is therefore to determine *which* UTXOs survive valid transfers, so that new blocks can be checked quickly against the current UTXO-set.\n\nCurrent implementations locate a valid block header chain and then download and validate each block sequentially. This process is slow; depending on CPU, RAM, disk, and network speed it can take anywhere from a few hours to several days.\n\nNumerous ideas for improving Bitcoin scalability, specifically initial block download (IBD) speed, have been proposed over the years - some have even been implemented.\n\nEarlier this year an observation was made that current bottleneck is not CPU speed but I/O. Because we process blocks in sequence, we constantly write, read, delete entries in the `chainstate` database on disk. Using an SSD for the `chainstate` folder and increasing `-dbcache` can accelerate sync considerably, but this remains a sequential process.\nSome researchers explored a new approach called **SwiftSync**, originally published by Ruben Somsen. Reading about SwiftSync provides useful background, and inspired our work, but it is not required to understand the method described here.\n\n---\n\n## Algorithm\n\nWe propose a map/reduce approach that lets a `-assumevalid` trusting node speed up UTXO-set construction using a parallel data pipeline.\n\nLet\u2019s start with a simpler question: What is the cardinality of the UTXO-set at height 922_527?\n\nblock hash:`00000000000000000000a08e6723dc9eac9a108d50d05eb7c94885f3f10696bc`\n\n0. Assume a valid copy of the block chain is available locally.\n1. **Shard** all block data so that each of the **N** available cores processes roughly the same amount of data. For every transaction, **extract** every *OutPoint* (`txid:vout`) from both inputs and outputs. Each core appends its extracted OutPoints to a CSV file with two columns (`txid`, `vout`). Provably unspendable outputs and coin-base inputs are skipped.\n2. **Sort** each CSV file in lexical order (by line) in parallel.\n3. Perform a **single pass** over each sorted file to eliminate duplicate lines; because the files are presorted, duplicates are guaranteed to be adjacent.\n4. **Recursively** merge two or more surviving CSV files (subject to available RAM), then repeat steps 2-4 until only one file remains.\n5. After the final iteration, the line count of the remaining file is the answer to the question above.\n\nTo obtain a full UTXO-set in a usable format, additional metadata (such as amounts, output scripts, creation height, etc.) can be added as extra columns to the CSV files. Keep in mind that plain HEX/CSV is not the most efficient encoding, it only served as simple explanation of the algorithm. External sorting and other techniques can be employed to reduce memory usage on constrained systems. Implementation is left as an exercise for the interested reader.\n\n### Known security tradeoffs\n\n* No signatures are processed or verified. This is unsafe unless another trustless node has performed full sequential validation, thereby confirming that the `-assumevalid` block hash is indeed user-valid.\n\n### Open questions\n\n* Are there additional attack vectors we have not considered?\n\n---\n\n*MergeSync* demonstrates that, under an assumevalid trust model, parallel processing and simple set operations can reduce wall clock for UTXO-set construction. Further work is needed to quantify performance gains on various hardware and to formalize security assumptions.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 977,
  "hidden": false,
  "trust_level": 0,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "As the Bitcoin block chain grows, the time required to set up a new node from scratch also increases. \nAt block height 910_000, just over 16 years into the Bitcoin experiment, more than 1.2 billion on-chain transactions have been recorded. Including witness data, this amounts to almost 700GiB of dat&hellip;",
  "truncated": true,
  "post_url": "/t/mergesync-parallel-utxo-set-construction-in-assumevalid-trust-model/2097/1",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null,
  "can_vote": false
}