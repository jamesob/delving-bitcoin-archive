{
  "id": 5172,
  "name": "Pieter Wuille",
  "username": "sipa",
  "avatar_template": "/user_avatar/delvingbitcoin.org/sipa/{size}/1100_2.png",
  "created_at": "2025-05-30T15:48:59.786Z",
  "cooked": "<p>I think the concern about block template construction is perhaps a bit too narrow. It is a complication on itself, and it gets worse as transaction sizes approach the block size, but it doesn\u2019t need too much hand-waving to assume software can be developed that can do block template building near-optimally <em>given a reasonable pool of candidate transactions</em>, and made available to anyone to use in mining operations. I think it would be far better if this was not needed, but there is a much more serious related issue.</p>\n<p>The issue, as I see it, is that as block template building for miners becomes more complex, reasoning <em>by relay nodes</em> (who possibly do not mine) about profitability becomes harder too, and far more so than actual block building, because it is a <strong>decision that needs to be made ahead of time</strong>, possibly long before the transaction might make it into a block, while other intervening transactions may arrive between relay and the block being found. Without information about miner-incentive-compatibility at relay time, and barring trivially-DoSable \u201cjust relay everything\u201d policies, an honest user who wishes to use giant transactions will not be able to rely on the public P2P network and anonymous miners to get the transaction confirmed anyway. And without the ability to rely on this, these users will be forced to make agreements with mining businesses anyway to get guaranteed block space. The goal of any proposal to deal with giant transactions must be <strong>to make users able to get their giant transactions confirmed, without private agreements with miners, and without significant extra cost</strong>. If a proposal fails at this, it does not matter how good block building available to the common denominator of miners is, because users and miners will bypass all of that anyway.</p>\n<p>To illustrate this problem, first consider a (intentionally unrealistically) extreme example: a user who wants a consensus-valid 999800 vB transaction to be confirmed. This is a legal size, in that consensus-valid blocks could exist that contain such a transaction. But due the size of a block header plus the minimum size of a coinbase transaction that includes a witness commitment, no miner will ever include it without a private agreement, as it would require them to burn all block income: there is no space for any secure normal transaction output in the coinbase anymore. That is no problem if the user pays the miner out of band for an amount above the displaced income they would get from the block otherwise, but I don\u2019t see how it could be done in a manner that only uses the public network.</p>\n<p>Okay, so we do need some limit on transaction sizes below that value, but how much? For exposition purposes, assume a hypothetical consensus limit of 990000 vB on transactions, assuming that no header+coinbase will realistically want to exceed 10000 vB. Does this solve our problem? Imagine this transaction pays a feerate that places it 20000 vB away from the top of the mempool (i.e., there exist 20000 vB worth of transactions that pay more). For DoS purposes (including relay rate limiting, RBF, and eviction decisions), the giant transaction will be treated as being better than <strong>all</strong> mempool transactions with lower feerate (possibly after chunking, though we can ignore that effect for the purpose of this discussion, as long as the giant transaction itself does not have big dependencies, or involved in CPFPs), including the 980000 vB worth of transactions that follow it. However, due to bin-packing effects, it may well be the case - even to an optimal knapsack solver - that this transaction stays outside of the optimal block template for a long time, or it may not be the case. It depends on the steepness of the fee-size diagram in the 10000 vB before it. This can result in bad relay decisions, bad replacement decisions, opening up the network to free relay attacks, transactions staying in the mempool which realistically won\u2019t actually be mined, and in the other direction if it happens closer to the bottom of the mempool, transactions which may be evicted despite being fairly reasonable for inclusion later. This effect is much less for small transactions, as they generally need a much bigger mempool change to be bumped to the next/previous block.</p>\n<p>In short: for small transactions, mining quality is a static decision that can be made once (e.g. at relay time). <strong>The larger a transaction gets, the more its mining quality will depend on other transactions available at block finding time.</strong> Uncertainty about these means that reasoning for DoS purposes breaks down.</p>\n<p>I\u2019m not convinced about the approach suggested here - it feels like it should lead to similar problems still, but thinking practically I can\u2019t really see many issues. At least the \u201conly 1 large transaction + coinbase in a block\u201d rule is effectively equivalent to \u201ctreat every transaction as if it had ~1 MvB in size\u201d, block building becomes \u201ctry normal block building, only considering non-giant transactions, and compare the fee of that with the highest absolute fee giant transaction\u201d. For everything else, the giant transaction looks like it can pretty much be treated as having <code>tx.fee / 1 MvB</code> as feerate. Due to only one transaction fitting in a block, CPFP and other dependency issues are largely gone. Pinning might be a problem, but only if those who need such transactions require replaceability in adverserial settings.</p>\n<p>(thanks to <a class=\"mention\" href=\"/u/sdaftuar\">@sdaftuar</a> for discussing this offline)</p>",
  "post_number": 8,
  "post_type": 1,
  "posts_count": 9,
  "updated_at": "2025-05-30T15:48:59.786Z",
  "reply_count": 0,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 481,
  "reads": 43,
  "readers_count": 42,
  "score": 2463.4,
  "yours": false,
  "topic_id": 1732,
  "topic_slug": "non-confiscatory-transaction-weight-limit",
  "topic_title": "Non-confiscatory Transaction Weight Limit",
  "topic_html_title": "Non-confiscatory Transaction Weight Limit",
  "category_id": 7,
  "display_username": "Pieter Wuille",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "I think the concern about block template construction is perhaps a bit too narrow. It is a complication on itself, and it gets worse as transaction sizes approach the block size, but it doesn't need too much hand-waving to assume software can be developed that can do block template building near-optimally *given a reasonable pool of candidate transactions*, and made available to anyone to use in mining operations. I think it would be far better if this was not needed, but there is a much more serious related issue.\n\nThe issue, as I see it, is that as block template building for miners becomes more complex, reasoning *by relay nodes* (who possibly do not mine) about profitability becomes harder too, and far more so than actual block building, because it is a **decision that needs to be made ahead of time**, possibly long before the transaction might make it into a block, while other intervening transactions may arrive between relay and the block being found. Without information about miner-incentive-compatibility at relay time, and barring trivially-DoSable \"just relay everything\" policies, an honest user who wishes to use giant transactions will not be able to rely on the public P2P network and anonymous miners to get the transaction confirmed anyway. And without the ability to rely on this, these users will be forced to make agreements with mining businesses anyway to get guaranteed block space. The goal of any proposal to deal with giant transactions must be **to make users able to get their giant transactions confirmed, without private agreements with miners, and without significant extra cost**. If a proposal fails at this, it does not matter how good block building available to the common denominator of miners is, because users and miners will bypass all of that anyway.\n\nTo illustrate this problem, first consider a (intentionally unrealistically) extreme example: a user who wants a consensus-valid 999800 vB transaction to be confirmed. This is a legal size, in that consensus-valid blocks could exist that contain such a transaction. But due the size of a block header plus the minimum size of a coinbase transaction that includes a witness commitment, no miner will ever include it without a private agreement, as it would require them to burn all block income: there is no space for any secure normal transaction output in the coinbase anymore. That is no problem if the user pays the miner out of band for an amount above the displaced income they would get from the block otherwise, but I don't see how it could be done in a manner that only uses the public network.\n\nOkay, so we do need some limit on transaction sizes below that value, but how much? For exposition purposes, assume a hypothetical consensus limit of 990000 vB on transactions, assuming that no header+coinbase will realistically want to exceed 10000 vB. Does this solve our problem? Imagine this transaction pays a feerate that places it 20000 vB away from the top of the mempool (i.e., there exist 20000 vB worth of transactions that pay more). For DoS purposes (including relay rate limiting, RBF, and eviction decisions), the giant transaction will be treated as being better than **all** mempool transactions with lower feerate (possibly after chunking, though we can ignore that effect for the purpose of this discussion, as long as the giant transaction itself does not have big dependencies, or involved in CPFPs), including the 980000 vB worth of transactions that follow it. However, due to bin-packing effects, it may well be the case - even to an optimal knapsack solver - that this transaction stays outside of the optimal block template for a long time, or it may not be the case. It depends on the steepness of the fee-size diagram in the 10000 vB before it. This can result in bad relay decisions, bad replacement decisions, opening up the network to free relay attacks, transactions staying in the mempool which realistically won't actually be mined, and in the other direction if it happens closer to the bottom of the mempool, transactions which may be evicted despite being fairly reasonable for inclusion later. This effect is much less for small transactions, as they generally need a much bigger mempool change to be bumped to the next/previous block.\n\nIn short: for small transactions, mining quality is a static decision that can be made once (e.g. at relay time). **The larger a transaction gets, the more its mining quality will depend on other transactions available at block finding time.** Uncertainty about these means that reasoning for DoS purposes breaks down.\n\nI'm not convinced about the approach suggested here - it feels like it should lead to similar problems still, but thinking practically I can't really see many issues. At least the \"only 1 large transaction + coinbase in a block\" rule is effectively equivalent to \"treat every transaction as if it had ~1 MvB in size\", block building becomes \"try normal block building, only considering non-giant transactions, and compare the fee of that with the highest absolute fee giant transaction\". For everything else, the giant transaction looks like it can pretty much be treated as having `tx.fee / 1 MvB` as feerate. Due to only one transaction fitting in a block, CPFP and other dependency issues are largely gone. Pinning might be a problem, but only if those who need such transactions require replaceability in adverserial settings.\n\n(thanks to @sdaftuar for discussing this offline)",
  "actions_summary": [
    {
      "id": 2,
      "count": 6
    }
  ],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 96,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "I think the concern about block template construction is perhaps a bit too narrow. It is a complication on itself, and it gets worse as transaction sizes approach the block size, but it doesn\u2019t need too much hand-waving to assume software can be developed that can do block template building near-opt&hellip;",
  "truncated": true,
  "post_url": "/t/non-confiscatory-transaction-weight-limit/1732/8",
  "reactions": [
    {
      "id": "+1",
      "type": "emoji",
      "count": 4
    },
    {
      "id": "thinking",
      "type": "emoji",
      "count": 2
    }
  ],
  "current_user_reaction": null,
  "reaction_users_count": 6,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}