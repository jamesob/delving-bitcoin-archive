{
  "id": 4911,
  "name": "Ruben Somsen",
  "username": "RubenSomsen",
  "avatar_template": "/user_avatar/delvingbitcoin.org/rubensomsen/{size}/188_2.png",
  "created_at": "2025-05-01T15:45:27.133Z",
  "cooked": "<p>Nice work. Makes sense to me that the speedup is less apparent when RAM isn\u2019t limited. Nice to see it\u2019s still quite significant.</p>\n<blockquote>\n<p>can we use a \u201cpretty-good\u201d hash [\u2026] Could we use a native 64/128-bit aggregator</p>\n</blockquote>\n<p>You could consider running a benchmark with a theoretical 0-cost hash aggregate function (i.e. simply don\u2019t hash or aggregate anything) to see what the maximum possible savings are. In terms of security, I believe it boils down to the chance of a single accidental collision, so a 4 byte hash would only have a one in 4 billion chance of incorrectly accepting an invalid chain. Not unreasonable, but the default should probably be at least 16 bytes, just to be able to say there\u2019s no reduction in security.</p>\n<blockquote>\n<p>Validate current state against hard-coded AssumeUTXO heights to fail early if inconsistencies are found</p>\n</blockquote>\n<p>Not fully sure I understood correctly, but if the assumeutxo hash and hints come from the same source, then I don\u2019t think failing early helps.</p>\n<blockquote>\n<p>Do we need to validate whether any of the outpoints accidentally hash to the same value?</p>\n</blockquote>\n<p>I wouldn\u2019t be concerned here about accidental hash collisions, as that\u2019s not happening with 32 bytes and a salt.</p>\n<blockquote>\n<p>What are the next bottlenecks</p>\n</blockquote>\n<p>Batch validation of Schnorr signatures is an interesting one to reduce CPU load (for the non-assumevalid version).</p>\n<blockquote>\n<p>I\u2019ll try to parallelize this in the following weeks</p>\n</blockquote>\n<p>Looking forward to those benchmarks!</p>",
  "post_number": 14,
  "post_type": 1,
  "posts_count": 18,
  "updated_at": "2025-05-01T15:45:27.133Z",
  "reply_count": 1,
  "reply_to_post_number": null,
  "quote_count": 0,
  "incoming_link_count": 1,
  "reads": 68,
  "readers_count": 67,
  "score": 23.4,
  "yours": false,
  "topic_id": 1562,
  "topic_slug": "swiftsync-speeding-up-ibd-with-pre-generated-hints-poc",
  "topic_title": "SwiftSync -- Speeding up IBD with pre-generated hints (PoC)",
  "topic_html_title": "SwiftSync &ndash; Speeding up IBD with pre-generated hints (PoC)",
  "category_id": 8,
  "display_username": "Ruben Somsen",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "badges_granted": [],
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": "",
  "bookmarked": false,
  "raw": "Nice work. Makes sense to me that the speedup is less apparent when RAM isn't limited. Nice to see it's still quite significant.\n\n>can we use a \u201cpretty-good\u201d hash [...] Could we use a native 64/128-bit aggregator\n\nYou could consider running a benchmark with a theoretical 0-cost hash aggregate function (i.e. simply don't hash or aggregate anything) to see what the maximum possible savings are. In terms of security, I believe it boils down to the chance of a single accidental collision, so a 4 byte hash would only have a one in 4 billion chance of incorrectly accepting an invalid chain. Not unreasonable, but the default should probably be at least 16 bytes, just to be able to say there's no reduction in security.\n\n>Validate current state against hard-coded AssumeUTXO heights to fail early if inconsistencies are found\n\nNot fully sure I understood correctly, but if the assumeutxo hash and hints come from the same source, then I don't think failing early helps.\n\n>Do we need to validate whether any of the outpoints accidentally hash to the same value?\n\nI wouldn't be concerned here about accidental hash collisions, as that's not happening with 32 bytes and a salt. \n\n>What are the next bottlenecks\n\nBatch validation of Schnorr signatures is an interesting one to reduce CPU load (for the non-assumevalid version). \n\n>I\u2019ll try to parallelize this in the following weeks\n\nLooking forward to those benchmarks!",
  "actions_summary": [],
  "moderator": true,
  "admin": true,
  "staff": true,
  "user_id": 2,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "excerpt": "Nice work. Makes sense to me that the speedup is less apparent when RAM isn\u2019t limited. Nice to see it\u2019s still quite significant. \n\ncan we use a \u201cpretty-good\u201d hash [\u2026] Could we use a native 64/128-bit aggregator \n\nYou could consider running a benchmark with a theoretical 0-cost hash aggregate functio&hellip;",
  "truncated": true,
  "post_url": "/t/swiftsync-speeding-up-ibd-with-pre-generated-hints-poc/1562/14",
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false,
  "can_accept_answer": false,
  "can_unaccept_answer": false,
  "accepted_answer": false,
  "topic_accepted_answer": null
}