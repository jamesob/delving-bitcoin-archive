{
  "id": 1038,
  "name": "Pieter Wuille",
  "username": "sipa",
  "avatar_template": "/user_avatar/delvingbitcoin.org/sipa/{size}/102_2.png",
  "created_at": "2024-01-09T13:55:01.463Z",
  "cooked": "<aside class=\"quote no-group\" data-username=\"GregTonoski\" data-post=\"18\" data-topic=\"327\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/gregtonoski/48/224_2.png\" class=\"avatar\"> GregTonoski:</div>\n<blockquote>\n<p>To the contrary, the network sees 1 byte as 1 byte (indiscriminately)</p>\n</blockquote>\n</aside>\n<p>The byte size of transactions in the P2P protocol is an artifact of the encoding scheme used. It does matter, because it directly correlates with bandwidth and disk usage for non-pruned nodes, but if we really cared about the impact these had we could easily adopt more efficient encodings for transactions on the network or on disk that encodes some parts of transactions more compactly. If we would do that, the consensus rules (ignoring witness discount) would still count transaction sizes by their old encoding, which would then not correspond to anything physical at all. Would you then still say 1 byte = 1 byte?</p>\n<p>So no, in my opinion weighing every byte in the current serialization equally is no less arbitrary than choosing to discount some part of that. A discussion can be had about what the proper way to weigh things is, but it is in my view misguided to treat \u201cbytes in the existing protocol\u201d as inherently more fair than something else.</p>\n<aside class=\"quote no-group\" data-username=\"GregTonoski\" data-post=\"15\" data-topic=\"327\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/gregtonoski/48/224_2.png\" class=\"avatar\"> GregTonoski:</div>\n<blockquote>\n<p>Not to prioritize but make equal to other transactions. Why would such a transaction remain at disadvantage?</p>\n</blockquote>\n</aside>\n<p>Ignoring the question of what the proper way to weigh transactions is, I think you\u2019re missing something else here too: the witness discount (or any alternative weighing we\u2019d want) doesn\u2019t just apply to fees, it also reduces the ability for transactions to push out others. This is because the discount literally shrinks transactions from the perspective of the weight limit (which is all that matters for consensus).</p>\n<p>As an example, imagine we implement some kind of different weighing scheme that doubles the weight of some subset of transactions. These transactions would now need to pay twice as much, relatively speaking. But they would also take twice as much \u201cspace\u201d, meaning they push out twice as much other transaction bytes from the block.</p>\n<p>Put otherwise, if you double the weight of some transactions, raising their cost, and thereby say halve the demand for them, their impact on the fee pressure (and your ability to get transactions confirmed) is unaffected. If your goal is getting relief from competition with your own transactions, you need a superlinear impact of fee on demand, which is far from a given.</p>\n<aside class=\"quote no-group\" data-username=\"GregTonoski\" data-post=\"19\" data-topic=\"327\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/user_avatar/delvingbitcoin.org/gregtonoski/48/224_2.png\" class=\"avatar\"> GregTonoski:</div>\n<blockquote>\n<p>The Bitcoin fee is not charged per UTXO set size. It is not charged from a node operator. Nodes are up and running independently of Bitcoin fees.</p>\n</blockquote>\n</aside>\n<p>This asymmetry is exactly the reason why weighing bytes properly (whatever that may mean) is important. Fees are paid to miners, and not to validation nodes. Yet not every byte is equally resource-intensive for validation nodes. Nodes are not powerless however, as they can collectively enforce consensus rules, and the witness discount is an attempt at discouraging bytes that are more impactful to them through consensus rules.</p>\n<hr>\n<aside class=\"quote no-group\" data-username=\"rustynail\" data-post=\"17\" data-topic=\"327\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://delvingbitcoin.org/letter_avatar_proxy/v4/letter/r/cc9497/48.png\" class=\"avatar\"> rustynail:</div>\n<blockquote>\n<p>Why have the discount if utreexo is going to make UTXO set size unimportant?</p>\n</blockquote>\n</aside>\n<p>It may eventually have an impact, but I think the reality is a lot more nuanced. Utreexo, as I understand it, introduces effectively a new node type that fully validates but needs proofs to do so. Transactions and blocks cannot be relayed from normal nodes to utreexo nodes directly, but must go through a bridge node that can add the proofs. The resource costs for a bridge node are significantly higher than running a normal full node today, and still scale with the UTXO set size.</p>\n<p>Unless the entire ecosystem migrates to a Utreexo model, these bridge nodes and their scalability remains important to the network. Lightweight wallets (as in ones that don\u2019t see full blocks) cannot produce their own proofs, so they\u2019ll inherently need some kind of service to do it for them. Further, presigned transactions cannot have proofs, as the proofs would grow outdated anyway as new blocks are added to the chain.</p>\n<p>Miners can adopt Utreexo, but unless nearly all of them do so, the ones who do will be at a disadvantage because they\u2019ll need a bridge service to construct proofs for blocks received from non-Utreexo nodes, which would incur a higher latency than just running an old nodes.</p>\n<p>So I think it\u2019s better to say that Utreexo moves the concern of UTXO set size elsewhere, to a place where it may become less relevant for the critical validation path, but doing so has costs too.</p>",
  "post_number": 20,
  "post_type": 1,
  "updated_at": "2024-01-09T13:55:01.463Z",
  "reply_count": 0,
  "reply_to_post_number": 18,
  "quote_count": 4,
  "incoming_link_count": 10,
  "reads": 6,
  "readers_count": 5,
  "score": 51.2,
  "yours": false,
  "topic_id": 327,
  "topic_slug": "bug-spammers-get-bitcoin-blockspace-at-discounted-price-lets-fix-it",
  "topic_title": "[BUG]: spammers get Bitcoin blockspace at discounted price. Let's fix it",
  "topic_html_title": "[BUG]: spammers get Bitcoin blockspace at discounted price. Let&rsquo;s fix it",
  "category_id": 7,
  "display_username": "Pieter Wuille",
  "primary_group_name": null,
  "flair_name": null,
  "flair_url": null,
  "flair_bg_color": null,
  "flair_color": null,
  "flair_group_id": null,
  "version": 1,
  "can_edit": false,
  "can_delete": false,
  "can_recover": false,
  "can_see_hidden_post": false,
  "can_wiki": false,
  "user_title": null,
  "bookmarked": false,
  "raw": "[quote=\"GregTonoski, post:18, topic:327\"]\nTo the contrary, the network sees 1 byte as 1 byte (indiscriminately)\n[/quote]\n\nThe byte size of transactions in the P2P protocol is an artifact of the encoding scheme used. It does matter, because it directly correlates with bandwidth and disk usage for non-pruned nodes, but if we really cared about the impact these had we could easily adopt more efficient encodings for transactions on the network or on disk that encodes some parts of transactions more compactly. If we would do that, the consensus rules (ignoring witness discount) would still count transaction sizes by their old encoding, which would then not correspond to anything physical at all. Would you then still say 1 byte = 1 byte?\n\nSo no, in my opinion weighing every byte in the current serialization equally is no less arbitrary than choosing to discount some part of that. A discussion can be had about what the proper way to weigh things is, but it is in my view misguided to treat \"bytes in the existing protocol\" as inherently more fair than something else.\n\n[quote=\"GregTonoski, post:15, topic:327\"]\nNot to prioritize but make equal to other transactions. Why would such a transaction remain at disadvantage?\n[/quote]\n\nIgnoring the question of what the proper way to weigh transactions is, I think you're missing something else here too: the witness discount (or any alternative weighing we'd want) doesn't just apply to fees, it also reduces the ability for transactions to push out others. This is because the discount literally shrinks transactions from the perspective of the weight limit (which is all that matters for consensus).\n\nAs an example, imagine we implement some kind of different weighing scheme that doubles the weight of some subset of transactions. These transactions would now need to pay twice as much, relatively speaking. But they would also take twice as much \"space\", meaning they push out twice as much other transaction bytes from the block.\n\nPut otherwise, if you double the weight of some transactions, raising their cost, and thereby say halve the demand for them, their impact on the fee pressure (and your ability to get transactions confirmed) is unaffected. If your goal is getting relief from competition with your own transactions, you need a superlinear impact of fee on demand, which is far from a given.\n\n[quote=\"GregTonoski, post:19, topic:327\"]\nThe Bitcoin fee is not charged per UTXO set size. It is not charged from a node operator. Nodes are up and running independently of Bitcoin fees.\n[/quote]\n\nThis asymmetry is exactly the reason why weighing bytes properly (whatever that may mean) is important. Fees are paid to miners, and not to validation nodes. Yet not every byte is equally resource-intensive for validation nodes. Nodes are not powerless however, as they can collectively enforce consensus rules, and the witness discount is an attempt at discouraging bytes that are more impactful to them through consensus rules.\n\n---\n\n[quote=\"rustynail, post:17, topic:327, full:true\"]\nWhy have the discount if utreexo is going to make UTXO set size unimportant?\n[/quote]\n\nIt may eventually have an impact, but I think the reality is a lot more nuanced. Utreexo, as I understand it, introduces effectively a new node type that fully validates but needs proofs to do so. Transactions and blocks cannot be relayed from normal nodes to utreexo nodes directly, but must go through a bridge node that can add the proofs. The resource costs for a bridge node are significantly higher than running a normal full node today, and still scale with the UTXO set size.\n\nUnless the entire ecosystem migrates to a Utreexo model, these bridge nodes and their scalability remains important to the network. Lightweight wallets (as in ones that don't see full blocks) cannot produce their own proofs, so they'll inherently need some kind of service to do it for them. Further, presigned transactions cannot have proofs, as the proofs would grow outdated anyway as new blocks are added to the chain.\n\nMiners can adopt Utreexo, but unless nearly all of them do so, the ones who do will be at a disadvantage because they'll need a bridge service to construct proofs for blocks received from non-Utreexo nodes, which would incur a higher latency than just running an old nodes.\n\nSo I think it's better to say that Utreexo moves the concern of UTXO set size elsewhere, to a place where it may become less relevant for the critical validation path, but doing so has costs too.",
  "actions_summary": [],
  "moderator": false,
  "admin": false,
  "staff": false,
  "user_id": 96,
  "hidden": false,
  "trust_level": 3,
  "deleted_at": null,
  "user_deleted": false,
  "edit_reason": null,
  "can_view_edit_history": true,
  "wiki": false,
  "reactions": [],
  "current_user_reaction": null,
  "reaction_users_count": 0,
  "current_user_used_main_reaction": false
}