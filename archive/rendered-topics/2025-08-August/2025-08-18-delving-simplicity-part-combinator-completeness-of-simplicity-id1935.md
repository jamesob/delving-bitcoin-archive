# Delving Simplicity Part ‚Ö°: Combinator Completeness of Simplicity

roconnor-blockstream | 2025-08-18 14:26:20 UTC | #1

In [Part ‚Ö†](https://delvingbitcoin.org/t/delving-simplicity-part-three-fundamental-ways-of-combining-computations/1902) of this series, we described the three methods of composition in programming: sequential, parallel, and conditional. In this part, we will see how these methods of composition are directly realized within Simplicity.

# Simplicity Types

Parallel compositions produce an output that is a product of two types. We write this product type as `A √ó B` or `A * B`. Conditional compositions consume an input that is a sum (also known as a tagged union) of two types `A + B`. In order to directly realize these forms of composition, Simplicity‚Äôs type system includes these two type formers. Simplicity has only one other type: the unit type.

## Unit Type

Simplicity‚Äôs unit type, written `ùüô` or `ONE`, is a type with only one value. This single value is typically denoted by the empty tuple, written `‚ü®‚ü©` or `()`. Since this type consists of only a single value, values of this type contain no information; it is essentially a zero-bit data type.

We use the `:` to denote a value‚Äôs type, so `‚ü®‚ü© : ùüô` means the empty tuple has unit type.

## Sum Type

The sum of two types, `A + B`, is the tagged union of two types. The tag is either ‚Äúleft‚Äù or ‚Äúright,‚Äù indicating whether a given value corresponds to the left-hand type of the sum or the right-hand type. If `a : A`, then we write `œÉ·¥∏(a)` or `inl(a)` for the left-tagged value of type `A + B` (i.e., `œÉ·¥∏(a) : A + B`). Similarly, if `b : B`, then we write `œÉ·¥ø(b)` or `inr(b)` for the right-tagged value (i.e., `œÉ·¥ø(b) : A + B`).

Values of a sum type always contains a tag, even when taking the sum of two identical types. In particular, if `a : A`, then `œÉ·¥∏(a)` and `œÉ·¥ø(a)` are two distinct values of type `A + A` because they have different tags.

### Boolean Type

We will discuss building data structures in more detail in the next part of this series, but as a preview, let‚Äôs look at our first non-trivial type: `ùüô + ùüô`. We typically denote this type using the shorthand `ùüö` or `TWO`. Keep in mind that this is purely notational convenience. This type is the tagged union of two unit types and therefore has two values: `œÉ·¥∏‚ü®‚ü© : ùüö` and `œÉ·¥ø‚ü®‚ü© : ùüö`. This is a one-bit data type. By convention, we consider `œÉ·¥∏‚ü®‚ü©` to be a zero or false value, which we may write as `0` or `false`. Similarly, we consider `œÉ·¥ø‚ü®‚ü©` to be a one or true value, which we may write as `1` or `true`. Again, this is just notational convenience.

## Product Type

The product of two types, `A √ó B`, contains pairs of values. If `a : A` and `b : B` are values of types `A` and `B`, then the pair, written as `‚ü®a, b‚ü©` or `(a, b)`, has type `A √ó B` (i.e., `‚ü®a, b‚ü© : A √ó B`).


üõà
Simplicity only has finite types. You can read off the number of values a type has by interpreting the type using arithmetic. For example, the type `ùüö √ó ùüö` has four values. The type `ùüö + ùüö` also has four values; however, they are different values from those of type `ùüö √ó ùüö`.

# Core Simplicity Expressions

In Part ‚Ö†, we presumed we had some set of basic operations, and each operation had an input type and an output type. We will write `f : A ‚ä¢ B` or `f : A |- B` to mean that `f` has input type `A` and output type `B`. 


‚ö†
While Simplicity expressions have input and output types, Simplicity‚Äôs types do not include function types. Simplicity is a ‚Äúfirst-order‚Äù programming language.

## Two Basic Operations

Simplicity provides several basic operations, most of which we will see later in this series when we talk about Simplicity jets. The core language provides expressions for only two basic operations: `iden : A ‚ä¢ A` and `unit : A ‚ä¢ ùüô`. Technically, these are two families of operations, with one of each operation per Simplicity type `A`.

The `iden` operation passes its input along to its output. The `unit` operation discards its input and returns the empty tuple, `‚ü®‚ü©`.

To be more precise, we need to distinguish between programming language syntax and semantics. `iden` and `unit` are Simplicity *expressions*, which constitute the language‚Äôs syntax. Simplicity expressions *denote* operations, which are functions from an input type to an output type. We use fancy square brackets to indicate what a Simplicity expression denotes:

* `‚ü¶iden‚üß(a) = a` (alternatively written`|[iden]|(a) = a`)


* `‚ü¶unit‚üß(a) = ‚ü®‚ü©`

## Three Composition Combinators

The sequential and parallel composition methods are directly implemented by Simplicity *combinators*. Recall that Simplicity expressions denote functions. A combinator is a function that takes functions and returns a function. We follow the usual programming language practice of defining these Simplicity expressions using typing rules:

* ```none
  f : A ‚ä¢ B    g : B ‚ä¢ C
  ----------------------
     comp f g : A ‚ä¢ C
  ```
* ```none
  f : A ‚ä¢ B    g : A ‚ä¢ C
  ----------------------
   pair f g : A ‚ä¢ B √ó C
  ```

The first rule states that if `f` is a Simplicity expression with input type `A` and output type `B`, and `g` is an expression with input type `B` and output type `C`, then `comp f g` is a Simplicity expression with input type `A` and output type `C`. The second rule defines the `pair f g` expression similarly.

We may also write `comp f g` as `f ‚®æ g` or `f >>> g` and we may write `pair f g` as `f ‚ñµ g` or `f &&& g`.

These combinators form expressions of exactly the right type to implement sequential and parallel composition. Accordingly, these expressions have semantics for these forms of composition.

* `‚ü¶f ‚®æ g‚üß(a) = ‚ü¶g‚üß(‚ü¶f‚üß(a))`
* `‚ü¶f ‚ñµ g‚üß(a) = ‚ü®‚ü¶f‚üß(a), ‚ü¶g‚üß(a)‚ü©`


‚ö†
The version of sequential composition used in mathematics is backwards with respect to our syntax.

The other method of composition is conditional composition. The natural approach would be, given `f : A ‚ä¢ C` and `g : B ‚ä¢ C`, to define `copair f g : A + B ‚ä¢ C`. However, `copair` doesn‚Äôt allow branches to access shared data. In particular, the distribution function,

 `dist : (A + B) √ó C ‚ä¢ A √ó C + B √ó C`,

cannot be defined in terms of `copair` alone.

Instead of `copair`, we use the following `case` combinator instead.

* ```none
  f : A √ó C ‚ä¢ D    g : B √ó C ‚ä¢ D
  ------------------------------
    case f g : (A + B) √ó C ‚ä¢ D
  ```

This `case` combinator does conditional composition and distribution all in one. It‚Äôs helpful to think of the type `C` as the type of a shared environment that both branches of the `case` expression get access to. Accordingly, the `case` combinator has the following semantics.

* `‚ü¶case f g‚üß‚ü®œÉ·¥∏(a), c‚ü© = ‚ü¶f‚üß‚ü®a, c‚ü©`
* `‚ü¶case f g‚üß‚ü®œÉ·¥ø(b), c‚ü© = ‚ü¶g‚üß‚ü®b, c‚ü©`

## Four more Combinators

We are still missing a few fundamental operations on our types. For instance, we can use `pair` to produce product types, but we have no way yet to consume product types. Similarly, the `case` combinator consumes sum types, but we have no way yet to produce sum types. Four additional combinators enable the consumption of product types and production of sum types. The first two combinators extract one of the values of a pair, and execute a function on it:

* ```none
       f : A ‚ä¢ C
  ------------------
  take f : A √ó B ‚ä¢ C
  ```


* ```none
       f : B ‚ä¢ C
  ------------------
  drop f : A √ó B ‚ä¢ C
  ```

The last two combinators execute a function and wrap the result with either a left tag or a right tag.

* ```none
       f : A ‚ä¢ B
  ------------------
  injl f : A ‚ä¢ B + C
  ```


* ```none
       f : A ‚ä¢ C
  ------------------
  injr f : A ‚ä¢ B + C
  ```

These combinators have the required semantics:

* `‚ü¶take f‚üß‚ü®a, b‚ü© = ‚ü¶f‚üß(a)`
* `‚ü¶drop f‚üß‚ü®a, b‚ü© = ‚ü¶f‚üß(b)`
* `‚ü¶injl f‚üß(a) = œÉ·¥∏(‚ü¶f‚üß(a))`
* `‚ü¶injr f‚üß(a) = œÉ·¥ø(‚ü¶f‚üß(a))`

# Simplicity and the Sequent Calculus

Readers familiar with formal logic will find a resemblance between our set of nine rules and the conjunctive-disjunctive fragment of Gentzen‚Äôs sequent calculus. The sequent calculus inspired Simplicity‚Äôs language design. One can think of Simplicity as a slightly tweaked variant of the ‚Äúfunctional interpretation‚Äù of Gentzen‚Äôs sequent calculus, which is analogous to the Curry-Howard correspondence between natural deduction and the lambda calculus.

Notice that the premises in the combinator rules of the core Simplicity language are typically ‚Äúsmaller‚Äù than the types in the conclusion. Later in this series we will describe ‚Äúthe Bit Machine‚Äù, an abstract stack machine for interpreting Simplicity expressions. The Bit Machine takes advantage of this phenomenon of ‚Äúsmaller types in the premises‚Äù to minimize the amount of data copied during execution. Only the `iden` and `comp` core combinators involve moving data around, and the rest of the core combinators are implemented with just some bookkeeping.

# Values are not Expressions

Above we gave notation for the values of the various types that Simplicity supports. It is important to note that Simplicity expressions only denote operations, and operations are functions, not values. We can construct values by starting with the `unit` function and composing it with `injl`, `injr` and `pair` combinators. To avoid writing out such tedious constructions, we introduce the following notation.

Given a value of some Simplicity type, `b : B`, we write `scribe b : A ‚ä¢ B` for the unique Simplicity expression that always returns the value `b`. For example, `scribe ‚ü®œÉ·¥∏‚ü®‚ü©, œÉ·¥ø‚ü®‚ü©‚ü©` is shorthand for `injl unit ‚ñµ injr unit : A ‚ä¢ ùüö √ó ùüö`. Keep in mind that, `scribe` isn‚Äôt a Simplicity combinator; it is a macro-like notional convenience.

Bitcoin Script has a similar property; Bitcoin Script only contains operations. For example `OP_1` is the operation that pushes the value `1` onto the stack, but there is no Bitcoin Script expression for the value `1` itself.

# Simplicity‚Äôs Completeness Theorem

Earlier we saw that the na√Øve realization of conditional composition would have left us unable to define the ‚Äúdistribution‚Äù function `dist : (A + B) √ó C ‚ä¢ A √ó C + B √ó C`. How do we know that we aren‚Äôt missing something else?

The answer is that the ‚ÄúSimplicity Completeness theorem‚Äù proves that for any function between two Simplicity types there exists some Simplicity expression that denotes it.

We already saw `scribe`, which is a special case of the Simplicity Completeness theorem for constant functions. For other functions, we can build an nested set of `case` expressions to fully decompose any input of any type and compose that with a `scribe` expression for every output value. This procedure constructs what is effectively a giant lookup table to implement any function.

But you don‚Äôt have to trust us. The core Simplicity language is formally specified in the Rocq proof assistant (formerly named the Coq proof assistant), and the [Simplicity Completeness theorem](https://github.com/BlockstreamResearch/simplicity/blob/8adafc9b55d694111c99fa11f90c7e48376ed1e5/Coq/Simplicity/Core.v#L92-L93) is one of the theorems we have formally verified.

# Conclusion

We described Simplicity‚Äôs type system, combinators, and basic expressions that make up Simplicity‚Äôs core computational language. Later in this series, we will describe how Simplicity interacts with transactions and introduce a few more Simplicity combinators.

But before delving into that, in Part ‚Ö¢, we will look at building data structures and computation from this seemingly meager language. While we do have the Simplicity Completeness theorem, the expressions that the theorem constructs are astronomical in size, making it only useful as a theoretical tool. The practice of generating succinct Simplicity expressions could be viewed as an exercise in compression by exploiting structure within computations.

-------------------------

